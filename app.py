import os
import io
import json
import time
import base64
import threading
import tempfile
import concurrent.futures
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any

import requests
import streamlit as st
import cv2
import numpy as np
from PIL import Image, ImageDraw
import yt_dlp

# =========================================================
# Z.ai / æ™ºè°±ï¼ˆZhipuï¼‰HTTP API åŸºç¡€é…ç½®
# =========================================================
ZAI_BASE_URL = "https://api.z.ai/api/paas/v4/chat/completions"  # å®˜æ–¹ç¤ºä¾‹æ¥å£ :contentReference[oaicite:2]{index=2}

DEFAULT_TEXT_MODEL = "glm-4.5"     # çº¯æ–‡æœ¬ï¼ˆä½ ä¹Ÿå¯åœ¨ä¾§è¾¹æ æ”¹ï¼‰
DEFAULT_VISION_MODEL = "glm-4.6v"  # è§†è§‰æ¨¡å‹ï¼Œæ”¯æŒ image_url :contentReference[oaicite:3]{index=3}

# =========================================================
# ç®€æ˜“é™æµå™¨ï¼šé¿å…å…è´¹é¢åº¦ / RPM è§¦å‘
# =========================================================
class RateLimiter:
    def __init__(self, rpm: int):
        self.rpm = max(1, int(rpm))
        self.lock = threading.Lock()
        self.calls: List[float] = []

    def acquire(self):
        with self.lock:
            now = time.time()
            window = 60.0
            self.calls = [t for t in self.calls if now - t < window]

            if len(self.calls) >= self.rpm:
                sleep_s = window - (now - self.calls[0]) + 0.05
                sleep_s = max(0.0, sleep_s)
            else:
                sleep_s = 0.0

        if sleep_s > 0:
            time.sleep(sleep_s)

        with self.lock:
            self.calls.append(time.time())


def zai_chat_completions(
    api_key: str,
    model: str,
    messages: List[Dict[str, Any]],
    temperature: float = 0.6,
    max_tokens: int = 2048,
    thinking_type: str = "disabled",
    timeout: int = 120,
    rate_limiter: Optional[RateLimiter] = None,
) -> str:
    """
    Z.ai Chat Completions HTTP è°ƒç”¨ã€‚
    - Authorization: Bearer <api_key> :contentReference[oaicite:4]{index=4}
    - Vision: messages.content æ”¯æŒ image_url / text æ··åˆ :contentReference[oaicite:5]{index=5}
    """
    if rate_limiter:
        rate_limiter.acquire()

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    payload: Dict[str, Any] = {
        "model": model,
        "messages": messages,
        "temperature": float(temperature),
        "max_tokens": int(max_tokens),
        "thinking": {"type": thinking_type},  # æ–‡æ¡£ç¤ºä¾‹ä¸­å­˜åœ¨ thinking.type :contentReference[oaicite:6]{index=6}
    }

    resp = requests.post(ZAI_BASE_URL, headers=headers, json=payload, timeout=timeout)
    if resp.status_code != 200:
        raise RuntimeError(f"HTTP {resp.status_code}: {resp.text[:800]}")

    data = resp.json()
    # å…¼å®¹å¸¸è§ç»“æ„ï¼šchoices[0].message.content
    try:
        return (data["choices"][0]["message"].get("content") or "").strip()
    except Exception:
        return json.dumps(data, ensure_ascii=False)[:2000]


def extract_json_object(text: str) -> Dict[str, Any]:
    """
    ä»æ¨¡å‹è¾“å‡ºé‡Œæå– JSON å¯¹è±¡ï¼ˆå®¹é”™ï¼šå¯èƒ½å¸¦é¢å¤–æ–‡å­—/ä»£ç å—ï¼‰ã€‚
    """
    if not text:
        raise ValueError("ç©ºå“åº”")
    # å»æ‰ ```json ``` åŒ…è£¹
    cleaned = text.strip()
    if cleaned.startswith("```"):
        cleaned = cleaned.strip("`")
        cleaned = cleaned.replace("json", "", 1).strip()

    start = cleaned.find("{")
    end = cleaned.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise ValueError("æœªæ£€æµ‹åˆ° JSON å¯¹è±¡")
    obj_str = cleaned[start:end + 1]
    return json.loads(obj_str)


def pil_to_data_url(img: Image.Image, quality: int = 92) -> str:
    """
    PIL -> data:image/jpeg;base64,...
    """
    buf = io.BytesIO()
    img.convert("RGB").save(buf, format="JPEG", quality=quality, optimize=True)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    return f"data:image/jpeg;base64,{b64}"


# =========================================================
# åŠŸèƒ½ Aï¼šåˆ†é•œ JSON ç”Ÿæˆï¼ˆåŸ Gemini ç‰ˆæœ¬ => æ”¹ Z.aiï¼‰
# =========================================================
def build_storyboard_prompt(brand: str, product: str, duration_sec: int, style: str) -> str:
    return f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çŸ­è§†é¢‘å¯¼æ¼”å’Œå¹¿å‘Šæ–‡æ¡ˆï¼Œæ“…é•¿ä¸ºæŠ–éŸ³ / å°çº¢ä¹¦ / è§†é¢‘å·è®¾è®¡é«˜è½¬åŒ–ç«–ç‰ˆå¹¿å‘Šã€‚

è¯·ä¸ºä¸‹é¢çš„äº§å“è®¾è®¡ä¸€ä¸ªæ—¶é•¿çº¦ {duration_sec} ç§’çš„ç«–ç‰ˆçŸ­è§†é¢‘å¹¿å‘Šåˆ†é•œï¼ŒåŒ…å«æ¯ä¸ªé•œå¤´çš„æ–‡æ¡ˆå’Œç”¨äº AI å‡ºå›¾çš„è‹±æ–‡æç¤ºè¯ã€‚

å“ç‰Œï¼š{brand}
äº§å“ï¼š{product}
æ•´ä½“é£æ ¼ï¼š{style}

è¦æ±‚ï¼š
1. è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSONï¼ˆä¸è¦ä»»ä½•å¤šä½™è§£é‡Šã€æ³¨é‡Šæˆ– Markdownï¼‰ï¼Œé¡¶å±‚ç»“æ„ï¼š
{{
  "brand": "...",
  "product": "...",
  "duration_sec": {duration_sec},
  "style": "...",
  "scenes": [
    {{
      "id": "S01",
      "time_range": "0.0-2.0",
      "shot_desc": "ä¸­æ–‡ï¼Œæè¿°ç”»é¢ï¼Œé€‚åˆç»™å¯¼æ¼”çœ‹çš„åˆ†é•œæè¿°",
      "camera": "ä¸­æ–‡ï¼Œé•œå¤´æœºä½ä¸è¿åŠ¨ï¼ˆå¦‚ï¼šæ‰‹æŒä¸­æ™¯æ¨è¿‘ã€èˆªæ‹ä¿¯è§†æ‘‡é•œç­‰ï¼‰",
      "action": "ä¸­æ–‡ï¼Œäººç‰©åŠ¨ä½œä¸å…³é”®è¡Œä¸º",
      "mood": "ä¸­æ–‡ï¼Œæƒ…ç»ªæ°›å›´ï¼ˆå¦‚æ¸©é¦¨ã€ç´§å¼ ã€æ²»æ„ˆã€çƒŸç«æ°”ï¼‰",
      "voiceover": "ä¸­æ–‡æ—ç™½/å£æ’­æ–‡æ¡ˆï¼Œå£è¯­åŒ–ã€æœ‰é”€å”®åŠ›ï¼Œé€‚åˆé…éŸ³ç›´æ¥å¿µ",
      "image_prompt_en": "è‹±æ–‡æç¤ºè¯ï¼Œç”¨äºç”Ÿæˆè¿™ä¸€é•œå¤´çš„ AI é™å¸§ç”»é¢ï¼ŒåŒ…å«äººç‰©ã€ç¯å¢ƒã€å…‰çº¿ã€é•œå¤´ã€ç”»è´¨ç­‰ç»†èŠ‚"
    }}
  ]
}}

2. æ³¨æ„ï¼š
- time_range ä» 0.0 ç§’å¼€å§‹ï¼Œåä¸€é•œå¤´çš„å¼€å§‹æ—¶é—´ç´§æ¥å‰ä¸€é•œå¤´ç»“æŸæ—¶é—´ï¼Œæ€»æ—¶é•¿æ§åˆ¶åœ¨ {duration_sec} ç§’å·¦å³ã€‚
- voiceover å°½é‡è‡ªç„¶å£è¯­åŒ–ï¼Œåƒä¸€ä¸ªçœŸå®ä¸»æ’­åœ¨è®²ï¼Œè€Œä¸æ˜¯æ–°é—»æ’­éŸ³è…”ã€‚
- image_prompt_en è¦å°½é‡è¯¦ç»†ã€æ‘„å½±æ„Ÿå¼ºï¼ˆclose-up / medium / wide / 9:16 / cinematic lighting / 8k ç­‰ï¼‰ã€‚
""".strip()


def generate_storyboard_zai(
    api_key: str,
    model: str,
    brand: str,
    product: str,
    duration_sec: int,
    style: str,
    limiter: RateLimiter,
) -> Dict[str, Any]:
    prompt = build_storyboard_prompt(brand, product, duration_sec, style)
    text = zai_chat_completions(
        api_key=api_key,
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=2500,
        thinking_type="disabled",
        rate_limiter=limiter,
    )
    return extract_json_object(text)


def extract_voiceover(data: Dict[str, Any]) -> str:
    scenes = data.get("scenes", []) or []
    lines = []
    for s in scenes:
        sid = s.get("id", "")
        t = s.get("time_range", "")
        vo = s.get("voiceover", "")
        if vo:
            lines.append(f"[{sid} | {t}] {vo}")
    return "\n".join(lines).strip()


# =========================================================
# åŠŸèƒ½ Bï¼šè§†é¢‘æŠ½å¸§ + å¤šå¸§è§†è§‰åˆ†æï¼ˆåŸ Gemini => æ”¹ Z.ai Visionï¼‰
# =========================================================
DISPLAY_IMAGE_WIDTH = 320
PALETTE_WIDTH = 320
PALETTE_HEIGHT = 26


def extract_keyframes_dynamic(
    video_path: str,
    min_frames: int = 6,
    max_frames: int = 30,
    base_fps: float = 0.8,
    start_sec: Optional[float] = None,
    end_sec: Optional[float] = None,
) -> Tuple[List[Image.Image], float, Tuple[float, float]]:
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    if not fps or fps <= 1e-2:
        fps = 25.0

    if total_frames <= 0:
        cap.release()
        return [], 0.0, (0.0, 0.0)

    duration = total_frames / fps

    if start_sec is None or start_sec < 0:
        start_sec = 0.0
    if end_sec is None or end_sec <= start_sec or end_sec > duration:
        end_sec = duration

    start_frame = int(start_sec * fps)
    end_frame_excl = min(total_frames, int(end_sec * fps))
    segment_frames = end_frame_excl - start_frame

    if segment_frames <= 0:
        start_sec, end_sec = 0.0, duration
        start_frame, end_frame_excl = 0, total_frames
        segment_frames = total_frames

    segment_duration = segment_frames / fps
    ideal_n = int(segment_duration * base_fps)
    target_n = max(min_frames, ideal_n)
    target_n = min(target_n, max_frames, segment_frames)
    if target_n <= 0:
        cap.release()
        return [], duration, (start_sec, end_sec)

    step = segment_frames / float(target_n)
    frame_indices = [start_frame + int(i * step) for i in range(target_n)]

    images: List[Image.Image] = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret and frame is not None:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            images.append(Image.fromarray(rgb))
        else:
            images.append(Image.new("RGB", (200, 200), color="gray"))

    cap.release()
    return images, duration, (float(start_sec), float(end_sec))


def download_video_from_url(url: str) -> str:
    if not url:
        raise ValueError("è§†é¢‘é“¾æ¥ä¸ºç©º")

    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tmp_path = tmp.name
    tmp.close()

    ydl_opts = {
        "format": "mp4/bestvideo+bestaudio/best",
        "outtmpl": tmp_path,
        "merge_output_format": "mp4",
        "quiet": True,
        "no_warnings": True,
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])

    return tmp_path


def get_color_palette(pil_img: Image.Image, num_colors: int = 5):
    img_small = pil_img.resize((120, 120))
    arr = np.array(img_small)
    data = arr.reshape((-1, 3)).astype(np.float32)

    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
    flags = cv2.KMEANS_RANDOM_CENTERS

    _, labels, centers = cv2.kmeans(data, num_colors, None, criteria, 10, flags)
    centers = centers.astype(int)
    return [tuple(map(int, c)) for c in centers]


def make_palette_image(colors, width: int = PALETTE_WIDTH, height: int = PALETTE_HEIGHT):
    if not colors:
        return Image.new("RGB", (width, height), color="gray")
    bar = Image.new("RGB", (width, height))
    draw = ImageDraw.Draw(bar)
    n = len(colors)
    band_width = max(width // n, 1)
    for i, color in enumerate(colors):
        x0 = i * band_width
        x1 = width if i == n - 1 else (i + 1) * band_width
        draw.rectangle([x0, 0, x1, height], fill=color)
    return bar


def rgb_to_hex(rgb_tuple):
    r, g, b = rgb_tuple
    return "#{:02X}{:02X}{:02X}".format(r, g, b)


def analyze_single_image_zai(
    api_key: str,
    vision_model: str,
    index: int,
    img: Image.Image,
    limiter: RateLimiter,
) -> Dict[str, Any]:
    """
    ç”¨ Z.ai è§†è§‰æ¨¡å‹ï¼ˆå¦‚ glm-4.6vï¼‰åšå•å¸§åˆ†æã€‚
    æŒ‰å®˜æ–¹æ ¼å¼ï¼šmessages[0].content = [ {type:image_url,...}, {type:text,...} ] :contentReference[oaicite:7]{index=7}
    """
    prompt = f"""
ä½ ç°åœ¨æ˜¯ç”µå½±å¯¼æ¼” + æ‘„å½±æŒ‡å¯¼ + æœåŒ–é“æ€»ç›‘ + æç¤ºè¯å·¥ç¨‹å¸ˆã€‚
è¯·ä»”ç»†åˆ†æç»™ä½ çš„è¿™ä¸€å¸§ç”»é¢ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼ˆåªè¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šï¼‰ã€‚

å¿…é¡»ä½¿ç”¨ä¸‹é¢è¿™äº› keyï¼ˆè‹±æ–‡ï¼‰ï¼Œvalue å¤§éƒ¨åˆ†ä¸ºä¸­æ–‡è¯´æ˜ï¼Œè‹±æ–‡æç¤ºè¯å­—æ®µä¸ºè‹±æ–‡ï¼š

{{
  "index": {index},
  "scene_description_zh": "1-3å¥ä¸­æ–‡ï¼Œæå…·ä½“æè¿°äººç‰©+åŠ¨ä½œè·¯å¾„+åœºæ™¯å‰ä¸­åæ™¯+æœºä½è§†è§’ï¼›å¿½ç•¥UIæ–‡å­—",
  "tags_zh": ["#æ ‡ç­¾1","#æ ‡ç­¾2"],
  "camera": {{
    "shot_type_zh": "è¿œæ™¯/å…¨æ™¯/ä¸­æ™¯/è¿‘æ™¯/ç‰¹å†™",
    "shot_type": "wide/full/medium/close-up",
    "angle_zh": "ä¿¯æ‹/ä»°æ‹/å¹³è§†/ä¾§æ‹",
    "angle": "high/low/eye-level",
    "movement_zh": "æ¨è¿‘/è·Ÿæ‹/æ¨ªç§»/æ‰‹æŒ/ç”©é•œ",
    "movement": "dolly-in/handheld tracking/pan",
    "composition_zh": "ä¸‰åˆ†æ³•/ä¸­å¿ƒ/å¯¹ç§°/å‰æ™¯-ä¸»ä½“-èƒŒæ™¯",
    "composition": "rule-of-thirds/center/symmetry"
  }},
  "color_and_light_zh": "1-2å¥è‰²è°ƒ/å…‰çº¿/ä¸»å…‰æ–¹å‘/è½®å»“å…‰",
  "mood_zh": "æƒ…ç»ªæ°›å›´",
  "characters": [
    {{
      "role_zh": "èº«ä»½",
      "gender_zh": "æ€§åˆ«",
      "age_look_zh": "å¹´é¾„è§‚æ„Ÿ",
      "body_type_zh": "ä½“å‹",
      "clothing_zh": "æœè£…é¢œè‰²æ¬¾å¼",
      "hair_zh": "å‘å‹å‘è‰²",
      "expression_zh": "è¡¨æƒ…",
      "pose_body_zh": "å§¿æ€",
      "props_zh": "é“å…·"
    }}
  ],
  "character_action_detail_zh": "1-3å¥ï¼Œå¤´->æ‰‹->èº¯å¹²->è…¿ï¼Œå†™æ¸…æ¥è§¦ç‚¹",
  "face_expression_detail_zh": "1-3å¥ï¼Œçœ‰çœ¼å˜´ä¸‹é¢Œã€çœ¼ç¥ç»†èŠ‚ã€å¤–åŠ›å½¢å˜å›å¼¹",
  "cloth_hair_reaction_zh": "1-3å¥ï¼Œé£/æƒ¯æ€§å¯¹å¤´å‘è¡£æœå½±å“",
  "environment_detail_zh": "2-4å¥ï¼Œå‰æ™¯/ä¸­æ™¯/èƒŒæ™¯ï¼Œæè´¨ä¸ç©ºé—´ç»“æ„",
  "weather_force_detail_zh": "é£é›¨é›ª/æ°”æµ/å†²å‡»æ³¢æ–¹å‘ä¸åé¦ˆï¼ˆæ— åˆ™å†™æ— ï¼‰",
  "props_and_tech_detail_zh": "1-3å¥ï¼Œåˆ—å‡ºå…³é”®é“å…·ä¸ä½ç½®çŠ¶æ€",
  "physics_reaction_detail_zh": "å—åŠ›/å½¢å˜/å›å¼¹è¿‡ç¨‹ï¼ˆæ— åˆ™å†™æ— ï¼‰",
  "structure_damage_detail_zh": "ç»“æ„æŸåï¼ˆæ— åˆ™å†™æ— ï¼‰",
  "debris_motion_detail_zh": "ç¢ç‰‡é£æ•£è½¨è¿¹ï¼ˆæ— åˆ™å†™æ— ï¼‰",
  "motion_detail_zh": "ä¸Šä¸€ç¬é—´->å½“å‰->ä¸‹ä¸€ç¬é—´åŠ¨ä½œæ¨æ–­",
  "fx_detail_zh": "ç«èŠ±çƒŸå°˜ç²’å­ï¼ˆæ— åˆ™å†™æ— ï¼‰",
  "lighting_color_detail_zh": "æ›´ç²¾ç»†å…‰æºæ•°é‡æ–¹å‘è‰²æ¸©å·®",
  "audio_cue_detail_zh": "ç¯å¢ƒå£°+ç‰¹æ•ˆå£°+BGMèŠ‚å¥ç‚¹",
  "edit_rhythm_detail_zh": "å‰ªè¾‘èŠ‚å¥/æ…¢åŠ¨ä½œ/é—ªç™½ç­‰",
  "midjourney_prompt": "ä¸€è¡Œè‹±æ–‡MJæç¤ºè¯",
  "midjourney_negative_prompt": "ä¸€è¡Œè‹±æ–‡è´Ÿé¢è¯",
  "video_prompt_en": "3-5å¥è‹±æ–‡è§†é¢‘æç¤ºè¯ï¼Œæœ€åä¸€å¥å†™ï¼š'4 second shot, vertical 9:16, 24fps, cinematic, highly detailed.'"
}}
""".strip()

    try:
        data_url = pil_to_data_url(img)
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": data_url}},
                    {"type": "text", "text": prompt},
                ],
            }
        ]
        text = zai_chat_completions(
            api_key=api_key,
            model=vision_model,
            messages=messages,
            temperature=0.4,
            max_tokens=2600,
            thinking_type="disabled",
            rate_limiter=limiter,
        )
        info = extract_json_object(text)

        # ä¿åº•å­—æ®µ
        info["index"] = index
        info.setdefault("tags_zh", [])
        info.setdefault("camera", {})
        cam = info["camera"]
        for k in ["shot_type_zh","shot_type","angle_zh","angle","movement_zh","movement","composition_zh","composition"]:
            cam.setdefault(k, "")

        for k in [
            "scene_description_zh","color_and_light_zh","mood_zh","characters",
            "character_action_detail_zh","face_expression_detail_zh","cloth_hair_reaction_zh",
            "environment_detail_zh","weather_force_detail_zh","props_and_tech_detail_zh",
            "physics_reaction_detail_zh","structure_damage_detail_zh","debris_motion_detail_zh",
            "motion_detail_zh","fx_detail_zh","lighting_color_detail_zh","audio_cue_detail_zh",
            "edit_rhythm_detail_zh","midjourney_prompt","midjourney_negative_prompt","video_prompt_en"
        ]:
            info.setdefault(k, "" if k != "characters" else [])

        return info

    except Exception as e:
        return {
            "index": index,
            "scene_description_zh": f"ï¼ˆAI åˆ†æå¤±è´¥ï¼š{e}ï¼‰",
            "tags_zh": [],
            "camera": {
                "shot_type_zh": "", "shot_type": "",
                "angle_zh": "", "angle": "",
                "movement_zh": "", "movement": "",
                "composition_zh": "", "composition": "",
            },
            "color_and_light_zh": "",
            "mood_zh": "",
            "characters": [],
            "character_action_detail_zh": "",
            "face_expression_detail_zh": "",
            "cloth_hair_reaction_zh": "",
            "environment_detail_zh": "",
            "weather_force_detail_zh": "",
            "props_and_tech_detail_zh": "",
            "physics_reaction_detail_zh": "",
            "structure_damage_detail_zh": "",
            "debris_motion_detail_zh": "",
            "motion_detail_zh": "",
            "fx_detail_zh": "",
            "lighting_color_detail_zh": "",
            "audio_cue_detail_zh": "",
            "edit_rhythm_detail_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
            "video_prompt_en": "",
        }


def analyze_images_concurrently_zai(
    api_key: str,
    vision_model: str,
    images: List[Image.Image],
    max_ai_frames: int,
    limiter: RateLimiter,
    max_workers: int = 4,
) -> List[Dict[str, Any]]:
    n = len(images)
    if n == 0:
        return []
    use_n = min(max_ai_frames, n)
    results: List[Dict[str, Any]] = [None] * n  # type: ignore

    status = st.empty()
    status.info(f"æ­£åœ¨å¯¹å‰ {use_n} å¸§è¿›è¡Œ AI åˆ†æï¼ˆå…± {n} å¸§ï¼‰â€¦")

    max_workers = max(1, min(int(max_workers), 8, use_n))
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {
            ex.submit(analyze_single_image_zai, api_key, vision_model, i + 1, images[i], limiter): i
            for i in range(use_n)
        }
        for fut in concurrent.futures.as_completed(futures):
            i = futures[fut]
            results[i] = fut.result()

    for i in range(use_n, n):
        results[i] = {
            "index": i + 1,
            "scene_description_zh": "ï¼ˆæœ¬å¸§æœªåš AI åˆ†æï¼Œç”¨äºèŠ‚çœé…é¢ï¼Œä½†ä»å¯ç”¨äºè§†è§‰å‚è€ƒå’Œè‰²å¡ã€‚ï¼‰",
            "tags_zh": [],
            "camera": {"shot_type_zh":"","shot_type":"","angle_zh":"","angle":"","movement_zh":"","movement":"","composition_zh":"","composition":""},
            "color_and_light_zh": "",
            "mood_zh": "",
            "characters": [],
            "character_action_detail_zh": "",
            "face_expression_detail_zh": "",
            "cloth_hair_reaction_zh": "",
            "environment_detail_zh": "",
            "weather_force_detail_zh": "",
            "props_and_tech_detail_zh": "",
            "physics_reaction_detail_zh": "",
            "structure_damage_detail_zh": "",
            "debris_motion_detail_zh": "",
            "motion_detail_zh": "",
            "fx_detail_zh": "",
            "lighting_color_detail_zh": "",
            "audio_cue_detail_zh": "",
            "edit_rhythm_detail_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
            "video_prompt_en": "",
        }

    status.empty()
    return results


def analyze_overall_video_zai(frame_infos: List[Dict[str, Any]], api_key: str, text_model: str, limiter: RateLimiter) -> str:
    described = [
        info for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆæ•´ä½“å‰§æƒ…å¤§çº²ã€‚ï¼‰"

    parts = []
    for info in described:
        cam = info.get("camera", {}) or {}
        tags = info.get("tags_zh", []) or []
        parts.append(
            f"ç¬¬ {info['index']} å¸§ï¼š{info.get('scene_description_zh','')}\n"
            f"æ™¯åˆ«ï¼š{cam.get('shot_type_zh','')}ï¼›è§’åº¦ï¼š{cam.get('angle_zh','')}ï¼›è¿é•œï¼š{cam.get('movement_zh','')}ï¼›æ„å›¾ï¼š{cam.get('composition_zh','')}\n"
            f"è‰²å½©ä¸å…‰å½±ï¼š{info.get('color_and_light_zh','')}\n"
            f"æƒ…ç»ªæ°›å›´ï¼š{info.get('mood_zh','')}\n"
            f"æ ‡ç­¾ï¼š{'ã€'.join(tags)}"
        )
    joined = "\n\n".join(parts)

    prompt = f"""
ä½ ç°åœ¨æ˜¯èµ„æ·±è§†é¢‘å¯¼æ¼” + å‰ªè¾‘å¸ˆ + çŸ­è§†é¢‘è¿è¥ä¸“å®¶ + å†…å®¹åˆè§„å®¡æ ¸å‘˜ã€‚
ä¸‹é¢æ˜¯ä»ä¸€æ®µè§†é¢‘ä¸­æŠ½å–çš„è‹¥å¹²å…³é”®å¸§çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›è¯´æ˜ï¼Œå¯¹æ•´æ®µè§†é¢‘åšæ•´ä½“åˆ†æã€‚

=== å¸§çº§è¯´æ˜å¼€å§‹ ===
{joined}
=== å¸§çº§è¯´æ˜ç»“æŸ ===

è¯·ä¸¥æ ¼æŒ‰ä¸‹é¢ç»“æ„è¾“å‡ºä¸­æ–‡åˆ†æï¼š

ã€å‰§æƒ…å¤§çº²ã€‘
ç”¨ 2-4 å¥æ¦‚æ‹¬è¿™æ®µè§†é¢‘çš„å¤§è‡´å†…å®¹/äººç‰©å…³ç³»/å‘ç”Ÿåœºæ™¯ã€‚

ã€æ•´ä½“è§†å¬é£æ ¼ã€‘
ä»èŠ‚å¥å¿«æ…¢ã€é•œå¤´æ„Ÿã€è‰²å½©æ°”è´¨ï¼ˆæš–/å†·/æ—¥å¸¸/æ¢¦å¹»ï¼‰ã€æƒ…ç»ªæ°›å›´ç­‰è§’åº¦æ€»ç»“æ•´ä½“é£æ ¼ã€‚

ã€é€‚åˆçš„è¯é¢˜æ ‡ç­¾ã€‘
ç”¨ #æ ‡ç­¾ å½¢å¼ç»™å‡º 5-10 ä¸ªã€‚

ã€å•†ä¸šä¸åˆè§„é£é™©ã€‘
æ•´ä½“é£é™©çº§åˆ«ï¼šä½ / ä¸­ / é«˜
å¹¶ç”¨ 2-3 å¥è¯è¯´æ˜éœ€è¦æ³¨æ„çš„ç‚¹ã€‚

åªè¾“å‡ºä»¥ä¸Š 4 ä¸ªå°èŠ‚ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
""".strip()

    return zai_chat_completions(
        api_key=api_key,
        model=text_model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=1800,
        thinking_type="disabled",
        rate_limiter=limiter,
    )


def generate_ad_script_zai(frame_infos: List[Dict[str, Any]], api_key: str, text_model: str, limiter: RateLimiter) -> str:
    described = [
        info for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆå¹¿å‘Šæ—ç™½è„šæœ¬ã€‚ï¼‰"

    joined = "\n".join([f"ç¬¬ {i['index']} å¸§ï¼š{i.get('scene_description_zh','')}" for i in described])

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±å¹¿å‘Šå¯¼æ¼” + æ–‡æ¡ˆã€‚
æˆ‘æœ‰ä¸€ä¸ªç”±è‹¥å¹²ç”»é¢ç»„æˆçš„ç«–ç‰ˆçŸ­è§†é¢‘ï¼Œæ—¶é•¿å¤§çº¦ 8-12 ç§’ã€‚
ä¸‹é¢æ˜¯æ¯ä¸ªç”»é¢çš„ç®€è¦è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›ä¿¡æ¯ï¼Œå†™ä¸€æ¡é€‚åˆé…åˆè¿™äº›ç”»é¢æ’­æ”¾çš„ä¸­æ–‡å¹¿å‘Šæ—ç™½è„šæœ¬ã€‚

=== å…³é”®å¸§æ¦‚è§ˆ ===
{joined}
=== å…³é”®å¸§æ¦‚è§ˆç»“æŸ ===

è¦æ±‚ï¼š
1) æ—ç™½æ€»æ—¶é•¿æ§åˆ¶åœ¨ 8-12 ç§’å·¦å³ï¼ˆæ­£å¸¸è¯­é€Ÿï¼‰ï¼Œæ–‡æœ¬ 35-70 å­—ï¼›
2) è‡ªç„¶å£è¯­åŒ–ä¸­æ–‡ï¼Œä¸è¦å‡ºç°â€œç”»é¢ä¸­â€â€œé•œå¤´é‡Œâ€ï¼›
3) ä¸ç”»é¢è°ƒæ€§åŒ¹é…ã€‚

æŒ‰ä¸‹é¢æ ¼å¼è¾“å‡ºï¼š

ã€10ç§’å¹¿å‘Šæ—ç™½è„šæœ¬ã€‘
ï¼ˆå®Œæ•´ä¸€æ®µæ—ç™½ï¼‰

ä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹ã€‚
""".strip()

    return zai_chat_completions(
        api_key=api_key,
        model=text_model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=600,
        thinking_type="disabled",
        rate_limiter=limiter,
    )


def generate_timeline_shotlist(frame_infos: List[Dict[str, Any]], used_range: Tuple[float, float]) -> str:
    n = len(frame_infos)
    if n == 0:
        return "ï¼ˆæš‚æ— å…³é”®å¸§ï¼Œæ— æ³•ç”Ÿæˆæ—¶é—´è½´åˆ†é•œè„šæœ¬ã€‚ï¼‰"

    start_used, end_used = used_range
    total_len = max(0.1, end_used - start_used)
    seg = total_len / n

    lines: List[str] = []
    for i, info in enumerate(frame_infos):
        t0 = i * seg
        t1 = total_len if i == n - 1 else (i + 1) * seg
        shot_id = f"S{i+1:02d}"

        cam = info.get("camera", {}) or {}
        tags = info.get("tags_zh", []) or []

        def g(k: str) -> str:
            v = info.get(k, "")
            return (v or "").strip() if isinstance(v, str) else ""

        block = [f"ã€{shot_id} | {t0:.1f}-{t1:.1f} ç§’ã€‘"]
        if g("scene_description_zh"): block.append(f"ç”»é¢å†…å®¹ï¼š{g('scene_description_zh')}")
        if g("character_action_detail_zh"): block.append(f"äººç‰©åŠ¨ä½œï¼š{g('character_action_detail_zh')}")
        if g("face_expression_detail_zh"): block.append(f"é¢éƒ¨ä¸çœ¼ç¥ï¼š{g('face_expression_detail_zh')}")
        if g("cloth_hair_reaction_zh"): block.append(f"æœè£…ä¸å¤´å‘ï¼š{g('cloth_hair_reaction_zh')}")
        if g("environment_detail_zh"): block.append(f"åœºæ™¯ä¸ç©ºé—´ï¼š{g('environment_detail_zh')}")
        if g("weather_force_detail_zh"): block.append(f"å¤©æ°”ä¸ç¯å¢ƒåŠ›ï¼š{g('weather_force_detail_zh')}")
        if g("props_and_tech_detail_zh"): block.append(f"é“å…·ä¸ç§‘æŠ€ï¼š{g('props_and_tech_detail_zh')}")
        if g("structure_damage_detail_zh"): block.append(f"ç»“æ„æŸåï¼š{g('structure_damage_detail_zh')}")
        if g("debris_motion_detail_zh"): block.append(f"ç¢ç‰‡ä¸é£æ•£è½¨è¿¹ï¼š{g('debris_motion_detail_zh')}")
        if g("physics_reaction_detail_zh"): block.append(f"å—åŠ›ä¸ç‰©ç†åé¦ˆï¼š{g('physics_reaction_detail_zh')}")
        if g("fx_detail_zh"): block.append(f"ç‰¹æ•ˆä¸ç²’å­ï¼š{g('fx_detail_zh')}")
        if g("lighting_color_detail_zh"): block.append(f"å…‰çº¿ä¸è‰²å½©ï¼š{g('lighting_color_detail_zh')}")

        cam_desc = []
        if cam.get("shot_type_zh"): cam_desc.append(f"æ™¯åˆ«ï¼š{cam.get('shot_type_zh')}")
        if cam.get("angle_zh"): cam_desc.append(f"è§’åº¦ï¼š{cam.get('angle_zh')}")
        if cam.get("movement_zh"): cam_desc.append(f"è¿é•œï¼š{cam.get('movement_zh')}")
        if cam.get("composition_zh"): cam_desc.append(f"æ„å›¾ï¼š{cam.get('composition_zh')}")
        if cam_desc: block.append("æœºä½ä¸è¿åŠ¨ï¼š" + "ï¼›".join(cam_desc))

        if g("mood_zh"): block.append(f"æƒ…ç»ªæ°›å›´ï¼š{g('mood_zh')}")
        if g("motion_detail_zh"): block.append(f"åŠ¨ä½œè¶‹åŠ¿ï¼š{g('motion_detail_zh')}")
        if g("audio_cue_detail_zh"): block.append(f"å£°éŸ³ä¸èŠ‚å¥ï¼š{g('audio_cue_detail_zh')}")
        if g("edit_rhythm_detail_zh"): block.append(f"å‰ªè¾‘ä¸èŠ‚å¥ï¼š{g('edit_rhythm_detail_zh')}")
        if tags: block.append("æ ‡ç­¾ï¼š" + " ".join(tags))

        lines.append("\n".join(block))

    return "\n\n".join(lines)


# =========================================================
# Streamlit UI
# =========================================================
st.set_page_config(page_title="æ™ºè°±/GLM åˆ†é•œ & è§†é¢‘åˆ†æå·¥å…·", page_icon="ğŸ¬", layout="wide")

# Session State
if "analysis_history" not in st.session_state:
    st.session_state["analysis_history"] = []

st.markdown(
    """
    <style>
    .main { background-color: #0f172a; color: #e5e7eb; }
    .stMarkdown, .stText, label, p, div { color: #e5e7eb !important; }
    .stCode { font-size: 0.85rem !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    """
    <div style="
        padding: 18px 24px;
        border-radius: 18px;
        margin-bottom: 16px;
        background: radial-gradient(circle at top left, #38bdf8 0, #0f172a 45%, #020617 100%);
        border: 1px solid rgba(148, 163, 184, 0.35);
    ">
      <h1 style="margin: 0 0 8px 0; color: #e5e7eb; font-size: 1.6rem;">
        ğŸ¬ æ™ºè°±/GLMï¼šåˆ†é•œç”Ÿæˆ + è§†é¢‘å…³é”®å¸§åˆ†æï¼ˆå…SDKç‰ˆï¼‰
      </h1>
      <p style="margin: 0; color: #cbd5f5; font-size: 0.96rem;">
        ç»Ÿä¸€èµ° Z.ai HTTP APIï¼šåˆ†é•œJSONç”Ÿæˆ / è§†é¢‘æŠ½å¸§ / è§†è§‰åˆ†æ / æ—¶é—´è½´è„šæœ¬ / å†å²è®°å½•ä¸ä¸‹è½½ã€‚
      </p>
    </div>
    """,
    unsafe_allow_html=True,
)

with st.sidebar:
    st.header("ğŸ”‘ API é…ç½®ï¼ˆZ.ai / æ™ºè°±ï¼‰")
    api_key_env = os.getenv("ZAI_API_KEY", "").strip()
    api_key = st.text_input("ZAI_API_KEYï¼ˆä¼˜å…ˆç”¨ç¯å¢ƒå˜é‡ï¼‰", type="password", value=api_key_env)

    st.markdown("---")
    text_model = st.text_input("æ–‡æœ¬æ¨¡å‹ï¼ˆç”¨äºï¼šåˆ†é•œ/æ€»ç»“/å¹¿å‘Šæ–‡æ¡ˆï¼‰", value=DEFAULT_TEXT_MODEL)
    vision_model = st.text_input("è§†è§‰æ¨¡å‹ï¼ˆç”¨äºï¼šå¸§åˆ†æï¼‰", value=DEFAULT_VISION_MODEL)

    st.markdown("---")
    rpm = st.slider("æ¯åˆ†é’Ÿæœ€å¤§è°ƒç”¨æ¬¡æ•°ï¼ˆé™æµï¼‰", 1, 60, 10, 1)
    limiter = RateLimiter(rpm=rpm)

    st.markdown("---")
    max_ai_frames = st.slider("æœ¬æ¬¡æœ€å¤šåš AI åˆ†æçš„å¸§æ•°", 1, 20, 10, 1)
    max_workers = st.slider("å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®® 2-4ï¼‰", 1, 8, 3, 1)

    st.markdown("---")
    st.markdown("â± åˆ†ææ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰")
    start_sec = st.number_input("ä»ç¬¬å‡ ç§’å¼€å§‹ï¼ˆå«ï¼‰", min_value=0.0, value=0.0, step=0.5)
    end_sec = st.number_input("åˆ°ç¬¬å‡ ç§’ç»“æŸï¼ˆ0 æˆ– â‰¤å¼€å§‹ç§’ è¡¨ç¤ºç›´åˆ°ç»“å°¾ï¼‰", min_value=0.0, value=0.0, step=0.5)

    if not api_key:
        st.warning("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZAI_API_KEYï¼Œæˆ–åœ¨æ­¤ç²˜è´´ Keyã€‚")
    else:
        st.success("Key å·²å°±ç»ªã€‚")


tab_storyboard, tab_video = st.tabs(["ğŸ§© åˆ†é•œ JSON ç”Ÿæˆï¼ˆæ–‡æœ¬ï¼‰", "ğŸ è§†é¢‘å…³é”®å¸§åˆ†æï¼ˆè§†è§‰ï¼‰"])

# -------------------------------
# Tab 1ï¼šåˆ†é•œ JSON ç”Ÿæˆ
# -------------------------------
with tab_storyboard:
    st.subheader("ğŸ§© åˆ†é•œ + å£æ’­æ–‡æ¡ˆ + è‹±æ–‡å‡ºå›¾æç¤ºè¯ï¼ˆJSONï¼‰")

    col1, col2 = st.columns(2)
    with col1:
        brand = st.text_input("å“ç‰Œï¼ˆå¿…å¡«ï¼‰", value="é‚µè­¦ç§˜å¤")
        product = st.text_input("äº§å“ï¼ˆå¿…å¡«ï¼‰", value="å¤é¸­è„–+å¤é¸­ç¿… å¤œå®µå¥—é¤")
        duration_sec = st.number_input("è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰", min_value=5, max_value=120, value=15, step=1)
    with col2:
        style = st.text_area("æ•´ä½“é£æ ¼ï¼ˆä¸­æ–‡æè¿°ï¼‰", value="çƒŸç«æ°”ã€å¤œå®µæ¡£ã€çœŸå®è¡—è¾¹é£æ ¼ï¼Œæœ‰ç‚¹å¹½é»˜ï¼Œé€‚åˆæŠ–éŸ³", height=110)

    if st.button("âœ¨ ç”Ÿæˆåˆ†é•œ & æ–‡æ¡ˆï¼ˆèµ°æ™ºè°±/GLMï¼‰", type="primary", key="btn_story"):
        if not api_key:
            st.error("è¯·å…ˆé…ç½® ZAI_API_KEY")
        elif not brand or not product:
            st.error("è¯·å…ˆå¡«å†™å“ç‰Œå’Œäº§å“")
        else:
            with st.spinner("æ­£åœ¨è°ƒç”¨æ™ºè°±/GLMç”Ÿæˆåˆ†é•œâ€¦"):
                try:
                    data = generate_storyboard_zai(api_key, text_model, brand, product, int(duration_sec), style, limiter)
                except Exception as e:
                    st.error(f"ç”Ÿæˆå¤±è´¥ï¼š{e}")
                else:
                    st.success("ç”Ÿæˆå®Œæˆï¼")
                    st.subheader("ğŸ“œ åˆ†é•œ JSON")
                    st.json(data)

                    voice_script = extract_voiceover(data)
                    st.subheader("ğŸ™ æ—ç™½è„šæœ¬")
                    st.text_area("å¯å¤åˆ¶ç»™é…éŸ³ç”¨", value=voice_script, height=220)

                    st.download_button(
                        "ä¸‹è½½ storyboard.json",
                        data=json.dumps(data, ensure_ascii=False, indent=2),
                        file_name="storyboard.json",
                        mime="application/json",
                    )
                    st.download_button(
                        "ä¸‹è½½ voiceover_script.txt",
                        data=voice_script,
                        file_name="voiceover_script.txt",
                        mime="text/plain",
                    )

# -------------------------------
# Tab 2ï¼šè§†é¢‘å…³é”®å¸§åˆ†æ
# -------------------------------
with tab_video:
    st.subheader("ğŸ ä¸Šä¼ /é“¾æ¥ â†’ æŠ½å…³é”®å¸§ â†’ è§†è§‰åˆ†æ â†’ æ—¶é—´è½´è„šæœ¬ â†’ JSON/å†å²è®°å½•")

    source_mode = st.radio(
        "ğŸ“¥ é€‰æ‹©è§†é¢‘æ¥æº",
        ["ä¸Šä¼ æœ¬åœ°æ–‡ä»¶", "è¾“å…¥ç½‘ç»œè§†é¢‘é“¾æ¥ï¼ˆæŠ–éŸ³ / Bç«™ / TikTok / YouTubeï¼‰"],
        index=0,
        horizontal=True,
    )
    video_url: Optional[str] = None
    uploaded_file = None

    if source_mode == "ä¸Šä¼ æœ¬åœ°æ–‡ä»¶":
        uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼ˆå»ºè®® < 50MBï¼‰", type=["mp4", "mov", "m4v", "avi", "mpeg"])
    else:
        video_url = st.text_input("ğŸ”— è¾“å…¥è§†é¢‘é“¾æ¥", placeholder="ä¾‹å¦‚ï¼šhttps://v.douyin.com/xxxxxx æˆ– https://www.youtube.com/watch?v=...")

    if st.button("ğŸš€ ä¸€é”®è§£ææ•´æ¡è§†é¢‘ï¼ˆèµ°æ™ºè°±/GLMè§†è§‰ï¼‰", type="primary", key="btn_video"):
        if not api_key:
            st.error("è¯·å…ˆé…ç½® ZAI_API_KEY")
            st.stop()

        tmp_path: Optional[str] = None
        source_label = ""
        source_type = ""

        try:
            if source_mode == "ä¸Šä¼ æœ¬åœ°æ–‡ä»¶":
                source_type = "upload"
                if not uploaded_file:
                    st.error("è¯·å…ˆä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚")
                    st.stop()
                suffix = os.path.splitext(uploaded_file.name)[1] or ".mp4"
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                    tmp.write(uploaded_file.read())
                    tmp_path = tmp.name
                source_label = uploaded_file.name
            else:
                source_type = "url"
                if not video_url:
                    st.error("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„è§†é¢‘é“¾æ¥ã€‚")
                    st.stop()
                st.info("ğŸŒ æ­£åœ¨ä»ç½‘ç»œä¸‹è½½è§†é¢‘ï¼Œè¯·ç¨å€™â€¦")
                tmp_path = download_video_from_url(video_url)
                source_label = video_url

            st.info("â³ æ­£åœ¨æŠ½å–å…³é”®å¸§â€¦")
            images, duration, used_range = extract_keyframes_dynamic(
                tmp_path,
                start_sec=float(start_sec),
                end_sec=float(end_sec) if end_sec and end_sec > 0 else None,
            )
            start_used, end_used = used_range

            try:
                if tmp_path:
                    os.remove(tmp_path)
            except OSError:
                pass

            if not images:
                st.error("æ— æ³•ä»è§†é¢‘ä¸­è¯»å–å¸§ï¼Œè¯·æ£€æŸ¥æ ¼å¼æˆ–æ–‡ä»¶æ˜¯å¦æŸåã€‚")
                st.stop()

            st.success(f"âœ… æŠ½å– {len(images)} å¸§ï¼ˆè§†é¢‘æ€»é•¿çº¦ {duration:.1f}sï¼›åˆ†æåŒºé—´ {start_used:.1f}-{end_used:.1f}sï¼‰")

            # è‰²å¡
            frame_palettes: List[List[Tuple[int, int, int]]] = []
            for img in images:
                try:
                    frame_palettes.append(get_color_palette(img, num_colors=5))
                except Exception:
                    frame_palettes.append([])

            # å¸§åˆ†æï¼ˆè§†è§‰ï¼‰
            with st.spinner("ğŸ§  æ­£åœ¨è°ƒç”¨è§†è§‰æ¨¡å‹åˆ†æå…³é”®å¸§â€¦"):
                frame_infos = analyze_images_concurrently_zai(
                    api_key=api_key,
                    vision_model=vision_model,
                    images=images,
                    frame_infos = analyze_images_concurrently_zai(
    api_key=api_key,
    vision_model=vision_model,
    images=images,
    max_ai_frames=int(max_ai_frames),
    limiter=limiter,
    max_workers=int(max_workers),
)
                    ,
                    limiter=limiter,
                    max_workers=int(max_workers),
                )

            # æ•´ä½“æ€»ç»“ + å¹¿å‘Šæ—ç™½ + æ—¶é—´è½´ï¼ˆæ–‡æœ¬ï¼‰
            with st.spinner("ğŸ“š æ•´ä½“å‰§æƒ…æ€»ç»“â€¦"):
                overall = analyze_overall_video_zai(frame_infos, api_key, text_model, limiter)
            with st.spinner("ğŸ¤ 10ç§’å¹¿å‘Šæ—ç™½â€¦"):
                ad_script = generate_ad_script_zai(frame_infos, api_key, text_model, limiter)
            with st.spinner("ğŸ¬ æ—¶é—´è½´åˆ†é•œè„šæœ¬â€¦"):
                timeline_shotlist = generate_timeline_shotlist(frame_infos, used_range=used_range)

            export_frames = []
            for info, palette in zip(frame_infos, frame_palettes):
                export_frames.append({
                    **info,
                    "palette_rgb": [list(c) for c in (palette or [])],
                    "palette_hex": [rgb_to_hex(c) for c in (palette or [])],
                })

            export_data = {
                "meta": {
                    "text_model": text_model,
                    "vision_model": vision_model,
                    "frame_count": len(images),
                    "max_ai_frames_this_run": int(max_ai_frames),
                    "duration_sec_est": float(duration),
                    "start_sec_used": float(start_used),
                    "end_sec_used": float(end_used),
                    "source_type": source_type,
                    "source_label": source_label,
                    "api_base_url": ZAI_BASE_URL,
                },
                "frames": export_frames,
                "overall_analysis": overall,
                "ad_script_10s": ad_script,
                "timeline_shotlist_zh": timeline_shotlist,
            }
            json_str = json.dumps(export_data, ensure_ascii=False, indent=2)

            # å†å²è®°å½•
            history = st.session_state["analysis_history"]
            run_id = f"run_{len(history) + 1}"
            history.append({
                "id": run_id,
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "meta": export_data["meta"],
                "data": export_data,
            })
            st.session_state["analysis_history"] = history

            tab_frames, tab_story, tab_json, tab_history = st.tabs(
                ["ğŸ å…³é”®å¸§ & æç¤ºè¯", "ğŸ“š æ€»ç»“ & å¹¿å‘Š & æ—¶é—´è½´", "ğŸ“¦ JSON å¯¼å‡ºï¼ˆæœ¬æ¬¡ï¼‰", "ğŸ•˜ å†å²è®°å½•"]
            )

            with tab_frames:
                st.markdown(f"å…± **{len(images)}** å¸§ï¼›å…¶ä¸­å‰ **{min(len(images), int(max_ai_frames))}** å¸§åšäº† AI è§†è§‰åˆ†æã€‚")
                st.markdown("---")

                for i, (img, info, palette) in enumerate(zip(images, frame_infos, frame_palettes)):
                    st.markdown(f"### ğŸ“˜ å…³é”®å¸§ {i+1}")
                    c1, c2 = st.columns([1.2, 2])

                    with c1:
                        st.image(img, caption=f"ç¬¬ {i+1} å¸§ç”»é¢", width=DISPLAY_IMAGE_WIDTH)
                        palette_img = make_palette_image(palette)
                        st.image(palette_img, caption="ä¸»è‰²è°ƒè‰²å¡", width=PALETTE_WIDTH)
                        if palette:
                            st.caption("ä¸»è‰² HEXï¼š" + ", ".join(rgb_to_hex(c) for c in palette))

                    with c2:
                        cam = info.get("camera", {}) or {}
                        tags = info.get("tags_zh", []) or []
                        analysis_text = "\n".join([
                            f"ã€æ™¯åˆ«ã€‘{cam.get('shot_type_zh','')}",
                            f"ã€è¿é•œã€‘{cam.get('movement_zh','')}",
                            f"ã€æ‹æ‘„è§’åº¦ã€‘{cam.get('angle_zh','')}",
                            f"ã€æ„å›¾ã€‘{cam.get('composition_zh','')}",
                            f"ã€è‰²å½©ä¸å…‰å½±ã€‘{info.get('color_and_light_zh','')}",
                            f"ã€ç”»é¢å†…å®¹ã€‘{info.get('scene_description_zh','')}",
                            f"ã€æƒ…ç»ªæ°›å›´ã€‘{info.get('mood_zh','')}",
                            f"ã€å…³é”®è¯æ ‡ç­¾ã€‘{' '.join(tags)}",
                        ]).strip()

                        st.markdown("**åˆ†é•œåˆ†æï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                        st.code(analysis_text or "ï¼ˆæš‚æ— ï¼‰", language="markdown")

                        st.markdown("**äººç‰©åŠ¨ä½œç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                        st.code(info.get("character_action_detail_zh") or "ï¼ˆæš‚æ— ï¼‰", language="markdown")

                        st.markdown("**åœºæ™¯ç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                        scene_detail = (info.get("environment_detail_zh") or "").strip()
                        props_detail = (info.get("props_and_tech_detail_zh") or "").strip()
                        st.code((scene_detail + "\n\né“å…·ä¸ç§‘æŠ€å…ƒç´ ï¼š" + props_detail).strip() or "ï¼ˆæš‚æ— ï¼‰", language="markdown")

                        advanced = []
                        for title, key in [
                            ("é¢éƒ¨ä¸çœ¼ç¥", "face_expression_detail_zh"),
                            ("æœè£…ä¸å¤´å‘", "cloth_hair_reaction_zh"),
                            ("å¤©æ°”ä¸ç¯å¢ƒåŠ›", "weather_force_detail_zh"),
                            ("å—åŠ›ä¸ç‰©ç†åé¦ˆ", "physics_reaction_detail_zh"),
                            ("ç»“æ„æŸå", "structure_damage_detail_zh"),
                            ("ç¢ç‰‡é£æ•£", "debris_motion_detail_zh"),
                            ("ç‰¹æ•ˆä¸ç²’å­", "fx_detail_zh"),
                            ("å…‰çº¿ç»†èŠ‚", "lighting_color_detail_zh"),
                            ("å£°éŸ³ä¸èŠ‚å¥", "audio_cue_detail_zh"),
                            ("å‰ªè¾‘èŠ‚å¥", "edit_rhythm_detail_zh"),
                        ]:
                            v = (info.get(key) or "").strip()
                            if v:
                                advanced.append(f"{v}")
                        if advanced:
                            st.markdown("**é«˜çº§ç‰©ç† / ç¯å¢ƒç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                            st.code("\n".join(advanced), language="markdown")

                        st.markdown("**SORA / VEO è§†é¢‘æç¤ºè¯ï¼ˆè‹±æ–‡ï¼‰ï¼š**")
                        st.code(info.get("video_prompt_en") or "ï¼ˆæš‚æ— ï¼‰", language="markdown")

                        st.markdown("**Midjourney é™å¸§æç¤ºè¯ï¼š**")
                        st.code(info.get("midjourney_prompt") or "ï¼ˆæš‚æ— ï¼‰", language="markdown")

                    st.markdown("---")

            with tab_story:
                st.markdown("### ğŸ“š æ•´ä½“å‰§æƒ…ä¸è§†å¬é£æ ¼æ€»ç»“")
                st.code(overall, language="markdown")
                st.markdown("### ğŸ¤ 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬")
                st.code(ad_script, language="markdown")
                st.markdown("### ğŸ¬ æ—¶é—´è½´åˆ†é•œè„šæœ¬")
                st.code(timeline_shotlist, language="markdown")

            with tab_json:
                st.download_button("â¬‡ï¸ ä¸‹è½½æœ¬æ¬¡ video_analysis.json", data=json_str, file_name="video_analysis.json", mime="application/json")
                with st.expander("ğŸ” é¢„è§ˆ JSONï¼ˆå‰ 3000 å­—ç¬¦ï¼‰"):
                    st.code(json_str[:3000] + ("\n...\n" if len(json_str) > 3000 else ""), language="json")

            with tab_history:
                history = st.session_state.get("analysis_history", [])
                if not history:
                    st.info("å½“å‰ä¼šè¯è¿˜æ²¡æœ‰å†å²è®°å½•ã€‚")
                else:
                    options = [
                        f"{len(history)-i}. {h['created_at']} | {h['meta'].get('source_label','')} | "
                        f"{h['meta'].get('frame_count',0)} å¸§ | åŒºé—´ {h['meta'].get('start_sec_used',0):.1f}-{h['meta'].get('end_sec_used',0):.1f}s"
                        for i, h in enumerate(reversed(history))
                    ]
                    idx_display = st.selectbox("é€‰æ‹©ä¸€æ¡å†å²è®°å½•æŸ¥çœ‹", options=list(range(len(history))), format_func=lambda i: options[i])
                    real_index = len(history) - 1 - idx_display
                    selected = history[real_index]

                    st.markdown(
                        f"**IDï¼š** `{selected['id']}`  \n"
                        f"**æ—¶é—´ï¼š** {selected['created_at']}  \n"
                        f"**æ¥æºç±»å‹ï¼š** {selected['meta'].get('source_type','')}  \n"
                        f"**æ¥æºæ ‡è¯†ï¼š** {selected['meta'].get('source_label','')}  \n"
                        f"**åˆ†æåŒºé—´ï¼š** {selected['meta'].get('start_sec_used',0):.1f}â€“{selected['meta'].get('end_sec_used',0):.1f} ç§’  \n"
                        f"**å¸§æ•°ï¼š** {selected['meta'].get('frame_count',0)}  \n"
                        f"**æ–‡æœ¬æ¨¡å‹ï¼š** {selected['meta'].get('text_model','')}  \n"
                        f"**è§†è§‰æ¨¡å‹ï¼š** {selected['meta'].get('vision_model','')}"
                    )

                    hist_json = json.dumps(selected["data"], ensure_ascii=False, indent=2)
                    st.download_button("â¬‡ï¸ ä¸‹è½½è¯¥å†å²è®°å½• JSON", data=hist_json, file_name=f"video_analysis_{selected['id']}.json", mime="application/json")

        except Exception as e:
            st.error(f"ä¸‹è½½æˆ–è§£æè§†é¢‘æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
