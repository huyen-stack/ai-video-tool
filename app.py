import os
import json
import time
import base64
import hmac
import hashlib
import tempfile
import concurrent.futures
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any

import streamlit as st
import requests
import cv2
import numpy as np
from PIL import Image, ImageDraw
import yt_dlp  # æŠ–éŸ³/Bç«™/TikTok/YouTube ä¸‹è½½


# ========================
# å…¨å±€é…ç½®ï¼ˆæ™ºè°±ï¼‰
# ========================

# BigModel ChatCompletions APIï¼ˆv4ï¼‰
DEFAULT_ZHIPU_BASE_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

# è§†è§‰æ¨¡å‹ï¼šç”¨äºã€Œprompt + imageã€åˆ†æï¼ˆå»ºè®®ç”¨ 4V / Vision ç±»æ¨¡å‹åï¼‰
DEFAULT_VISION_MODEL = "glm-4v"

# æ–‡æœ¬æ¨¡å‹ï¼šç”¨äºã€Œæ•´ä½“æ€»ç»“/å¹¿å‘Šæ–‡æ¡ˆã€ï¼ˆå¯ç”¨åŒä¸€ä¸ªï¼Œä¹Ÿå¯åˆ†å¼€ï¼‰
DEFAULT_TEXT_MODEL = "glm-4.6"

# ä½ åŸå…ˆçš„å…è´¹ RPM é™åˆ¶æ˜¯ Gemini çš„ï¼›æ™ºè°±çš„é…é¢å› è´¦å·è€Œå¼‚ã€‚
# è¿™é‡Œä¿ç•™ä¸€ä¸ªâ€œè‡ªæˆ‘èŠ‚æµâ€çš„å‚æ•°ï¼Œé¿å…å¹¶å‘æŠŠæ¥å£æ‰“çˆ†ï¼š
DEFAULT_MAX_RPM = 30
DEFAULT_MAX_CONCURRENT = 2

DISPLAY_IMAGE_WIDTH = 320
PALETTE_WIDTH = 320
PALETTE_HEIGHT = 26


# ========================
# Session State
# ========================
if "zhipu_api_key" not in st.session_state:
    st.session_state["zhipu_api_key"] = os.getenv("ZHIPU_API_KEY", "")
if "analysis_history" not in st.session_state:
    st.session_state["analysis_history"] = []


# ========================
# JWTï¼ˆå¯é€‰ï¼‰ç”Ÿæˆï¼šä¸ä¾èµ– pyjwt
# æŸäº›æ™ºè°± key æ˜¯ id.secret å½¢å¼ï¼Œè‹¥ç›´æ¥ Bearer ä¸è¡Œå¯ç”¨ JWT æ¨¡å¼
# ========================
def _b64url(data: bytes) -> str:
    return base64.urlsafe_b64encode(data).decode("utf-8").rstrip("=")


def make_jwt_from_id_secret(api_key: str, exp_seconds: int = 60) -> str:
    if "." not in api_key:
        raise ValueError("JWT æ¨¡å¼éœ€è¦ api_key ä¸º {id}.{secret} æ ¼å¼ã€‚")
    kid, secret = api_key.split(".", 1)

    header = {"alg": "HS256", "sign_type": "SIGN"}
    now_ms = int(time.time() * 1000)
    payload = {
        "api_key": kid,
        "exp": now_ms + exp_seconds * 1000,
        "timestamp": now_ms,
    }

    header_b64 = _b64url(json.dumps(header, separators=(",", ":"), ensure_ascii=False).encode("utf-8"))
    payload_b64 = _b64url(json.dumps(payload, separators=(",", ":"), ensure_ascii=False).encode("utf-8"))
    signing_input = f"{header_b64}.{payload_b64}".encode("utf-8")
    signature = hmac.new(secret.encode("utf-8"), signing_input, hashlib.sha256).digest()
    sig_b64 = _b64url(signature)
    return f"{header_b64}.{payload_b64}.{sig_b64}"


def build_auth_header(raw_key: str, auth_mode: str) -> str:
    raw_key = (raw_key or "").strip()
    if not raw_key:
        raise ValueError("è¯·å…ˆå¡«å†™ ZHIPU_API_KEYï¼ˆæ™ºè°± API Keyï¼‰ã€‚")

    if auth_mode == "ç›´æ¥ API Keyï¼ˆæ¨èï¼‰":
        return f"Bearer {raw_key}"

    if auth_mode == "JWTï¼ˆid.secretï¼‰":
        token = make_jwt_from_id_secret(raw_key)
        return f"Bearer {token}"

    return f"Bearer {raw_key}"


# ========================
# æ™ºè°±è°ƒç”¨ï¼ˆæ–‡æœ¬ / å›¾æ–‡ï¼‰
# ========================
_last_call_ts = 0.0
_call_lock = concurrent.futures.thread.Lock() if hasattr(concurrent.futures, "thread") else None
_semaphore = None  # runtime set


def _throttle(max_rpm: int):
    """ç®€å•èŠ‚æµï¼šæŒ‰ max_rpm æ§åˆ¶æœ€å°é—´éš”ã€‚"""
    global _last_call_ts
    if max_rpm <= 0:
        return
    min_interval = 60.0 / float(max_rpm)
    now = time.time()
    wait = (_last_call_ts + min_interval) - now
    if wait > 0:
        time.sleep(wait)
    _last_call_ts = time.time()


def _extract_content_from_bigmodel(resp_json: Dict[str, Any]) -> str:
    try:
        return resp_json["choices"][0]["message"]["content"]
    except Exception:
        return json.dumps(resp_json, ensure_ascii=False)


def bigmodel_text(
    base_url: str,
    api_key: str,
    auth_mode: str,
    model: str,
    prompt: str,
    max_rpm: int,
    timeout_sec: int = 120,
    temperature: float = 0.6,
    top_p: float = 0.95,
    max_tokens: int = 4096,
) -> str:
    global _semaphore
    if _semaphore is None:
        _semaphore = concurrent.futures.thread.Semaphore(DEFAULT_MAX_CONCURRENT) if hasattr(concurrent.futures, "thread") else None

    auth = build_auth_header(api_key, auth_mode)
    headers = {"Authorization": auth, "Content-Type": "application/json"}

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": float(temperature),
        "top_p": float(top_p),
        "max_tokens": int(max_tokens),
    }

    # å…œåº•ï¼šæ—  semaphore ç¯å¢ƒä¹Ÿèƒ½è·‘
    if _semaphore is not None:
        with _semaphore:
            _throttle(max_rpm)
            r = requests.post(base_url, headers=headers, json=payload, timeout=timeout_sec)
    else:
        _throttle(max_rpm)
        r = requests.post(base_url, headers=headers, json=payload, timeout=timeout_sec)

    if r.status_code != 200:
        try:
            raise RuntimeError(f"HTTP {r.status_code}: {json.dumps(r.json(), ensure_ascii=False)}")
        except Exception:
            raise RuntimeError(f"HTTP {r.status_code}: {r.text}")

    return _extract_content_from_bigmodel(r.json())


def bigmodel_vision(
    base_url: str,
    api_key: str,
    auth_mode: str,
    model: str,
    prompt: str,
    img: Image.Image,
    max_rpm: int,
    timeout_sec: int = 180,
    temperature: float = 0.4,
    top_p: float = 0.95,
    max_tokens: int = 4096,
) -> str:
    """å›¾æ–‡å¤šæ¨¡æ€ï¼šæŠŠ PIL å›¾ç‰‡ç¼–ç ä¸º data URL ä¼ å…¥ã€‚"""
    global _semaphore
    if _semaphore is None:
        _semaphore = concurrent.futures.thread.Semaphore(DEFAULT_MAX_CONCURRENT) if hasattr(concurrent.futures, "thread") else None

    auth = build_auth_header(api_key, auth_mode)
    headers = {"Authorization": auth, "Content-Type": "application/json"}

    # PIL -> PNG base64
    buf = tempfile.SpooledTemporaryFile()
    img.save(buf, format="PNG")
    buf.seek(0)
    b64 = base64.b64encode(buf.read()).decode("utf-8")
    data_url = f"data:image/png;base64,{b64}"

    # BigModel v4 å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆå¸¸è§æ ¼å¼ï¼šcontent ä¸ºæ•°ç»„ï¼Œå« text + image_urlï¼‰
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": data_url}},
                ],
            }
        ],
        "temperature": float(temperature),
        "top_p": float(top_p),
        "max_tokens": int(max_tokens),
    }

    if _semaphore is not None:
        with _semaphore:
            _throttle(max_rpm)
            r = requests.post(base_url, headers=headers, json=payload, timeout=timeout_sec)
    else:
        _throttle(max_rpm)
        r = requests.post(base_url, headers=headers, json=payload, timeout=timeout_sec)

    if r.status_code != 200:
        try:
            raise RuntimeError(f"HTTP {r.status_code}: {json.dumps(r.json(), ensure_ascii=False)}")
        except Exception:
            raise RuntimeError(f"HTTP {r.status_code}: {r.text}")

    return _extract_content_from_bigmodel(r.json())


# ========================
# é¡µé¢ / å…¨å±€æ ·å¼
# ========================
st.set_page_config(
    page_title="AI è‡ªåŠ¨å…³é”®å¸§åˆ†é•œ & è§†é¢‘æç¤ºè¯åŠ©æ‰‹ï¼ˆæ™ºè°±ç‰ˆï¼‰",
    page_icon="ğŸ¬",
    layout="wide",
)

st.markdown(
    """
    <style>
    .main { background-color: #0f172a; color: #e5e7eb; }
    .stMarkdown, .stText { color: #e5e7eb; }
    .stCode { font-size: 0.85rem !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    """
    <div style="
        padding: 18px 24px;
        border-radius: 18px;
        margin-bottom: 16px;
        background: radial-gradient(circle at top left, #38bdf8 0, #0f172a 45%, #020617 100%);
        border: 1px solid rgba(148, 163, 184, 0.35);
    ">
      <h1 style="margin: 0 0 8px 0; color: #e5e7eb; font-size: 1.6rem;">
        ğŸ¬ AI è‡ªåŠ¨å…³é”®å¸§åˆ†é•œåŠ©æ‰‹ Pro Â·ï¼ˆæ™ºè°± BigModel ç‰ˆï¼‰
      </h1>
      <p style="margin: 0; color: #cbd5f5; font-size: 0.96rem;">
        ä¸Šä¼ è§†é¢‘æˆ–è¾“å…¥æŠ–éŸ³/Bç«™/TikTok/YouTube é“¾æ¥ï¼Œè®¾ç½®åˆ†ææ—¶é—´åŒºé—´ï¼Œè‡ªåŠ¨æŠ½å–å…³é”®å¸§ï¼Œç”Ÿæˆ
        <b>ç»“æ„åŒ– JSON + Midjourney æç¤ºè¯ + SORA/VEO è‹±æ–‡è§†é¢‘æç¤ºè¯ + åˆ†é•œè§£è¯» + å‰§æƒ…å¤§çº² + 10 ç§’å¹¿å‘Šæ—ç™½ + æ—¶é—´è½´åˆ†é•œè„šæœ¬</b>ï¼Œ
        å¹¶åœ¨å½“å‰ä¼šè¯ä¸­ä¿å­˜å¤šæ¡åˆ†æè®°å½•ï¼Œæ–¹ä¾¿å¯¹æ¯”ä¸ä¸‹è½½ã€‚
      </p>
    </div>
    """,
    unsafe_allow_html=True,
)


# ========================
# æŠ½å…³é”®å¸§ï¼ˆæ”¯æŒæ—¶é—´åŒºé—´ï¼‰
# ========================
def extract_keyframes_dynamic(
    video_path: str,
    min_frames: int = 6,
    max_frames: int = 30,
    base_fps: float = 0.8,
    start_sec: Optional[float] = None,
    end_sec: Optional[float] = None,
) -> Tuple[List[Image.Image], float, Tuple[float, float]]:
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps is None or fps <= 1e-2:
        fps = 25.0

    if total_frames <= 0:
        cap.release()
        return [], 0.0, (0.0, 0.0)

    duration = total_frames / fps

    if start_sec is None or start_sec < 0:
        start_sec = 0.0
    if end_sec is None or end_sec <= start_sec or end_sec > duration:
        end_sec = duration

    start_frame = int(start_sec * fps)
    end_frame_excl = min(total_frames, int(end_sec * fps))
    segment_frames = end_frame_excl - start_frame

    if segment_frames <= 0:
        start_sec = 0.0
        end_sec = duration
        start_frame = 0
        end_frame_excl = total_frames
        segment_frames = total_frames

    segment_duration = segment_frames / fps
    ideal_n = int(segment_duration * base_fps)
    target_n = max(min_frames, ideal_n)
    target_n = min(target_n, max_frames, segment_frames)

    if target_n <= 0:
        cap.release()
        return [], duration, (start_sec, end_sec)

    step = segment_frames / float(target_n)
    frame_indices = [start_frame + int(i * step) for i in range(target_n)]

    images: List[Image.Image] = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret and frame is not None:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            images.append(Image.fromarray(rgb_frame))
        else:
            images.append(Image.new("RGB", (200, 200), color="gray"))

    cap.release()
    return images, duration, (start_sec, end_sec)


# ========================
# ä»é“¾æ¥ä¸‹è½½è§†é¢‘
# ========================
def download_video_from_url(url: str) -> str:
    if not url:
        raise ValueError("è§†é¢‘é“¾æ¥ä¸ºç©º")

    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tmp_path = tmp.name
    tmp.close()

    ydl_opts = {
        "format": "mp4/bestvideo+bestaudio/best",
        "outtmpl": tmp_path,
        "merge_output_format": "mp4",
        "quiet": True,
        "no_warnings": True,
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])

    return tmp_path


# ========================
# ä¸»è‰²è°ƒè‰²å¡ç›¸å…³
# ========================
def get_color_palette(pil_img: Image.Image, num_colors: int = 5):
    img_small = pil_img.resize((120, 120))
    arr = np.array(img_small)
    data = arr.reshape((-1, 3)).astype(np.float32)

    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
    flags = cv2.KMEANS_RANDOM_CENTERS

    _, labels, centers = cv2.kmeans(data, num_colors, None, criteria, 10, flags)
    centers = centers.astype(int)
    colors = [tuple(map(int, c)) for c in centers]
    return colors


def make_palette_image(colors, width: int = PALETTE_WIDTH, height: int = PALETTE_HEIGHT):
    if not colors:
        return Image.new("RGB", (width, height), color="gray")

    bar = Image.new("RGB", (width, height))
    draw = ImageDraw.Draw(bar)
    n = len(colors)
    band_width = max(width // n, 1)

    for i, color in enumerate(colors):
        x0 = i * band_width
        x1 = width if i == n - 1 else (i + 1) * band_width
        draw.rectangle([x0, 0, x1, height], fill=color)

    return bar


def rgb_to_hex(rgb_tuple):
    r, g, b = rgb_tuple
    return "#{:02X}{:02X}{:02X}".format(r, g, b)


# ========================
# å•å¸§åˆ†æï¼šç»“æ„åŒ– JSON + MJ æç¤ºè¯ + è§†é¢‘æç¤ºè¯ï¼ˆæ™ºè°±å›¾æ–‡ï¼‰
# ========================
def analyze_single_image(
    img: Image.Image,
    index: int,
    zhipu_cfg: Dict[str, Any],
) -> Dict[str, Any]:
    try:
        prompt = f"""
ä½ ç°åœ¨æ˜¯ç”µå½±å¯¼æ¼” + æ‘„å½±æŒ‡å¯¼ + æœåŒ–é“æ€»ç›‘ + æç¤ºè¯å·¥ç¨‹å¸ˆã€‚
è¯·ä»”ç»†åˆ†æç»™ä½ çš„è¿™ä¸€å¸§ç”»é¢ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œç”¨äºï¼š
1ï¼‰äººç±»å¯¼æ¼”é˜…è¯»åˆ†é•œ
2ï¼‰Midjourney ç”Ÿæˆåˆ†é•œå›¾
3ï¼‰SORA/VEO ç­‰è§†é¢‘æ¨¡å‹ç”Ÿæˆå¯¹åº”é•œå¤´

å¿…é¡»ä½¿ç”¨ä¸‹é¢è¿™äº› keyï¼ˆè‹±æ–‡ï¼‰ï¼Œvalue å¤§éƒ¨åˆ†ä¸ºä¸­æ–‡è¯´æ˜ï¼Œè‹±æ–‡æç¤ºè¯å­—æ®µä¸ºè‹±æ–‡ï¼š

{{
  "index": {index},
  "scene_description_zh": "ç”¨ 1ï½3 å¥å®Œæ•´ä¸­æ–‡ï¼ŒæŠŠå½“å‰ç”»é¢æè¿°å¾—å°½é‡å…·ä½“ï¼ˆå¿½ç•¥ UI å…ƒç´ ï¼‰",
  "tags_zh": ["#çŸ­ä¸­æ–‡æ ‡ç­¾1", "#æ ‡ç­¾2"],
  "camera": {{
    "shot_type_zh": "è¿œæ™¯/å…¨æ™¯/ä¸­æ™¯/è¿‘æ™¯/ç‰¹å†™",
    "shot_type": "wide shot/full shot/medium shot/close-up",
    "angle_zh": "ä¿¯æ‹/ä»°æ‹/å¹³è§†/ä¾§æ‹ç­‰",
    "angle": "high angle/low angle/eye-level",
    "movement_zh": "æ¨è¿‘/è·Ÿæ‹/æ¨ªç§»/ç”©é•œç­‰",
    "movement": "slow dolly-in/handheld tracking/pan",
    "composition_zh": "ä¸‰åˆ†æ³•/ä¸­å¿ƒ/å¯¹ç§°/å‰æ™¯-ä¸»ä½“-èƒŒæ™¯ç­‰",
    "composition": "rule-of-thirds/centered/symmetry"
  }},
  "color_and_light_zh": "1-2 å¥ä¸­æ–‡æè¿°è‰²è°ƒä¸å…‰çº¿",
  "mood_zh": "ä¸­æ–‡æƒ…ç»ªæ°›å›´",
  "characters": [
    {{
      "role_zh": "äººç‰©èº«ä»½",
      "gender_zh": "å¥³æ€§/ç”·æ€§/ä¸æ˜æ˜¾",
      "age_look_zh": "å¹´é¾„è§‚æ„Ÿ",
      "body_type_zh": "ä½“å‹",
      "clothing_zh": "æœè£…é£æ ¼ä¸é¢œè‰²",
      "hair_zh": "å‘å‹ä¸å‘è‰²",
      "expression_zh": "è¡¨æƒ…",
      "pose_body_zh": "å§¿æ€",
      "props_zh": "é“å…·"
    }}
  ],
  "character_action_detail_zh": "åŠ¨ä½œç»†èŠ‚ï¼ˆå¤´â†’æ‰‹â†’èº¯å¹²â†’è…¿ï¼Œå†™å…·ä½“æ¥è§¦ç‚¹ä¸æ–¹å‘/é€Ÿåº¦ï¼‰",
  "face_expression_detail_zh": "é¢éƒ¨ä¸çœ¼ç¥ç»†èŠ‚ï¼ˆå«å¤–åŠ›å½¢å˜å›å¼¹è‹¥æœ‰ï¼‰",
  "cloth_hair_reaction_zh": "å¤´å‘ä¸è¡£ç‰©å¯¹é£/æƒ¯æ€§çš„ååº”",
  "environment_detail_zh": "å‰æ™¯/ä¸­æ™¯/èƒŒæ™¯çš„ç©ºé—´ç»“æ„ä¸æè´¨",
  "weather_force_detail_zh": "é£é›¨é›ª/å†²å‡»æ³¢ç­‰ç¯å¢ƒåŠ›ç»†èŠ‚ï¼ˆæ— åˆ™å†™æ— æ˜æ˜¾ï¼‰",
  "props_and_tech_detail_zh": "å…³é”®é“å…·/ç§‘æŠ€å…ƒç´ ï¼ˆä½ç½®/å¤–è§‚/çŠ¶æ€ï¼‰",
  "physics_reaction_detail_zh": "å—åŠ›ä¸å½¢å˜å›å¼¹è¿‡ç¨‹",
  "structure_damage_detail_zh": "ç»“æ„æŸåï¼ˆå“ªéƒ¨åˆ†æ€æ ·ç ´æŸï¼‰",
  "debris_motion_detail_zh": "ç¢ç‰‡é£æ•£è½¨è¿¹ï¼ˆæ— åˆ™å†™æ— æ˜æ˜¾ï¼‰",
  "motion_detail_zh": "ä¸Šä¸€ç¬é—´â†’å½“å‰â†’ä¸‹ä¸€ç¬é—´åŠ¨ä½œæ¨æ–­",
  "fx_detail_zh": "çƒŸå°˜/ç«èŠ±/èƒ½é‡ç²’å­ç­‰ï¼ˆæ— å¯ç©ºï¼‰",
  "lighting_color_detail_zh": "æ›´ç²¾ç»†å…‰æºæ–¹å‘/è‰²æ¸©å·®/é¢‘é—ªç­‰",
  "audio_cue_detail_zh": "å£°éŸ³è®¾è®¡ï¼ˆç¯å¢ƒå£°/ç‰¹æ•ˆå£°/BGMï¼‰",
  "edit_rhythm_detail_zh": "å‰ªè¾‘èŠ‚å¥ï¼ˆæ…¢åŠ¨ä½œ/é—ªç™½/ç”©é•œç­‰ï¼‰",
  "midjourney_prompt": "ä¸€è¡Œè‹±æ–‡ MJ v6 æç¤ºè¯",
  "midjourney_negative_prompt": "ä¸€è¡Œè‹±æ–‡è´Ÿé¢æç¤ºè¯",
  "video_prompt_en": "3-5 å¥è‹±æ–‡è§†é¢‘æç¤ºè¯ï¼Œæœ€åä¸€å¥å†™ 4 second shot, vertical 9:16, 24fps, cinematic, highly detailed."
}}

è¦æ±‚ï¼š
1) åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ï¼›
2) å…¨éƒ¨åŒå¼•å·ï¼›æ— æ³¨é‡Šï¼›æ— å¤šä½™é€—å·ã€‚
""".strip()

        text = bigmodel_vision(
            base_url=zhipu_cfg["base_url"],
            api_key=zhipu_cfg["api_key"],
            auth_mode=zhipu_cfg["auth_mode"],
            model=zhipu_cfg["vision_model"],
            prompt=prompt,
            img=img,
            max_rpm=zhipu_cfg["max_rpm"],
            temperature=0.35,
            max_tokens=4096,
        )

        start = text.find("{")
        end = text.rfind("}")
        if start == -1 or end == -1 or end <= start:
            raise ValueError("æœªæ£€æµ‹åˆ°æœ‰æ•ˆ JSON ç»“æ„")

        info = json.loads(text[start : end + 1])

        # å…œåº•å­—æ®µ
        info["index"] = index
        info.setdefault("scene_description_zh", "")
        info.setdefault("tags_zh", [])
        info.setdefault("camera", {})
        info.setdefault("color_and_light_zh", "")
        info.setdefault("mood_zh", "")
        info.setdefault("characters", [])
        info.setdefault("character_action_detail_zh", "")
        info.setdefault("face_expression_detail_zh", "")
        info.setdefault("cloth_hair_reaction_zh", "")
        info.setdefault("environment_detail_zh", "")
        info.setdefault("weather_force_detail_zh", "")
        info.setdefault("props_and_tech_detail_zh", "")
        info.setdefault("physics_reaction_detail_zh", "")
        info.setdefault("structure_damage_detail_zh", "")
        info.setdefault("debris_motion_detail_zh", "")
        info.setdefault("motion_detail_zh", "")
        info.setdefault("fx_detail_zh", "")
        info.setdefault("lighting_color_detail_zh", "")
        info.setdefault("audio_cue_detail_zh", "")
        info.setdefault("edit_rhythm_detail_zh", "")
        info.setdefault("midjourney_prompt", "")
        info.setdefault("midjourney_negative_prompt", "")
        info.setdefault("video_prompt_en", "")

        cam = info["camera"]
        cam.setdefault("shot_type_zh", "")
        cam.setdefault("shot_type", "")
        cam.setdefault("angle_zh", "")
        cam.setdefault("angle", "")
        cam.setdefault("movement_zh", "")
        cam.setdefault("movement", "")
        cam.setdefault("composition_zh", "")
        cam.setdefault("composition", "")

        return info

    except Exception as e:
        return {
            "index": index,
            "scene_description_zh": f"ï¼ˆAI åˆ†æå¤±è´¥ï¼š{e}ï¼‰",
            "tags_zh": [],
            "camera": {
                "shot_type_zh": "",
                "shot_type": "",
                "angle_zh": "",
                "angle": "",
                "movement_zh": "",
                "movement": "",
                "composition_zh": "",
                "composition": "",
            },
            "color_and_light_zh": "",
            "mood_zh": "",
            "characters": [],
            "character_action_detail_zh": "",
            "face_expression_detail_zh": "",
            "cloth_hair_reaction_zh": "",
            "environment_detail_zh": "",
            "weather_force_detail_zh": "",
            "props_and_tech_detail_zh": "",
            "physics_reaction_detail_zh": "",
            "structure_damage_detail_zh": "",
            "debris_motion_detail_zh": "",
            "motion_detail_zh": "",
            "fx_detail_zh": "",
            "lighting_color_detail_zh": "",
            "audio_cue_detail_zh": "",
            "edit_rhythm_detail_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
            "video_prompt_en": "",
        }


def analyze_images_concurrently(
    images: List[Image.Image],
    max_ai_frames: int,
    zhipu_cfg: Dict[str, Any],
) -> List[Dict[str, Any]]:
    n = len(images)
    if n == 0:
        return []

    use_n = min(max_ai_frames, n)
    results: List[Dict[str, Any]] = [None] * n  # type: ignore

    status = st.empty()
    status.info(f"âš¡ æ­£åœ¨å¯¹å‰ {use_n} å¸§è¿›è¡Œ AI åˆ†æï¼ˆå…± {n} å¸§ï¼‰ï¼Œå…¶ä½™å¸§ä¿ç•™æˆªå›¾ä¸è‰²å¡ã€‚")

    # å¹¶å‘ä¸è¦å¼€å¤ªå¤§ï¼Œé¿å…è§¦å‘é™æµï¼›è¿™é‡Œæœ€å¤š 4
    workers = min(use_n, 4)

    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_index = {
            executor.submit(analyze_single_image, images[i], i + 1, zhipu_cfg): i
            for i in range(use_n)
        }
        for future in concurrent.futures.as_completed(future_to_index):
            i = future_to_index[future]
            try:
                results[i] = future.result()
            except Exception as e:
                results[i] = {
                    "index": i + 1,
                    "scene_description_zh": f"ï¼ˆAI åˆ†æå¤±è´¥ï¼š{e}ï¼‰",
                    "tags_zh": [],
                    "camera": {
                        "shot_type_zh": "",
                        "shot_type": "",
                        "angle_zh": "",
                        "angle": "",
                        "movement_zh": "",
                        "movement": "",
                        "composition_zh": "",
                        "composition": "",
                    },
                    "color_and_light_zh": "",
                    "mood_zh": "",
                    "characters": [],
                    "character_action_detail_zh": "",
                    "face_expression_detail_zh": "",
                    "cloth_hair_reaction_zh": "",
                    "environment_detail_zh": "",
                    "weather_force_detail_zh": "",
                    "props_and_tech_detail_zh": "",
                    "physics_reaction_detail_zh": "",
                    "structure_damage_detail_zh": "",
                    "debris_motion_detail_zh": "",
                    "motion_detail_zh": "",
                    "fx_detail_zh": "",
                    "lighting_color_detail_zh": "",
                    "audio_cue_detail_zh": "",
                    "edit_rhythm_detail_zh": "",
                    "midjourney_prompt": "",
                    "midjourney_negative_prompt": "",
                    "video_prompt_en": "",
                }

    for i in range(use_n, n):
        results[i] = {
            "index": i + 1,
            "scene_description_zh": "ï¼ˆæœ¬å¸§æœªåš AI åˆ†æï¼Œç”¨äºèŠ‚çœå½“å‰é…é¢ï¼Œä½†ä»å¯ç”¨äºè§†è§‰å‚è€ƒå’Œè‰²å¡ã€‚ï¼‰",
            "tags_zh": [],
            "camera": {
                "shot_type_zh": "",
                "shot_type": "",
                "angle_zh": "",
                "angle": "",
                "movement_zh": "",
                "movement": "",
                "composition_zh": "",
                "composition": "",
            },
            "color_and_light_zh": "",
            "mood_zh": "",
            "characters": [],
            "character_action_detail_zh": "",
            "face_expression_detail_zh": "",
            "cloth_hair_reaction_zh": "",
            "environment_detail_zh": "",
            "weather_force_detail_zh": "",
            "props_and_tech_detail_zh": "",
            "physics_reaction_detail_zh": "",
            "structure_damage_detail_zh": "",
            "debris_motion_detail_zh": "",
            "motion_detail_zh": "",
            "fx_detail_zh": "",
            "lighting_color_detail_zh": "",
            "audio_cue_detail_zh": "",
            "edit_rhythm_detail_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
            "video_prompt_en": "",
        }

    status.empty()
    return results


# ========================
# æ•´ä½“è§†é¢‘å±‚é¢çš„æ€»ç»“ï¼ˆæ™ºè°±æ–‡æœ¬ï¼‰
# ========================
def analyze_overall_video(frame_infos: List[Dict[str, Any]], zhipu_cfg: Dict[str, Any]) -> str:
    described = [
        info for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆæ•´ä½“å‰§æƒ…å¤§çº²ã€‚ï¼‰"

    parts = []
    for info in described:
        idx = info["index"]
        cam = info.get("camera", {})
        tags = info.get("tags_zh", [])
        part = (
            f"ç¬¬ {idx} å¸§ï¼š{info.get('scene_description_zh', '')}\n"
            f"æ™¯åˆ«ï¼š{cam.get('shot_type_zh', '')}ï¼›è§’åº¦ï¼š{cam.get('angle_zh', '')}ï¼›è¿é•œï¼š{cam.get('movement_zh', '')}ï¼›æ„å›¾ï¼š{cam.get('composition_zh', '')}\n"
            f"è‰²å½©ä¸å…‰å½±ï¼š{info.get('color_and_light_zh', '')}\n"
            f"æƒ…ç»ªæ°›å›´ï¼š{info.get('mood_zh', '')}\n"
            f"æ ‡ç­¾ï¼š{'ã€'.join(tags)}"
        )
        parts.append(part)

    joined = "\n\n".join(parts)

    prompt = f"""
ä½ ç°åœ¨æ˜¯èµ„æ·±è§†é¢‘å¯¼æ¼” + å‰ªè¾‘å¸ˆ + çŸ­è§†é¢‘è¿è¥ä¸“å®¶ + å†…å®¹åˆè§„å®¡æ ¸å‘˜ã€‚
ä¸‹é¢æ˜¯ä»ä¸€æ®µè§†é¢‘ä¸­æŠ½å–çš„è‹¥å¹²å…³é”®å¸§çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›è¯´æ˜ï¼Œå¯¹æ•´æ®µè§†é¢‘åšæ•´ä½“åˆ†æã€‚

=== å¸§çº§è¯´æ˜å¼€å§‹ ===
{joined}
=== å¸§çº§è¯´æ˜ç»“æŸ ===

è¯·ä¸¥æ ¼æŒ‰ä¸‹é¢ç»“æ„è¾“å‡ºä¸­æ–‡åˆ†æï¼š

ã€å‰§æƒ…å¤§çº²ã€‘
ç”¨ 2-4 å¥æ¦‚æ‹¬è¿™æ®µè§†é¢‘çš„å¤§è‡´å†…å®¹/äººç‰©å…³ç³»/å‘ç”Ÿåœºæ™¯ã€‚

ã€æ•´ä½“è§†å¬é£æ ¼ã€‘
ä»èŠ‚å¥å¿«æ…¢ã€é•œå¤´æ„Ÿã€è‰²å½©æ°”è´¨ï¼ˆæš–/å†·/æ—¥å¸¸/æ¢¦å¹»ï¼‰ã€æƒ…ç»ªæ°›å›´ç­‰è§’åº¦æ€»ç»“æ•´ä½“é£æ ¼ã€‚

ã€é€‚åˆçš„è¯é¢˜æ ‡ç­¾ã€‘
ç”¨ #æ ‡ç­¾ å½¢å¼ç»™å‡º 5-10 ä¸ªï¼Œé€‚åˆæŠ–éŸ³/å°çº¢ä¹¦/è§†é¢‘å·ç­‰å¹³å°ã€‚

ã€å•†ä¸šä¸åˆè§„é£é™©ã€‘
æ•´ä½“é£é™©çº§åˆ«ï¼šä½ / ä¸­ / é«˜
å¹¶ç”¨ 2-3 å¥è¯è¯´æ˜éœ€è¦æ³¨æ„çš„ç‚¹ã€‚

åªè¾“å‡ºä»¥ä¸Š 4 ä¸ªå°èŠ‚ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
""".strip()

    try:
        return bigmodel_text(
            base_url=zhipu_cfg["base_url"],
            api_key=zhipu_cfg["api_key"],
            auth_mode=zhipu_cfg["auth_mode"],
            model=zhipu_cfg["text_model"],
            prompt=prompt,
            max_rpm=zhipu_cfg["max_rpm"],
            temperature=0.5,
            max_tokens=2048,
        )
    except Exception as e:
        return f"æ•´ä½“åˆ†æå¤±è´¥ï¼š{e}"


# ========================
# 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬ç”Ÿæˆï¼ˆæ™ºè°±æ–‡æœ¬ï¼‰
# ========================
def generate_ad_script(frame_infos: List[Dict[str, Any]], zhipu_cfg: Dict[str, Any]) -> str:
    described = [
        info for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆå¹¿å‘Šæ—ç™½è„šæœ¬ã€‚ï¼‰"

    parts = []
    for info in described:
        idx = info["index"]
        tags = info.get("tags_zh", [])
        parts.append(f"ç¬¬ {idx} å¸§ï¼š{info.get('scene_description_zh', '')}ï¼›æ ‡ç­¾ï¼š{'ã€'.join(tags)}")
    joined = "\n".join(parts)

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±å¹¿å‘Šå¯¼æ¼” + æ–‡æ¡ˆã€‚
æˆ‘æœ‰ä¸€ä¸ªç”±è‹¥å¹²ç”»é¢ç»„æˆçš„ç«–ç‰ˆçŸ­è§†é¢‘ï¼Œæ—¶é•¿å¤§çº¦ 8-12 ç§’ã€‚
ä¸‹é¢æ˜¯æ¯ä¸ªç”»é¢çš„ç®€è¦è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›ä¿¡æ¯ï¼Œå†™ä¸€æ¡é€‚åˆé…åˆè¿™äº›ç”»é¢æ’­æ”¾çš„ä¸­æ–‡å¹¿å‘Šæ—ç™½è„šæœ¬ã€‚

=== å…³é”®å¸§æ¦‚è§ˆ ===
{joined}
=== å…³é”®å¸§æ¦‚è§ˆç»“æŸ ===

è¦æ±‚ï¼š
1. æ—ç™½æ€»æ—¶é•¿æ§åˆ¶åœ¨ 8-12 ç§’å·¦å³ï¼ˆæ­£å¸¸è¯­é€Ÿï¼‰ï¼Œæ–‡æœ¬ 35-70 å­—å³å¯ã€‚
2. é£æ ¼ä¸ç”»é¢è°ƒæ€§åŒ¹é…ã€‚
3. ç”¨è‡ªç„¶å£è¯­åŒ–ä¸­æ–‡ï¼Œä¸è¦å‡ºç°â€œç”»é¢ä¸­â€â€œé•œå¤´é‡Œâ€å­—çœ¼ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢æ ¼å¼è¾“å‡ºï¼š

ã€10ç§’å¹¿å‘Šæ—ç™½è„šæœ¬ã€‘
ï¼ˆåœ¨è¿™é‡Œå†™å®Œæ•´çš„ä¸€æ®µæ—ç™½ï¼‰

ä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹ã€‚
""".strip()

    try:
        return bigmodel_text(
            base_url=zhipu_cfg["base_url"],
            api_key=zhipu_cfg["api_key"],
            auth_mode=zhipu_cfg["auth_mode"],
            model=zhipu_cfg["text_model"],
            prompt=prompt,
            max_rpm=zhipu_cfg["max_rpm"],
            temperature=0.7,
            max_tokens=1024,
        )
    except Exception as e:
        return f"å¹¿å‘Šæ–‡æ¡ˆç”Ÿæˆå¤±è´¥ï¼š{e}"


# ========================
# æ—¶é—´è½´åˆ†é•œè„šæœ¬ç”Ÿæˆï¼ˆçº¯æ‹¼æ¥ï¼Œä¸è°ƒç”¨ AIï¼‰
# ========================
def generate_timeline_shotlist(
    frame_infos: List[Dict[str, Any]],
    used_range: Tuple[float, float],
) -> str:
    n = len(frame_infos)
    if n == 0:
        return "ï¼ˆæš‚æ— å…³é”®å¸§ï¼Œæ— æ³•ç”Ÿæˆæ—¶é—´è½´åˆ†é•œè„šæœ¬ã€‚ï¼‰"

    start_used, end_used = used_range
    total_len = max(0.1, end_used - start_used)
    seg = total_len / n
    lines: List[str] = []

    for i, info in enumerate(frame_infos):
        t0 = i * seg
        t1 = (i + 1) * seg
        if i == n - 1:
            t1 = total_len

        shot_id = f"S{i+1:02d}"
        cam = info.get("camera", {}) or {}
        tags = info.get("tags_zh", []) or []

        def _s(k): return (info.get(k) or "").strip()

        scene = _s("scene_description_zh")
        char_act = _s("character_action_detail_zh")
        env = _s("environment_detail_zh")
        props = _s("props_and_tech_detail_zh")
        motion = _s("motion_detail_zh")
        mood = _s("mood_zh")

        face = _s("face_expression_detail_zh")
        cloth_hair = _s("cloth_hair_reaction_zh")
        weather = _s("weather_force_detail_zh")
        physics = _s("physics_reaction_detail_zh")
        structure_damage = _s("structure_damage_detail_zh")
        debris_motion = _s("debris_motion_detail_zh")
        fx = _s("fx_detail_zh")
        lighting = _s("lighting_color_detail_zh")
        audio = _s("audio_cue_detail_zh")
        edit = _s("edit_rhythm_detail_zh")

        shot_type = cam.get("shot_type_zh", "")
        angle = cam.get("angle_zh", "")
        movement = cam.get("movement_zh", "")
        composition = cam.get("composition_zh", "")

        block_lines: List[str] = []
        block_lines.append(f"ã€{shot_id} | {t0:.1f}-{t1:.1f} ç§’ã€‘")

        if scene: block_lines.append(f"ç”»é¢å†…å®¹ï¼š{scene}")
        if char_act: block_lines.append(f"äººç‰©åŠ¨ä½œï¼š{char_act}")
        if face: block_lines.append(f"é¢éƒ¨ä¸çœ¼ç¥ï¼š{face}")
        if cloth_hair: block_lines.append(f"æœè£…ä¸å¤´å‘ï¼š{cloth_hair}")

        if env: block_lines.append(f"åœºæ™¯ä¸ç©ºé—´ï¼š{env}")
        if weather: block_lines.append(f"å¤©æ°”ä¸ç¯å¢ƒåŠ›ï¼š{weather}")

        if props: block_lines.append(f"é“å…·ä¸ç§‘æŠ€ï¼š{props}")
        if structure_damage: block_lines.append(f"ç»“æ„æŸåï¼š{structure_damage}")
        if debris_motion: block_lines.append(f"ç¢ç‰‡ä¸é£æ•£è½¨è¿¹ï¼š{debris_motion}")
        if physics: block_lines.append(f"å—åŠ›ä¸ç‰©ç†åé¦ˆï¼š{physics}")

        if fx: block_lines.append(f"ç‰¹æ•ˆä¸ç²’å­ï¼š{fx}")
        if lighting: block_lines.append(f"å…‰çº¿ä¸è‰²å½©ï¼š{lighting}")

        cam_desc_parts = []
        if shot_type: cam_desc_parts.append(f"æ™¯åˆ«ï¼š{shot_type}")
        if angle: cam_desc_parts.append(f"è§’åº¦ï¼š{angle}")
        if movement: cam_desc_parts.append(f"è¿é•œï¼š{movement}")
        if composition: cam_desc_parts.append(f"æ„å›¾ï¼š{composition}")
        if cam_desc_parts:
            block_lines.append("æœºä½ä¸è¿åŠ¨ï¼š" + "ï¼›".join(cam_desc_parts))

        if mood: block_lines.append(f"æƒ…ç»ªæ°›å›´ï¼š{mood}")
        if motion: block_lines.append(f"åŠ¨ä½œè¶‹åŠ¿ï¼š{motion}")

        if audio: block_lines.append(f"å£°éŸ³ä¸èŠ‚å¥ï¼š{audio}")
        if edit: block_lines.append(f"å‰ªè¾‘ä¸èŠ‚å¥ï¼š{edit}")

        if tags: block_lines.append("æ ‡ç­¾ï¼š" + " ".join(tags))
        lines.append("\n".join(block_lines))

    return "\n\n".join(lines)


# ========================
# ä¾§è¾¹æ ï¼šæ™ºè°±é…ç½®
# ========================
with st.sidebar:
    st.header("ğŸ”‘ ç¬¬ä¸€æ­¥ï¼šé…ç½®æ™ºè°± BigModel")

    zhipu_key = st.text_input(
        "ZHIPU_API_KEY",
        type="password",
        value=st.session_state["zhipu_api_key"],
        help="å»ºè®®åœ¨éƒ¨ç½²ç¯å¢ƒå˜é‡é‡Œé…ç½® ZHIPU_API_KEY",
    )
    st.session_state["zhipu_api_key"] = zhipu_key

    auth_mode = st.selectbox("é‰´æƒæ–¹å¼", ["ç›´æ¥ API Keyï¼ˆæ¨èï¼‰", "JWTï¼ˆid.secretï¼‰"], index=0)
    base_url = st.text_input("æ™ºè°±æ¥å£åœ°å€", value=DEFAULT_ZHIPU_BASE_URL)
    vision_model = st.text_input("è§†è§‰æ¨¡å‹ï¼ˆå›¾+æ–‡ï¼‰", value=DEFAULT_VISION_MODEL)
    text_model = st.text_input("æ–‡æœ¬æ¨¡å‹ï¼ˆæ€»ç»“/æ–‡æ¡ˆï¼‰", value=DEFAULT_TEXT_MODEL)

    st.markdown("---")
    max_rpm = st.slider("æœ€å¤§è¯·æ±‚é€Ÿç‡ï¼ˆè‡ªæˆ‘èŠ‚æµ RPMï¼‰", 1, 120, DEFAULT_MAX_RPM, 1)
    max_concurrent = st.slider("æœ€å¤§å¹¶å‘ï¼ˆå»ºè®® 1-3ï¼‰", 1, 6, DEFAULT_MAX_CONCURRENT, 1)

    st.markdown("---")
    max_ai_frames = st.slider(
        "æœ¬æ¬¡æœ€å¤šåš AI åˆ†æçš„å¸§æ•°ï¼ˆæ¶ˆè€—é…é¢ï¼‰",
        min_value=4,
        max_value=20,
        value=10,
        step=1,
    )
    st.caption("å»ºè®®ï¼š10 ç§’è§†é¢‘ 6~10 å¸§å³å¯ï¼›è¶…å‡ºéƒ¨åˆ†ä»ä¼šæ˜¾ç¤ºæˆªå›¾å’Œè‰²å¡ï¼Œä½†ä¸è°ƒ AIã€‚")

    st.markdown("---")
    st.markdown("â± åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå•ä½ï¼šç§’ï¼‰")
    start_sec = st.number_input(
        "ä»ç¬¬å‡ ç§’å¼€å§‹ï¼ˆå«ï¼‰", min_value=0.0, value=0.0, step=0.5,
        help="ç²¾ç¡®åˆ° 0.5 ç§’ï¼›é»˜è®¤ 0 è¡¨ç¤ºä»å¤´å¼€å§‹"
    )
    end_sec = st.number_input(
        "åˆ°ç¬¬å‡ ç§’ç»“æŸï¼ˆ0 æˆ– â‰¤å¼€å§‹ç§’ è¡¨ç¤ºç›´åˆ°ç»“å°¾ï¼‰",
        min_value=0.0, value=0.0, step=0.5,
        help="ä¾‹å¦‚ï¼šåªåˆ†æ 3~8 ç§’ï¼Œå°±å¡« 3 å’Œ 8ï¼›å¡« 0 æˆ–ä¸å¤§äºå¼€å§‹ç§’åˆ™åˆ†æåˆ°ç»“å°¾"
    )

    if not zhipu_key:
        st.warning("ğŸ”´ è¿˜æ²¡æœ‰ Keyï¼šè¯·å»æ™ºè°±å¼€æ”¾å¹³å°åˆ›å»º API Keyï¼Œå¹¶é…ç½®åˆ°ç¯å¢ƒå˜é‡ ZHIPU_API_KEY")
    else:
        st.success("ğŸŸ¢ Key å·²å°±ç»ª")


# æŠŠå¹¶å‘æ§åˆ¶åŒæ­¥åˆ° semaphore
try:
    import threading
    _semaphore = threading.Semaphore(int(max_concurrent))
except Exception:
    _semaphore = None

zhipu_cfg = {
    "api_key": zhipu_key,
    "auth_mode": auth_mode,
    "base_url": (base_url or "").strip(),
    "vision_model": (vision_model or "").strip(),
    "text_model": (text_model or "").strip(),
    "max_rpm": int(max_rpm),
}


# ========================
# ä¸»æµç¨‹ï¼šä¸Šä¼ /é“¾æ¥ é€‰æ‹© + æŠ½å¸§ + åˆ†æ + å±•ç¤º
# ========================
source_mode = st.radio(
    "ğŸ“¥ é€‰æ‹©è§†é¢‘æ¥æº",
    ["ä¸Šä¼ æœ¬åœ°æ–‡ä»¶", "è¾“å…¥ç½‘ç»œè§†é¢‘é“¾æ¥ï¼ˆæŠ–éŸ³ / Bç«™ / TikTok / YouTubeï¼‰"],
    index=0,
)

video_url: Optional[str] = None
uploaded_file = None

if source_mode == "ä¸Šä¼ æœ¬åœ°æ–‡ä»¶":
    uploaded_file = st.file_uploader(
        "ğŸ“‚ ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼ˆå»ºè®® < 50MBï¼‰",
        type=["mp4", "mov", "m4v", "avi", "mpeg"],
    )
else:
    video_url = st.text_input(
        "ğŸ”— è¾“å…¥è§†é¢‘é“¾æ¥",
        placeholder="ä¾‹å¦‚ï¼šhttps://v.douyin.com/xxxxxx æˆ– https://www.douyin.com/video/xxxxxxxxx",
    )

if st.button("ğŸš€ ä¸€é”®è§£ææ•´æ¡è§†é¢‘"):
    if not zhipu_cfg["api_key"]:
        st.error("è¯·å…ˆåœ¨å·¦ä¾§è¾“å…¥æœ‰æ•ˆçš„æ™ºè°± API Keyã€‚")
    else:
        tmp_path: Optional[str] = None
        source_label = ""
        source_type = ""

        try:
            # 1) å‡†å¤‡è§†é¢‘è·¯å¾„
            if source_mode == "ä¸Šä¼ æœ¬åœ°æ–‡ä»¶":
                source_type = "upload"
                if not uploaded_file:
                    st.error("è¯·å…ˆä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚")
                    st.stop()
                suffix = os.path.splitext(uploaded_file.name)[1] or ".mp4"
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                    tmp.write(uploaded_file.read())
                    tmp_path = tmp.name
                source_label = uploaded_file.name
            else:
                source_type = "url"
                if not video_url:
                    st.error("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„è§†é¢‘é“¾æ¥ã€‚")
                    st.stop()
                st.info("ğŸŒ æ­£åœ¨ä»ç½‘ç»œä¸‹è½½è§†é¢‘ï¼Œè¯·ç¨å€™...")
                tmp_path = download_video_from_url(video_url)
                source_label = video_url

            if not tmp_path:
                st.error("è§†é¢‘è·¯å¾„å¼‚å¸¸ï¼Œè¯·é‡è¯•ã€‚")
                st.stop()

            # 2) æŠ½å¸§ï¼ˆå¸¦æ—¶é—´åŒºé—´ï¼‰
            st.info("â³ æ­£åœ¨æ ¹æ®æŒ‡å®šæ—¶é—´åŒºé—´è‡ªåŠ¨æŠ½å–å…³é”®å¸§...")
            images, duration, used_range = extract_keyframes_dynamic(
                tmp_path,
                start_sec=start_sec,
                end_sec=end_sec if end_sec > 0 else None,
            )
            start_used, end_used = used_range

            try:
                os.remove(tmp_path)
            except OSError:
                pass

            if not images:
                st.error("âŒ æ— æ³•ä»è§†é¢‘ä¸­è¯»å–å¸§ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ˜¯å¦æŸåæˆ–æ ¼å¼å¼‚å¸¸ã€‚")
                st.stop()

            st.success(
                f"âœ… å·²æˆåŠŸæŠ½å– {len(images)} ä¸ªå…³é”®å¸§ï¼ˆè§†é¢‘æ€»é•¿çº¦ {duration:.1f} ç§’ï¼Œ"
                f"æœ¬æ¬¡åˆ†æåŒºé—´ï¼š{start_used:.1f}â€“{end_used:.1f} ç§’ï¼‰ã€‚"
            )

            # 3) ä¸»è‰²è°ƒ
            frame_palettes: List[List[Tuple[int, int, int]]] = []
            for img in images:
                try:
                    palette_colors = get_color_palette(img, num_colors=5)
                except Exception:
                    palette_colors = []
                frame_palettes.append(palette_colors)

            # 4) å¸§çº§åˆ†æ
            with st.spinner("ğŸ§  æ­£åœ¨ä¸ºå…³é”®å¸§ç”Ÿæˆç»“æ„åŒ–åˆ†æ + MJ æç¤ºè¯ + è§†é¢‘æç¤ºè¯..."):
                frame_infos = analyze_images_concurrently(
                    images, max_ai_frames=max_ai_frames, zhipu_cfg=zhipu_cfg
                )

            # 5) æ•´ä½“åˆ†æ + å¹¿å‘Šæ–‡æ¡ˆ + æ—¶é—´è½´åˆ†é•œï¼ˆæ—¶é—´è½´ä¸ºçº¯æ‹¼æ¥ï¼‰
            with st.spinner("ğŸ“š æ­£åœ¨ç”Ÿæˆæ•´æ®µè§†é¢‘çš„å‰§æƒ…å¤§çº²ä¸è¯é¢˜æ ‡ç­¾..."):
                overall = analyze_overall_video(frame_infos, zhipu_cfg)
            with st.spinner("ğŸ¤ æ­£åœ¨ç”Ÿæˆ 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬..."):
                ad_script = generate_ad_script(frame_infos, zhipu_cfg)
            with st.spinner("ğŸ¬ æ­£åœ¨ç”Ÿæˆæ—¶é—´è½´åˆ†é•œè„šæœ¬ï¼ˆçº¯æ‹¼æ¥ç‰ˆï¼‰..."):
                timeline_shotlist = generate_timeline_shotlist(frame_infos, used_range=used_range)

            # 6) ç»„è£… export_data + å†™å…¥å†å²è®°å½•
            export_frames = []
            for info, palette in zip(frame_infos, frame_palettes):
                export_frames.append(
                    {
                        "index": info.get("index"),
                        "scene_description_zh": info.get("scene_description_zh", ""),
                        "tags_zh": info.get("tags_zh", []),
                        "camera": info.get("camera", {}),
                        "color_and_light_zh": info.get("color_and_light_zh", ""),
                        "mood_zh": info.get("mood_zh", ""),
                        "characters": info.get("characters", []),
                        "character_action_detail_zh": info.get("character_action_detail_zh", ""),
                        "face_expression_detail_zh": info.get("face_expression_detail_zh", ""),
                        "cloth_hair_reaction_zh": info.get("cloth_hair_reaction_zh", ""),
                        "environment_detail_zh": info.get("environment_detail_zh", ""),
                        "weather_force_detail_zh": info.get("weather_force_detail_zh", ""),
                        "props_and_tech_detail_zh": info.get("props_and_tech_detail_zh", ""),
                        "physics_reaction_detail_zh": info.get("physics_reaction_detail_zh", ""),
                        "structure_damage_detail_zh": info.get("structure_damage_detail_zh", ""),
                        "debris_motion_detail_zh": info.get("debris_motion_detail_zh", ""),
                        "motion_detail_zh": info.get("motion_detail_zh", ""),
                        "fx_detail_zh": info.get("fx_detail_zh", ""),
                        "lighting_color_detail_zh": info.get("lighting_color_detail_zh", ""),
                        "audio_cue_detail_zh": info.get("audio_cue_detail_zh", ""),
                        "edit_rhythm_detail_zh": info.get("edit_rhythm_detail_zh", ""),
                        "midjourney_prompt": info.get("midjourney_prompt", ""),
                        "midjourney_negative_prompt": info.get("midjourney_negative_prompt", ""),
                        "video_prompt_en": info.get("video_prompt_en", ""),
                        "palette_rgb": [list(c) for c in (palette or [])],
                        "palette_hex": [rgb_to_hex(c) for c in (palette or [])],
                    }
                )

            export_data = {
                "meta": {
                    "provider": "zhipu_bigmodel",
                    "vision_model": zhipu_cfg["vision_model"],
                    "text_model": zhipu_cfg["text_model"],
                    "frame_count": len(images),
                    "max_ai_frames_this_run": min(max_ai_frames, len(images)),
                    "duration_sec_est": duration,
                    "start_sec_used": start_used,
                    "end_sec_used": end_used,
                    "source_type": source_type,
                    "source_label": source_label,
                    "base_url": zhipu_cfg["base_url"],
                },
                "frames": export_frames,
                "overall_analysis": overall,
                "ad_script_10s": ad_script,
                "timeline_shotlist_zh": timeline_shotlist,
            }

            json_str = json.dumps(export_data, ensure_ascii=False, indent=2)

            history = st.session_state["analysis_history"]
            run_id = f"run_{len(history) + 1}"
            history.append(
                {
                    "id": run_id,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "meta": export_data["meta"],
                    "data": export_data,
                }
            )
            st.session_state["analysis_history"] = history

            # 7) Tabs å±•ç¤º
            tab_frames, tab_story, tab_json, tab_history = st.tabs(
                ["ğŸ å…³é”®å¸§ & æç¤ºè¯", "ğŸ“š å‰§æƒ…æ€»ç»“ & å¹¿å‘Šæ—ç™½ & æ—¶é—´è½´åˆ†é•œ", "ğŸ“¦ JSON å¯¼å‡ºï¼ˆæœ¬æ¬¡ï¼‰", "ğŸ•˜ å†å²è®°å½•ï¼ˆæœ¬ä¼šè¯ï¼‰"]
            )

            with tab_frames:
                st.markdown(f"å…±æŠ½å– **{len(images)}** ä¸ªå…³é”®å¸§ï¼Œå…¶ä¸­å‰ **{min(len(images), max_ai_frames)}** å¸§åšäº† AI åˆ†æã€‚")
                st.markdown("---")

                for i, (img, info, palette) in enumerate(zip(images, frame_infos, frame_palettes)):
                    with st.container():
                        st.markdown(f"### ğŸ“˜ å…³é”®å¸§ {i + 1}")
                        c1, c2 = st.columns([1.2, 2])

                        with c1:
                            st.image(img, caption=f"ç¬¬ {i + 1} å¸§ç”»é¢", width=DISPLAY_IMAGE_WIDTH)
                            palette_img = make_palette_image(palette)
                            st.image(palette_img, caption="ä¸»è‰²è°ƒè‰²å¡", width=PALETTE_WIDTH)
                            if palette:
                                st.caption("ä¸»è‰² HEXï¼š" + ", ".join(rgb_to_hex(c) for c in palette))

                        with c2:
                            cam = info.get("camera", {})
                            tags = info.get("tags_zh", [])

                            analysis_lines = [
                                f"ã€æ™¯åˆ«ã€‘{cam.get('shot_type_zh', '')}",
                                f"ã€è¿é•œã€‘{cam.get('movement_zh', '')}",
                                f"ã€æ‹æ‘„è§’åº¦ã€‘{cam.get('angle_zh', '')}",
                                f"ã€æ„å›¾ã€‘{cam.get('composition_zh', '')}",
                                f"ã€è‰²å½©ä¸å…‰å½±ã€‘{info.get('color_and_light_zh', '')}",
                                f"ã€ç”»é¢å†…å®¹ã€‘{info.get('scene_description_zh', '')}",
                                f"ã€æƒ…ç»ªæ°›å›´ã€‘{info.get('mood_zh', '')}",
                                f"ã€å…³é”®è¯æ ‡ç­¾ã€‘{' '.join(tags)}",
                            ]
                            st.markdown("**åˆ†é•œåˆ†æï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                            st.code("\n".join([x for x in analysis_lines if x.strip()]), language="markdown")

                            st.markdown("**äººç‰©åŠ¨ä½œç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                            st.code(info.get("character_action_detail_zh") or "ï¼ˆæš‚æ— åŠ¨ä½œç»†èŠ‚ï¼‰", language="markdown")

                            st.markdown("**åœºæ™¯ç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                            scene_detail = info.get("environment_detail_zh", "")
                            props_detail = info.get("props_and_tech_detail_zh", "")
                            scene_text = (scene_detail + "\n\né“å…·ä¸ç§‘æŠ€å…ƒç´ ï¼š" + props_detail).strip()
                            st.code(scene_text or "ï¼ˆæš‚æ— åœºæ™¯ç»†èŠ‚ï¼‰", language="markdown")

                            advanced = []
                            for k, title in [
                                ("face_expression_detail_zh", "é¢éƒ¨ä¸çœ¼ç¥"),
                                ("cloth_hair_reaction_zh", "æœè£…ä¸å¤´å‘"),
                                ("weather_force_detail_zh", "å¤©æ°”ä¸ç¯å¢ƒåŠ›"),
                                ("physics_reaction_detail_zh", "å—åŠ›ä¸ç‰©ç†åé¦ˆ"),
                                ("structure_damage_detail_zh", "ç»“æ„æŸå"),
                                ("debris_motion_detail_zh", "ç¢ç‰‡é£æ•£"),
                                ("fx_detail_zh", "ç‰¹æ•ˆä¸ç²’å­"),
                                ("lighting_color_detail_zh", "å…‰çº¿ç»†èŠ‚"),
                                ("audio_cue_detail_zh", "å£°éŸ³ä¸èŠ‚å¥"),
                                ("edit_rhythm_detail_zh", "å‰ªè¾‘èŠ‚å¥"),
                            ]:
                                v = info.get(k) or ""
                                if v.strip():
                                    advanced.append(f"{v.strip()}")
                            if advanced:
                                st.markdown("**é«˜çº§ç‰©ç† / ç¯å¢ƒç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                                st.code("\n".join(advanced), language="markdown")

                            st.markdown("**SORA / VEO è§†é¢‘æç¤ºè¯ï¼ˆè‹±æ–‡ï¼Œå¯å¤åˆ¶ï¼‰ï¼š**")
                            st.code(info.get("video_prompt_en") or "ï¼ˆæš‚æ— è§†é¢‘æç¤ºè¯ï¼‰", language="markdown")

                            st.markdown("**Midjourney é™å¸§æç¤ºè¯ï¼ˆå¯é€‰ï¼‰ï¼š**")
                            st.code(info.get("midjourney_prompt") or "ï¼ˆæš‚æ—  Midjourney æç¤ºè¯ï¼‰", language="markdown")

                        st.markdown("---")

            with tab_story:
                st.markdown("### ğŸ“š æ•´ä½“å‰§æƒ…ä¸è§†å¬é£æ ¼æ€»ç»“")
                st.code(overall, language="markdown")
                st.markdown("### ğŸ¤ 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬")
                st.code(ad_script, language="markdown")
                st.markdown("### ğŸ¬ æ—¶é—´è½´åˆ†é•œè„šæœ¬ï¼ˆå¯å¤åˆ¶ï¼‰")
                st.code(timeline_shotlist, language="markdown")

            with tab_json:
                st.markdown("### ğŸ“¦ ä¸‹è½½æœ¬æ¬¡åˆ†æçš„ JSON æ–‡ä»¶")
                st.download_button(
                    label="â¬‡ï¸ ä¸‹è½½æœ¬æ¬¡ video_analysis.json",
                    data=json_str,
                    file_name="video_analysis.json",
                    mime="application/json",
                )
                with st.expander("ğŸ” é¢„è§ˆéƒ¨åˆ† JSON å†…å®¹"):
                    preview = json_str[:3000] + ("\n...\n" if len(json_str) > 3000 else "")
                    st.code(preview, language="json")

            with tab_history:
                st.markdown("### ğŸ•˜ å½“å‰ä¼šè¯å†å²è®°å½•ï¼ˆåˆ·æ–°é¡µé¢ä¼šæ¸…ç©ºï¼‰")
                history = st.session_state.get("analysis_history", [])
                if not history:
                    st.info("å½“å‰ä¼šè¯è¿˜æ²¡æœ‰ä»»ä½•å†å²è®°å½•ã€‚")
                else:
                    options = [
                        f"{len(history) - i}. {h['created_at']} | {h['meta'].get('source_label','')} | "
                        f"{h['meta'].get('frame_count',0)} å¸§ | åŒºé—´ {h['meta'].get('start_sec_used',0):.1f}-{h['meta'].get('end_sec_used',0):.1f}s"
                        for i, h in enumerate(reversed(history))
                    ]
                    idx_display = st.selectbox(
                        "é€‰æ‹©ä¸€æ¡å†å²è®°å½•æŸ¥çœ‹",
                        options=list(range(len(history))),
                        format_func=lambda i: options[i],
                    )
                    real_index = len(history) - 1 - idx_display
                    selected = history[real_index]

                    st.markdown(
                        f"**IDï¼š** `{selected['id']}`  \n"
                        f"**æ—¶é—´ï¼š** {selected['created_at']}  \n"
                        f"**æ¥æºç±»å‹ï¼š** {selected['meta'].get('source_type','')}  \n"
                        f"**æ¥æºæ ‡è¯†ï¼š** {selected['meta'].get('source_label','')}  \n"
                        f"**åˆ†æåŒºé—´ï¼š** {selected['meta'].get('start_sec_used',0):.1f}â€“{selected['meta'].get('end_sec_used',0):.1f} ç§’  \n"
                        f"**å¸§æ•°ï¼š** {selected['meta'].get('frame_count',0)}  \n"
                        f"**è§†è§‰æ¨¡å‹ï¼š** {selected['meta'].get('vision_model','')}  \n"
                        f"**æ–‡æœ¬æ¨¡å‹ï¼š** {selected['meta'].get('text_model','')}"
                    )

                    hist_json = json.dumps(selected["data"], ensure_ascii=False, indent=2)
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½è¯¥å†å²è®°å½• JSON",
                        data=hist_json,
                        file_name=f"video_analysis_{selected['id']}.json",
                        mime="application/json",
                    )

                    frames = selected["data"].get("frames", [])
                    if frames:
                        st.markdown("#### éƒ¨åˆ†å¸§é¢„è§ˆï¼ˆä¸­æ–‡åœºæ™¯ + è‹±æ–‡è§†é¢‘æç¤ºè¯ï¼‰")
                        for f in frames[:3]:
                            st.markdown(f"**ç¬¬ {f.get('index')} å¸§ï¼š**")
                            st.write(f.get("scene_description_zh", ""))
                            vp = f.get("video_prompt_en", "")
                            if vp:
                                st.code(vp, language="markdown")
                            st.markdown("---")

        except Exception as e:
            st.error(f"ä¸‹è½½æˆ–è§£æè§†é¢‘æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
