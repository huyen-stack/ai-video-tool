import streamlit as st
import google.generativeai as genai
import tempfile
import os
import cv2
import numpy as np
from PIL import Image, ImageDraw
import concurrent.futures
import json


# ========================
# å…¨å±€é…ç½®
# ========================

# å¯æŒ‰éœ€æ›¿æ¢ï¼š
#   "gemini-flash-latest"
#   "gemini-2.5-flash-lite"
#   "gemini-2.5-flash"
GEMINI_MODEL_NAME = "gemini-flash-latest"

# å±•ç¤ºæ—¶çš„å›¾ç‰‡ä¸è‰²å¡å®½åº¦
DISPLAY_IMAGE_WIDTH = 320
PALETTE_WIDTH = 320
PALETTE_HEIGHT = 26


# ========================
# é¡µé¢ / å…¨å±€æ ·å¼
# ========================

st.set_page_config(
    page_title="AI è‡ªåŠ¨å…³é”®å¸§åˆ†é•œ & Midjourney æç¤ºè¯åŠ©æ‰‹",
    page_icon="ğŸ¬",
    layout="wide",
)

st.markdown(
    """
    <style>
    .main {
        background-color: #0f172a;
        color: #e5e7eb;
    }
    .stMarkdown, .stText {
        color: #e5e7eb;
    }
    .stCode {
        font-size: 0.85rem !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# é¡¶éƒ¨ Hero
st.markdown(
    """
    <div style="
        padding: 18px 24px;
        border-radius: 18px;
        margin-bottom: 16px;
        background: radial-gradient(circle at top left, #38bdf8 0, #0f172a 45%, #020617 100%);
        border: 1px solid rgba(148, 163, 184, 0.35);
    ">
      <h1 style="margin: 0 0 8px 0; color: #e5e7eb; font-size: 1.6rem;">
        ğŸ¬ AI è‡ªåŠ¨å…³é”®å¸§åˆ†é•œåŠ©æ‰‹ Pro Â· Midjourney æç¤ºè¯ç‰ˆ
      </h1>
      <p style="margin: 0; color: #cbd5f5; font-size: 0.96rem;">
        ä¸Šä¼ ä¸€ä¸ªè§†é¢‘ï¼Œè‡ªåŠ¨æŠ½å–å…³é”®å¸§ï¼Œç”Ÿæˆ
        <b>ç»“æ„åŒ– JSON + Midjourney æç¤ºè¯ + åˆ†é•œè§£è¯» + å‰§æƒ…å¤§çº² + 10 ç§’å¹¿å‘Šæ—ç™½</b>ï¼Œ
        ç›´æ¥å½“ã€ŒAI å¯¼æ¼” + MJ æç¤ºè¯å·¥ç¨‹å¸ˆã€ä½¿ç”¨ã€‚
      </p>
    </div>
    """,
    unsafe_allow_html=True,
)


# ========================
# æŠ½å…³é”®å¸§ï¼ˆæ ¹æ®æ—¶é•¿è‡ªåŠ¨å†³å®šæ•°é‡ï¼‰
# ========================

def extract_keyframes_dynamic(
    video_path: str,
    min_frames: int = 6,
    max_frames: int = 30,
    base_fps: float = 0.8,
):
    """
    æ ¹æ®è§†é¢‘æ—¶é•¿è‡ªåŠ¨æŠ½å–å…³é”®å¸§ï¼š
    - ä¼°ç®—ç›®æ ‡å¸§æ•°: ideal_n = duration * base_fps
    - é™åˆ¶åœ¨ [min_frames, max_frames]
    - å‡åŒ€æŠ½å¸§
    è¿”å› PIL.Image åˆ—è¡¨ã€‚
    """
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps is None or fps <= 1e-2:
        fps = 25.0

    if total_frames <= 0:
        cap.release()
        return [], 0.0

    duration = total_frames / fps  # ç§’
    ideal_n = int(duration * base_fps)
    target_n = max(min_frames, ideal_n)
    target_n = min(target_n, max_frames, total_frames)

    if target_n <= 0:
        cap.release()
        return [], duration

    step = total_frames / float(target_n)
    frame_indices = [int(i * step) for i in range(target_n)]

    images = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret and frame is not None:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            images.append(Image.fromarray(rgb_frame))
        else:
            images.append(Image.new("RGB", (200, 200), color="gray"))

    cap.release()
    return images, duration


# ========================
# ä¸»è‰²è°ƒè‰²å¡ç›¸å…³
# ========================

def get_color_palette(pil_img: Image.Image, num_colors: int = 5):
    """
    ä½¿ç”¨ KMeans èšç±»æå–å›¾ç‰‡ä¸»è‰²è°ƒï¼Œè¿”å› [(R,G,B), ...]ã€‚
    """
    img = pil_img.resize((120, 120))
    arr = np.array(img)
    data = arr.reshape((-1, 3)).astype(np.float32)

    criteria = (
        cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,
        20,
        1.0,
    )
    flags = cv2.KMEANS_RANDOM_CENTERS

    _, labels, centers = cv2.kmeans(
        data,
        num_colors,
        None,
        criteria,
        10,
        flags,
    )
    centers = centers.astype(int)
    colors = [tuple(map(int, c)) for c in centers]
    return colors


def make_palette_image(colors, width: int = PALETTE_WIDTH, height: int = PALETTE_HEIGHT):
    """
    æŠŠä¸€ç»„ RGB é¢œè‰²ç”»æˆä¸€æ¡æ°´å¹³è‰²å¡æ¡ã€‚
    """
    if not colors:
        return Image.new("RGB", (width, height), color="gray")

    bar = Image.new("RGB", (width, height))
    draw = ImageDraw.Draw(bar)
    n = len(colors)
    band_width = max(width // n, 1)

    for i, color in enumerate(colors):
        x0 = i * band_width
        x1 = width if i == n - 1 else (i + 1) * band_width
        draw.rectangle([x0, 0, x1, height], fill=color)

    return bar


def rgb_to_hex(rgb_tuple):
    r, g, b = rgb_tuple
    return "#{:02X}{:02X}{:02X}".format(r, g, b)


# ========================
# è§£æ Gemini è¿”å›
# ========================

def _extract_text_from_response(resp) -> str:
    """
    å…¼å®¹ä¸åŒç‰ˆæœ¬ SDK çš„ Gemini å“åº”è§£æã€‚
    """
    text = getattr(resp, "text", None)
    if text and isinstance(text, str) and text.strip():
        return text.strip()

    try:
        texts = []
        for cand in getattr(resp, "candidates", []) or []:
            content = getattr(cand, "content", None)
            if not content:
                continue
            for part in getattr(content, "parts", []) or []:
                part_text = getattr(part, "text", None)
                if part_text:
                    texts.append(part_text)
        if texts:
            return " ".join(texts).strip()
    except Exception:
        pass

    try:
        return str(resp)
    except Exception:
        return ""


# ========================
# å•å¸§åˆ†æï¼šç”Ÿæˆç»“æ„åŒ– JSON + MJ æç¤ºè¯
# ========================

def analyze_single_image(img: Image.Image, model, index: int):
    """
    è¾“å‡ºä¸€ä¸ªç»“æ„åŒ– dictï¼š
    {
      "index": index,
      "scene_description_zh": ...,
      "tags_zh": [...],
      "camera": {
        "shot_type_zh": ...,
        "angle_zh": ...,
        "movement_zh": ...,
        "composition_zh": ...
      },
      "color_and_light_zh": ...,
      "mood_zh": ...,
      "midjourney_prompt": ...,
      "midjourney_negative_prompt": ...
    }
    """
    try:
        prompt = f"""
ä½ ç°åœ¨æ˜¯ä¸“ä¸šçš„ç”µå½±åˆ†é•œå¸ˆ + æç¤ºè¯å·¥ç¨‹å¸ˆã€‚
è¯·ä»”ç»†åˆ†æç»™ä½ çš„è¿™ä¸€å¸§ç”»é¢ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œç”¨äºåœ¨ Midjourney ä¸­å¤ç°è¯¥ç”»é¢ã€‚

å¿…é¡»ä½¿ç”¨ä¸‹é¢è¿™äº› keyï¼ˆè‹±æ–‡ï¼‰ï¼Œvalue å¤šä¸ºä¸­æ–‡è¯´æ˜ï¼ŒMidjourney ç›¸å…³å­—æ®µä¸ºè‹±æ–‡ï¼š

{{
  "index": æ•´æ•°ï¼Œå½“å‰å¸§åºå·ï¼Œå›ºå®šä¸º {index},
  "scene_description_zh": "ä¸€å¥å®Œæ•´çš„ä¸­æ–‡å¥å­ï¼Œæè¿°ç”»é¢ä¸­æ˜¯è°åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åšä»€ä¹ˆ",
  "tags_zh": ["#çŸ­ä¸­æ–‡æ ‡ç­¾1", "#æ ‡ç­¾2", "..."],
  "camera": {{
    "shot_type_zh": "æ™¯åˆ«ï¼Œä¾‹å¦‚ï¼šè¿œæ™¯ / å…¨æ™¯ / ä¸­æ™¯ / è¿‘æ™¯ / ç‰¹å†™",
    "angle_zh": "æ‹æ‘„è§’åº¦ï¼Œä¾‹å¦‚ï¼šä¿¯æ‹ / ä»°æ‹ / å¹³è§† / ä¸Šå¸è§†è§’",
    "movement_zh": "è¿é•œæ–¹å¼ï¼Œä¾‹å¦‚ï¼šé™æ­¢é•œå¤´ / è½»å¾®æ¨é•œ / è·Ÿæ‹ ç­‰",
    "composition_zh": "æ„å›¾æ–¹å¼ï¼Œä¾‹å¦‚ï¼šä¸‰åˆ†æ³•æ„å›¾ / ä¸­å¿ƒæ„å›¾ / å¯¹ç§°æ„å›¾ / å‰æ™¯-ä¸»ä½“-èƒŒæ™¯ ç­‰"
  }},
  "color_and_light_zh": "ç”¨ä¸€ä¸¤å¥ä¸­æ–‡æè¿°ç”»é¢çš„è‰²è°ƒå’Œå…‰çº¿ï¼Œä¾‹å¦‚ï¼šæ•´ä½“åæš–ï¼Œé«˜è°ƒæŸ”å’Œé€†å…‰ï¼Œç²‰è‰²å’Œç±³ç™½ä¸ºä¸»è‰²",
  "mood_zh": "ç”¨ä¸­æ–‡æ¦‚æ‹¬æƒ…ç»ªæ°›å›´ï¼Œä¾‹å¦‚ï¼šäº²åˆ‡ã€ç”œç¾ã€å¸¦è´§åˆ†äº«æ°›å›´",
  "midjourney_prompt": "ä¸€è¡Œè‹±æ–‡ Midjourney v6 æç¤ºè¯ï¼Œç”¨é€—å·åˆ†éš”çŸ­è¯­ï¼Œå°½é‡ç²¾ç¡®æè¿°äººç‰©å¤–è§‚ã€å§¿æ€ã€åœºæ™¯ã€é“å…·ã€å…‰çº¿ã€è‰²è°ƒã€æ„å›¾å’Œæ°›å›´ï¼Œé€‚åˆ 9:16 ç«–ç‰ˆï¼Œç»“å°¾åŠ  --ar 9:16 --v 6.0 --style raw",
  "midjourney_negative_prompt": "ä¸€è¡Œè‹±æ–‡è´Ÿé¢æç¤ºè¯ï¼Œä¾‹å¦‚ï¼štext, subtitle, watermark, extra fingers, deformed hands, distorted face, low resolution, blurry, cartoon, anime, painting"
}}

è¦æ±‚ï¼š
1. åªè¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚
2. æ‰€æœ‰å­—ç¬¦ä¸²ä½¿ç”¨åŒå¼•å·ï¼Œä¸è¦ä½¿ç”¨å•å¼•å·ã€‚
3. JSON ä¸­ä¸èƒ½æœ‰æ³¨é‡Šï¼Œä¸èƒ½æœ‰å¤šä½™çš„é€—å·ã€‚
4. midjourney_prompt å¿…é¡»æ˜¯è‹±æ–‡ï¼Œé€‚åˆç›´æ¥ç²˜è´´ç»™ Midjourney v6ã€‚
"""
        resp = model.generate_content([prompt, img])
        text = _extract_text_from_response(resp)
        if not text:
            raise ValueError("æ¨¡å‹æœªè¿”å›æ–‡æœ¬")

        # å°è¯•ä»æ–‡æœ¬ä¸­æˆªå– JSON å­ä¸²
        start = text.find("{")
        end = text.rfind("}")
        if start == -1 or end == -1 or end <= start:
            raise ValueError("æœªæ£€æµ‹åˆ°æœ‰æ•ˆ JSON ç»“æ„")

        json_str = text[start : end + 1]
        info = json.loads(json_str)

        # ç¡®ä¿ index å­˜åœ¨ä¸”æ­£ç¡®
        info["index"] = index

        # å¡«å……é»˜è®¤ç»“æ„ï¼Œé¿å…åç»­ KeyError
        info.setdefault("scene_description_zh", "")
        info.setdefault("tags_zh", [])
        info.setdefault("camera", {})
        info.setdefault("color_and_light_zh", "")
        info.setdefault("mood_zh", "")
        info.setdefault("midjourney_prompt", "")
        info.setdefault("midjourney_negative_prompt", "")
        cam = info["camera"]
        cam.setdefault("shot_type_zh", "")
        cam.setdefault("angle_zh", "")
        cam.setdefault("movement_zh", "")
        cam.setdefault("composition_zh", "")

        return info

    except Exception as e:
        # è§£æå¤±è´¥æ—¶è¿”å›ä¸€ä¸ªå ä½ç»“æ„
        return {
            "index": index,
            "scene_description_zh": f"ï¼ˆAI åˆ†æå¤±è´¥ï¼š{e}ï¼‰",
            "tags_zh": [],
            "camera": {
                "shot_type_zh": "",
                "angle_zh": "",
                "movement_zh": "",
                "composition_zh": "",
            },
            "color_and_light_zh": "",
            "mood_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
        }


def analyze_images_concurrently(images, model, max_ai_frames: int):
    """
    å¹¶å‘åˆ†æå¤šå¼ å›¾ç‰‡ï¼ŒåŠ é€Ÿæ•´ä½“é€Ÿåº¦ã€‚
    åªå¯¹å‰ max_ai_frames å¸§åš AI è°ƒç”¨ï¼Œå…¶ä½™å¸§ç”¨å ä½è¯´æ˜ã€‚
    è¿”å›ï¼šé•¿åº¦ç­‰äº images çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸Šé¢å®šä¹‰çš„ dictã€‚
    """
    n = len(images)
    if n == 0:
        return []

    use_n = min(max_ai_frames, n)
    results = [None] * n

    status = st.empty()
    status.info(f"âš¡ æ­£åœ¨å¯¹å‰ {use_n} å¸§è¿›è¡Œ AI åˆ†æï¼ˆå…± {n} å¸§ï¼‰ï¼Œå…¶ä½™å¸§ä¿ç•™æˆªå›¾ä¸è‰²å¡ã€‚")

    # å…ˆå¯¹éœ€è¦åˆ†æçš„å¸§å¹¶å‘è°ƒç”¨
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(use_n, 6)) as executor:
        future_to_index = {
            executor.submit(analyze_single_image, images[i], model, i + 1): i
            for i in range(use_n)
        }
        for future in concurrent.futures.as_completed(future_to_index):
            i = future_to_index[future]
            try:
                results[i] = future.result()
            except Exception as e:
                results[i] = {
                    "index": i + 1,
                    "scene_description_zh": f"ï¼ˆAI åˆ†æå¤±è´¥ï¼š{e}ï¼‰",
                    "tags_zh": [],
                    "camera": {
                        "shot_type_zh": "",
                        "angle_zh": "",
                        "movement_zh": "",
                        "composition_zh": "",
                    },
                    "color_and_light_zh": "",
                    "mood_zh": "",
                    "midjourney_prompt": "",
                    "midjourney_negative_prompt": "",
                }

    # å¯¹æœªåˆ†æçš„å¸§å¡«å……å ä½
    for i in range(use_n, n):
        results[i] = {
            "index": i + 1,
            "scene_description_zh": "ï¼ˆæœ¬å¸§æœªåš AI åˆ†æï¼Œç”¨äºèŠ‚çœå½“å‰ API é…é¢ï¼Œä½†ä»å¯ç”¨äºè§†è§‰å‚è€ƒå’Œè‰²å¡ã€‚ï¼‰",
            "tags_zh": [],
            "camera": {
                "shot_type_zh": "",
                "angle_zh": "",
                "movement_zh": "",
                "composition_zh": "",
            },
            "color_and_light_zh": "",
            "mood_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
        }

    status.empty()
    return results


# ========================
# æ•´ä½“è§†é¢‘å±‚é¢çš„æ€»ç»“
# ========================

def analyze_overall_video(frame_infos, model):
    """
    ä½¿ç”¨å·²æœ‰çš„å¸§çº§ä¿¡æ¯ï¼Œç”Ÿæˆæ•´æ®µè§†é¢‘çš„å‰§æƒ…å¤§çº²ç­‰ã€‚
    """
    described = [
        info
        for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆæ•´ä½“å‰§æƒ…å¤§çº²ã€‚ï¼‰"

    parts = []
    for info in described:
        idx = info["index"]
        cam = info.get("camera", {})
        tags = info.get("tags_zh", [])
        part = (
            f"ç¬¬ {idx} å¸§ï¼š{info.get('scene_description_zh', '')}\n"
            f"æ™¯åˆ«ï¼š{cam.get('shot_type_zh', '')}ï¼›è§’åº¦ï¼š{cam.get('angle_zh', '')}ï¼›è¿é•œï¼š{cam.get('movement_zh', '')}ï¼›æ„å›¾ï¼š{cam.get('composition_zh', '')}\n"
            f"è‰²å½©ä¸å…‰å½±ï¼š{info.get('color_and_light_zh', '')}\n"
            f"æƒ…ç»ªæ°›å›´ï¼š{info.get('mood_zh', '')}\n"
            f"æ ‡ç­¾ï¼š{'ã€'.join(tags)}"
        )
        parts.append(part)

    joined = "\n\n".join(parts)

    prompt = f"""
ä½ ç°åœ¨æ˜¯èµ„æ·±è§†é¢‘å¯¼æ¼” + å‰ªè¾‘å¸ˆ + çŸ­è§†é¢‘è¿è¥ä¸“å®¶ + å†…å®¹åˆè§„å®¡æ ¸å‘˜ã€‚
ä¸‹é¢æ˜¯ä»ä¸€æ®µè§†é¢‘ä¸­æŠ½å–çš„è‹¥å¹²å…³é”®å¸§çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›è¯´æ˜ï¼Œå¯¹æ•´æ®µè§†é¢‘åšæ•´ä½“åˆ†æã€‚

=== å…³é”®å¸§è¯´æ˜å¼€å§‹ ===
{joined}
=== å…³é”®å¸§è¯´æ˜ç»“æŸ ===

è¯·ä¸¥æ ¼æŒ‰ä¸‹é¢ç»“æ„è¾“å‡ºä¸­æ–‡åˆ†æï¼š

ã€å‰§æƒ…å¤§çº²ã€‘
ç”¨ 2-4 å¥æ¦‚æ‹¬è¿™æ®µè§†é¢‘çš„å¤§è‡´å†…å®¹/äººç‰©å…³ç³»/å‘ç”Ÿåœºæ™¯ã€‚

ã€æ•´ä½“è§†å¬é£æ ¼ã€‘
ä»èŠ‚å¥å¿«æ…¢ã€é•œå¤´æ„Ÿã€è‰²å½©æ°”è´¨ï¼ˆæš–/å†·/æ—¥å¸¸/æ¢¦å¹»ï¼‰ã€æƒ…ç»ªæ°›å›´ç­‰è§’åº¦æ€»ç»“æ•´ä½“é£æ ¼ã€‚

ã€é€‚åˆçš„è¯é¢˜æ ‡ç­¾ã€‘
ç”¨ #æ ‡ç­¾ å½¢å¼ç»™å‡º 5-10 ä¸ªï¼Œé€‚åˆæŠ–éŸ³/å°çº¢ä¹¦/è§†é¢‘å·ç­‰å¹³å°ï¼Œä¾‹å¦‚ï¼š
#åŸå¸‚å¤œæ™¯ #æ²»æ„ˆè‡ªæ‹ #æ°›å›´æ„Ÿç¾å¥³

ã€å•†ä¸šä¸åˆè§„é£é™©ã€‘
ä»â€œè¡€è…¥/æš´åŠ›/è‰²æƒ…/æ”¿æ²»/å“ç‰Œå•†æ ‡â€ç­‰ç»´åº¦ï¼Œç®€å•è¯„ä¼°ï¼š
æ•´ä½“é£é™©çº§åˆ«ï¼šä½ / ä¸­ / é«˜
å¹¶ç”¨ 2-3 å¥è¯è¯´æ˜éœ€è¦æ³¨æ„çš„ç‚¹ï¼ˆä¾‹å¦‚ï¼šæœè£…æš´éœ²ç¨‹åº¦ã€æœªæˆå¹´äººå½¢è±¡ã€æ˜¯å¦æœ‰æ˜æ˜¾å“ç‰Œ Logo ç­‰ï¼‰ã€‚

è¯·ç›´æ¥è¾“å‡ºä»¥ä¸Š 4 ä¸ªå°èŠ‚ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
    try:
        resp = model.generate_content(prompt)
        return _extract_text_from_response(resp)
    except Exception as e:
        return f"æ•´ä½“åˆ†æå¤±è´¥ï¼š{e}"


# ========================
# 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬ç”Ÿæˆ
# ========================

def generate_ad_script(frame_infos, model):
    """
    åŸºäºè‹¥å¹²å…³é”®å¸§çš„åˆ†æï¼Œç”Ÿæˆä¸€æ¡ 10 ç§’å·¦å³çš„ä¸­æ–‡å¹¿å‘Šæ—ç™½è„šæœ¬ã€‚
    """
    described = [
        info
        for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆå¹¿å‘Šæ—ç™½è„šæœ¬ã€‚ï¼‰"

    parts = []
    for info in described:
        idx = info["index"]
        tags = info.get("tags_zh", [])
        parts.append(
            f"ç¬¬ {idx} å¸§ï¼š{info.get('scene_description_zh', '')}ï¼›æ ‡ç­¾ï¼š{'ã€'.join(tags)}"
        )
    joined = "\n".join(parts)

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±å¹¿å‘Šå¯¼æ¼” + æ–‡æ¡ˆã€‚
æˆ‘æœ‰ä¸€ä¸ªç”±è‹¥å¹²ç”»é¢ç»„æˆçš„ç«–ç‰ˆçŸ­è§†é¢‘ï¼Œæ—¶é•¿å¤§çº¦ 8-12 ç§’ã€‚
ä¸‹é¢æ˜¯æ¯ä¸ªç”»é¢çš„ç®€è¦è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›ä¿¡æ¯ï¼Œå†™ä¸€æ¡é€‚åˆé…åˆè¿™äº›ç”»é¢æ’­æ”¾çš„ä¸­æ–‡å¹¿å‘Šæ—ç™½è„šæœ¬ã€‚

=== å…³é”®å¸§æ¦‚è§ˆ ===
{joined}
=== å…³é”®å¸§æ¦‚è§ˆç»“æŸ ===

è¦æ±‚ï¼š
1. æ—ç™½æ€»æ—¶é•¿æ§åˆ¶åœ¨ 8-12 ç§’å·¦å³ï¼ˆæ­£å¸¸è¯­é€Ÿï¼‰ï¼Œæ–‡æœ¬ 35-70 å­—å³å¯ã€‚
2. é£æ ¼ä¸ç”»é¢è°ƒæ€§åŒ¹é…ï¼ˆå¦‚æœæ˜¯æ°›å›´æ„Ÿè‡ªæ‹ï¼Œå°±åæƒ…ç»ª/ç”Ÿæ´»æ–¹å¼ï¼›å¦‚æœæ˜¯äº§å“å±•ç¤ºï¼Œå°±å¤šè®²å–ç‚¹ï¼‰ã€‚
3. ç”¨è‡ªç„¶å£è¯­åŒ–ä¸­æ–‡ï¼Œä¸è¦å‡ºç°â€œç”»é¢ä¸­â€â€œé•œå¤´é‡Œâ€è¿™ç±»å­—çœ¼ï¼Œç›´æ¥å¯¹è§‚ä¼—è¯´è¯ã€‚
4. å¦‚æœç”»é¢çœ‹èµ·æ¥åƒä¸ªäººç”Ÿæ´» vlogï¼Œå¯ä»¥å¼±åŒ–â€œè´­ä¹°å·å¬â€ï¼Œæ›´åå‘æƒ…ç»ªæ„ŸæŸ“ã€‚
5. å¦‚æœç”»é¢ä¸­æœ‰æ˜æ˜¾äº§å“æˆ–å“ç‰Œï¼ˆå¦‚é¥®æ–™ã€é›¶é£Ÿã€æŠ¤è‚¤å“ç­‰ï¼‰ï¼Œå¯ä»¥é€‚å½“åŠ å…¥æ¸©æŸ”çš„â€œç§è‰è¯æœ¯â€ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢æ ¼å¼è¾“å‡ºï¼š

ã€10ç§’å¹¿å‘Šæ—ç™½è„šæœ¬ã€‘
ï¼ˆåœ¨è¿™é‡Œå†™å®Œæ•´çš„ä¸€æ®µæ—ç™½ï¼Œä¸è¦æ‹†æˆå¤šè¡Œï¼Œä¸è¦æ ‡æ³¨é•œå¤´ç¼–å·ï¼‰

ä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹ã€‚
"""
    try:
        resp = model.generate_content(prompt)
        return _extract_text_from_response(resp)
    except Exception as e:
        return f"å¹¿å‘Šæ–‡æ¡ˆç”Ÿæˆå¤±è´¥ï¼š{e}"


# ========================
# ä¾§è¾¹æ ï¼šAPI Key & å‚æ•°è®¾ç½®
# ========================

with st.sidebar:
    st.header("ğŸ”‘ ç¬¬ä¸€æ­¥ï¼šé…ç½® Gemini API Key")
    api_key = st.text_input(
        "è¾“å…¥ Google API Key",
        type="password",
        help="ç²˜è´´ä½ çš„ Gemini API Keyï¼ˆé€šå¸¸ä»¥ AIza å¼€å¤´ï¼‰",
    )

    st.markdown("---")
    max_ai_frames = st.slider(
        "æœ¬æ¬¡æœ€å¤šåš AI åˆ†æçš„å¸§æ•°ï¼ˆæ¶ˆè€—é…é¢ï¼‰",
        min_value=4,
        max_value=20,
        value=10,
        step=1,
    )
    st.caption("å»ºè®®ï¼š10 ç§’è§†é¢‘ 6~10 å¸§å³å¯ï¼›è¶…å‡ºéƒ¨åˆ†ä»ä¼šæ˜¾ç¤ºæˆªå›¾å’Œè‰²å¡ï¼Œä½†ä¸è°ƒ AIã€‚")

    if not api_key:
        st.warning("ğŸ”´ è¿˜æ²¡æœ‰ Keyï¼Œå…ˆå» https://ai.google.dev/ ç”³è¯·ä¸€ä¸ª")
    else:
        st.success("ğŸŸ¢ Key å·²å°±ç»ª")


# ========================
# åˆå§‹åŒ– Gemini æ¨¡å‹
# ========================

model = None
if api_key:
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
    except Exception as e:
        st.error(f"âŒ åˆå§‹åŒ– Gemini æ¨¡å‹å¤±è´¥ï¼š{e}")
        model = None


# ========================
# ä¸»æµç¨‹ï¼šä¸Šä¼ è§†é¢‘ + æŠ½å¸§ + åˆ†æ + å¸ƒå±€å±•ç¤º
# ========================

uploaded_file = st.file_uploader(
    "ğŸ“‚ ç¬¬äºŒæ­¥ï¼šæ‹–å…¥è§†é¢‘æ–‡ä»¶ï¼ˆå»ºè®® < 50MBï¼‰",
    type=["mp4", "mov", "m4v", "avi", "mpeg"],
)

if uploaded_file and st.button("ğŸš€ ä¸€é”®è§£ææ•´æ¡è§†é¢‘"):
    if not api_key or model is None:
        st.error("è¯·å…ˆåœ¨å·¦ä¾§è¾“å…¥æœ‰æ•ˆçš„ Google API Keyã€‚")
    else:
        # 1. ä¿å­˜ä¸Šä¼ çš„è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        suffix = os.path.splitext(uploaded_file.name)[1] or ".mp4"
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name

        st.info("â³ æ­£åœ¨æ ¹æ®è§†é¢‘æ—¶é•¿è‡ªåŠ¨æŠ½å–å…³é”®å¸§...")
        images, duration = extract_keyframes_dynamic(tmp_path)

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(tmp_path)
        except OSError:
            pass

        if not images:
            st.error("âŒ æ— æ³•ä»è§†é¢‘ä¸­è¯»å–å¸§ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦æŸåæˆ–æ ¼å¼å¼‚å¸¸ã€‚")
        else:
            st.success(
                f"âœ… å·²æˆåŠŸæŠ½å– {len(images)} ä¸ªå…³é”®å¸§ï¼ˆè§†é¢‘çº¦ {duration:.1f} ç§’ï¼Œå½“å‰æœ€å¤šå¯¹ {max_ai_frames} å¸§åš AI åˆ†æï¼‰ã€‚"
            )

            # è®°å½•æ¯å¸§çš„ä¸»è‰²è°ƒ
            frame_palettes = []
            for img in images:
                try:
                    palette_colors = get_color_palette(img, num_colors=5)
                except Exception:
                    palette_colors = []
                frame_palettes.append(palette_colors)

            # 3. è°ƒç”¨ Gemini åšé€å¸§åˆ†æï¼ˆç»“æ„åŒ– JSON + MJ æç¤ºè¯ï¼‰
            with st.spinner("ğŸ§  æ­£åœ¨ä¸ºå…³é”®å¸§ç”Ÿæˆç»“æ„åŒ–åˆ†æ + Midjourney æç¤ºè¯..."):
                frame_infos = analyze_images_concurrently(
                    images, model, max_ai_frames=max_ai_frames
                )

            # 4. æ•´ä½“æ€»ç»“ & å¹¿å‘Šæ–‡æ¡ˆ
            with st.spinner("ğŸ“š æ­£åœ¨ç”Ÿæˆæ•´æ®µè§†é¢‘çš„å‰§æƒ…å¤§çº²ä¸è¯é¢˜æ ‡ç­¾..."):
                overall = analyze_overall_video(frame_infos, model)
            with st.spinner("ğŸ¤ æ­£åœ¨ç”Ÿæˆ 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬..."):
                ad_script = generate_ad_script(frame_infos, model)

            # 5. Tabs å¸ƒå±€ï¼šåƒç½‘ç«™é‚£æ ·åˆ†åŒºå±•ç¤º
            tab_frames, tab_story, tab_json = st.tabs(
                ["ğŸ å…³é”®å¸§ & MJ æç¤ºè¯", "ğŸ“š å‰§æƒ…æ€»ç»“ & å¹¿å‘Šæ—ç™½", "ğŸ“¦ JSON å¯¼å‡º"]
            )

            # --- Tab1ï¼šé€å¸§å¡ç‰‡å¸ƒå±€ ---
            with tab_frames:
                st.markdown(
                    f"å…±æŠ½å– **{len(images)}** ä¸ªå…³é”®å¸§ï¼Œå…¶ä¸­å‰ **{min(len(images), max_ai_frames)}** å¸§åšäº† AI åˆ†æå’Œ Midjourney æç¤ºè¯ç”Ÿæˆã€‚"
                )
                st.markdown("---")

                for i, (img, info, palette) in enumerate(
                    zip(images, frame_infos, frame_palettes)
                ):
                    with st.container():
                        st.markdown(f"### ğŸ“˜ å…³é”®å¸§ {i + 1}")

                        c1, c2 = st.columns([1.2, 2])

                        with c1:
                            st.image(
                                img,
                                caption=f"ç¬¬ {i + 1} å¸§ç”»é¢",
                                width=DISPLAY_IMAGE_WIDTH,
                            )
                            palette_img = make_palette_image(palette)
                            st.image(
                                palette_img,
                                caption="ä¸»è‰²è°ƒè‰²å¡",
                                width=PALETTE_WIDTH,
                            )
                            if palette:
                                hex_list = ", ".join(rgb_to_hex(c) for c in palette)
                                st.caption(f"ä¸»è‰² HEXï¼š{hex_list}")

                        with c2:
                            # ç”¨ JSON æ‹¼å‡ºä¸€ä¸ªâ€œ8 è¡Œåˆ†é•œåˆ†æâ€
                            cam = info.get("camera", {})
                            tags = info.get("tags_zh", [])
                            analysis_text = "\n".join(
                                [
                                    f"ã€æ™¯åˆ«ã€‘{cam.get('shot_type_zh', '')}",
                                    f"ã€è¿é•œã€‘{cam.get('movement_zh', '')}",
                                    f"ã€æ‹æ‘„è§’åº¦ã€‘{cam.get('angle_zh', '')}",
                                    f"ã€æ„å›¾ã€‘{cam.get('composition_zh', '')}",
                                    f"ã€è‰²å½©ä¸å…‰å½±ã€‘{info.get('color_and_light_zh', '')}",
                                    f"ã€ç”»é¢å†…å®¹ã€‘{info.get('scene_description_zh', '')}",
                                    f"ã€æƒ…ç»ªæ°›å›´ã€‘{info.get('mood_zh', '')}",
                                    f"ã€å…³é”®è¯æ ‡ç­¾ã€‘{' '.join(tags)}",
                                ]
                            ).strip()

                            st.markdown("**åˆ†é•œåˆ†æï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                            st.code(
                                analysis_text
                                or "ï¼ˆæš‚æ— åˆ†é•œåˆ†æï¼Œå¯èƒ½æœªåš AI åˆ†æï¼‰",
                                language="markdown",
                            )

                            st.markdown("**Midjourney æç¤ºè¯ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                            st.code(
                                info.get("midjourney_prompt")
                                or "ï¼ˆæš‚æ—  Midjourney æç¤ºè¯ï¼Œå¯èƒ½æœªåš AI åˆ†æï¼‰",
                                language="markdown",
                            )

                            st.markdown("**Midjourney è´Ÿé¢æç¤ºè¯ï¼ˆå¯é€‰ï¼‰ï¼š**")
                            st.code(
                                info.get("midjourney_negative_prompt") or "",
                                language="markdown",
                            )

                        st.markdown("---")

            # --- Tab2ï¼šæ•´ä½“åˆ†æ + å¹¿å‘Šæ–‡æ¡ˆ ---
            with tab_story:
                st.markdown("### ğŸ“š æ•´ä½“å‰§æƒ…ä¸è§†å¬é£æ ¼æ€»ç»“")
                st.code(overall, language="markdown")

                st.markdown("### ğŸ¤ 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬")
                st.code(ad_script, language="markdown")

            # --- Tab3ï¼šJSON å¯¼å‡º ---
            with tab_json:
                st.markdown("### ğŸ“¦ å¯¼å‡º JSON åˆ†æç»“æœ")

                export_frames = []
                for info, palette in zip(frame_infos, frame_palettes):
                    export_frames.append(
                        {
                            "index": info.get("index"),
                            "scene_description_zh": info.get(
                                "scene_description_zh", ""
                            ),
                            "tags_zh": info.get("tags_zh", []),
                            "camera": info.get("camera", {}),
                            "color_and_light_zh": info.get(
                                "color_and_light_zh", ""
                            ),
                            "mood_zh": info.get("mood_zh", ""),
                            "midjourney_prompt": info.get("midjourney_prompt", ""),
                            "midjourney_negative_prompt": info.get(
                                "midjourney_negative_prompt", ""
                            ),
                            "palette_rgb": [list(c) for c in (palette or [])],
                            "palette_hex": [rgb_to_hex(c) for c in (palette or [])],
                        }
                    )

                export_data = {
                    "meta": {
                        "model": GEMINI_MODEL_NAME,
                        "frame_count": len(images),
                        "max_ai_frames_this_run": max_ai_frames,
                    },
                    "frames": export_frames,
                    "overall_analysis": overall,
                    "ad_script_10s": ad_script,
                }

                json_str = json.dumps(export_data, ensure_ascii=False, indent=2)

                st.download_button(
                    label="â¬‡ï¸ ä¸‹è½½ JSON åˆ†ææ–‡ä»¶",
                    data=json_str,
                    file_name="video_analysis.json",
                    mime="application/json",
                )

                with st.expander("ğŸ” é¢„è§ˆéƒ¨åˆ† JSON å†…å®¹"):
                    preview = json_str[:3000] + (
                        "\n...\n" if len(json_str) > 3000 else ""
                    )
                    st.code(preview, language="json")
