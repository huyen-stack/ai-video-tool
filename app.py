import streamlit as st
import google.generativeai as genai
import tempfile
import os
import cv2
import numpy as np
from PIL import Image, ImageDraw
import concurrent.futures
import json
from datetime import datetime
import yt_dlp  # æŠ–éŸ³/Bç«™/TikTok/YouTube ä¸‹è½½
from typing import Optional, Tuple, List, Dict, Any

# ========================
# å…¨å±€é…ç½®
# ========================

GEMINI_MODEL_NAME = "gemini-flash-latest"  # å¯æ¢ gemini-2.5-flash-lite ç­‰
FREE_TIER_RPM_LIMIT = 10  # å…è´¹ç‰ˆå¤§çº¦æ¯åˆ†é’Ÿ 10 æ¬¡ generateContent

DISPLAY_IMAGE_WIDTH = 320
PALETTE_WIDTH = 320
PALETTE_HEIGHT = 26

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼šAPI Key + å†å²è®°å½•
if "api_key" not in st.session_state:
    st.session_state["api_key"] = ""
if "analysis_history" not in st.session_state:
    st.session_state["analysis_history"] = []


# ========================
# é¡µé¢æ ·å¼
# ========================

st.set_page_config(
    page_title="AI è‡ªåŠ¨å…³é”®å¸§åˆ†é•œ & è§†é¢‘æç¤ºè¯åŠ©æ‰‹",
    page_icon="ğŸ¬",
    layout="wide",
)

st.markdown(
    """
    <style>
    .main {
        background-color: #0f172a;
        color: #e5e7eb;
    }
    .stMarkdown, .stText {
        color: #e5e7eb;
    }
    .stCode {
        font-size: 0.85rem !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    """
    <div style="
        padding: 18px 24px;
        border-radius: 18px;
        margin-bottom: 16px;
        background: radial-gradient(circle at top left, #38bdf8 0, #0f172a 45%, #020617 100%);
        border: 1px solid rgba(148, 163, 184, 0.35);
    ">
      <h1 style="margin: 0 0 8px 0; color: #e5e7eb; font-size: 1.6rem;">
        ğŸ¬ AI è‡ªåŠ¨å…³é”®å¸§åˆ†é•œåŠ©æ‰‹ Pro Â· SORA/VEO æç¤ºè¯ + æ—¶é—´åŒºé—´ + å†å²è®°å½•
      </h1>
      <p style="margin: 0; color: #cbd5f5; font-size: 0.96rem;">
        ä¸Šä¼ è§†é¢‘æˆ–è¾“å…¥æŠ–éŸ³/Bç«™/TikTok/YouTube é“¾æ¥ï¼Œè®¾ç½®åˆ†ææ—¶é—´åŒºé—´ï¼Œè‡ªåŠ¨æŠ½å–å…³é”®å¸§ï¼Œç”Ÿæˆ
        <b>ç»“æ„åŒ– JSON + Midjourney æç¤ºè¯ + SORA/VEO è‹±æ–‡è§†é¢‘æç¤ºè¯ + å‰§æƒ…å¤§çº² + 10 ç§’å¹¿å‘Šæ—ç™½ + æ—¶é—´è½´æ€»è§ˆ</b>ï¼Œ
        å¹¶åœ¨å½“å‰ä¼šè¯ä¸­ä¿å­˜å¤šæ¡åˆ†æè®°å½•ï¼Œæ–¹ä¾¿å¯¹æ¯”ä¸ä¸‹è½½ã€‚
      </p>
    </div>
    """,
    unsafe_allow_html=True,
)


# ========================
# æŠ½å…³é”®å¸§ï¼ˆæ”¯æŒæ—¶é—´åŒºé—´ï¼‰
# ========================

def extract_keyframes_dynamic(
    video_path: str,
    min_frames: int = 6,
    max_frames: int = 30,
    base_fps: float = 0.8,
    start_sec: Optional[float] = None,
    end_sec: Optional[float] = None,
) -> Tuple[List[Image.Image], float, Tuple[float, float]]:
    """
    æ ¹æ®è§†é¢‘æ—¶é•¿è‡ªåŠ¨æŠ½å–å…³é”®å¸§ï¼Œä»…åœ¨ [start_sec, end_sec] èŒƒå›´å†…ã€‚
    è¿”å›ï¼š
      images: æŠ½åˆ°çš„ PIL.Image åˆ—è¡¨
      duration: æ•´æ¡è§†é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
      used_range: (start_used, end_used) å®é™…åˆ†ææ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
    """
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps is None or fps <= 1e-2:
        fps = 25.0

    if total_frames <= 0:
        cap.release()
        return [], 0.0, (0.0, 0.0)

    duration = total_frames / fps

    # è§„èŒƒæ—¶é—´èŒƒå›´
    if start_sec is None or start_sec < 0:
        start_sec = 0.0
    if end_sec is None or end_sec <= start_sec or end_sec > duration:
        end_sec = duration

    start_frame = int(start_sec * fps)
    end_frame_excl = min(total_frames, int(end_sec * fps))
    segment_frames = end_frame_excl - start_frame

    # åŒºé—´éæ³•åˆ™é€€å›æ•´æ®µ
    if segment_frames <= 0:
        start_sec = 0.0
        end_sec = duration
        start_frame = 0
        end_frame_excl = total_frames
        segment_frames = total_frames

    segment_duration = segment_frames / fps

    ideal_n = int(segment_duration * base_fps)
    target_n = max(min_frames, ideal_n)
    target_n = min(target_n, max_frames, segment_frames)

    if target_n <= 0:
        cap.release()
        return [], duration, (start_sec, end_sec)

    step = segment_frames / float(target_n)
    frame_indices = [start_frame + int(i * step) for i in range(target_n)]

    images: List[Image.Image] = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret and frame is not None:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            images.append(Image.fromarray(rgb_frame))
        else:
            images.append(Image.new("RGB", (200, 200), color="gray"))

    cap.release()
    return images, duration, (start_sec, end_sec)


# ========================
# ä»é“¾æ¥ä¸‹è½½è§†é¢‘
# ========================

def download_video_from_url(url: str) -> str:
    """ä½¿ç”¨ yt-dlp ä»ç»™å®š URL ä¸‹è½½è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œè¿”å›è·¯å¾„ã€‚"""
    if not url:
        raise ValueError("è§†é¢‘é“¾æ¥ä¸ºç©º")

    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tmp_path = tmp.name
    tmp.close()

    ydl_opts = {
        "format": "mp4/bestvideo+bestaudio/best",
        "outtmpl": tmp_path,
        "merge_output_format": "mp4",
        "quiet": True,
        "no_warnings": True,
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])

    return tmp_path


# ========================
# ä¸»è‰²è°ƒè‰²å¡
# ========================

def get_color_palette(pil_img: Image.Image, num_colors: int = 5):
    img_small = pil_img.resize((120, 120))
    arr = np.array(img_small)
    data = arr.reshape((-1, 3)).astype(np.float32)

    criteria = (
        cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,
        20,
        1.0,
    )
    flags = cv2.KMEANS_RANDOM_CENTERS

    _, labels, centers = cv2.kmeans(
        data,
        num_colors,
        None,
        criteria,
        10,
        flags,
    )
    centers = centers.astype(int)
    colors = [tuple(map(int, c)) for c in centers]
    return colors


def make_palette_image(colors, width: int = PALETTE_WIDTH, height: int = PALETTE_HEIGHT):
    if not colors:
        return Image.new("RGB", (width, height), color="gray")

    bar = Image.new("RGB", (width, height))
    draw = ImageDraw.Draw(bar)
    n = len(colors)
    band_width = max(width // n, 1)

    for i, color in enumerate(colors):
        x0 = i * band_width
        x1 = width if i == n - 1 else (i + 1) * band_width
        draw.rectangle([x0, 0, x1, height], fill=color)

    return bar


def rgb_to_hex(rgb_tuple):
    r, g, b = rgb_tuple
    return "#{:02X}{:02X}{:02X}".format(r, g, b)


# ========================
# è§£æ Gemini è¿”å›
# ========================

def _extract_text_from_response(resp) -> str:
    text = getattr(resp, "text", None)
    if text and isinstance(text, str) and text.strip():
        return text.strip()

    try:
        texts = []
        for cand in getattr(resp, "candidates", []) or []:
            content = getattr(cand, "content", None)
            if not content:
                continue
            for part in getattr(content, "parts", []) or []:
                part_text = getattr(part, "text", None)
                if part_text:
                    texts.append(part_text)
        if texts:
            return " ".join(texts).strip()
    except Exception:
        pass

    try:
        return str(resp)
    except Exception:
        return ""


# ========================
# å•å¸§åˆ†æï¼šç»“æ„åŒ– JSON + MJ æç¤ºè¯ + è§†é¢‘æç¤ºè¯
# ========================

def analyze_single_image(img: Image.Image, model, index: int) -> Dict[str, Any]:
    """
    å¯¹å•å¸§åšå…¨é¢åˆ†æï¼š
    - ä¸­æ–‡åˆ†é•œï¼ˆæ™¯åˆ«/æœºä½/å…‰çº¿/æƒ…ç»ª/æ ‡ç­¾ï¼‰
    - äººç‰©æœé¥°/è¡¨æƒ…/åŠ¨ä½œ/é“å…·
    - åœºæ™¯ç»†èŠ‚ / å¤©æ°” / ç‰©ç†å—åŠ› / ç»“æ„æŸå / ç¢ç‰‡ / ç‰¹æ•ˆ
    - Midjourney æç¤ºè¯
    - SORA/VEO ç”¨è‹±æ–‡è§†é¢‘æç¤ºè¯ video_prompt_en
    """
    try:
        prompt = f"""
ä½ ç°åœ¨æ˜¯ã€Œç”µå½±å¯¼æ¼” + æ‘„å½±æŒ‡å¯¼ + æœåŒ–é“æ€»ç›‘ + æç¤ºè¯å·¥ç¨‹å¸ˆã€ã€‚

è¯·åŸºäºç»™ä½ çš„è¿™ä¸€å¸§ç”»é¢ï¼Œåªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼ˆä¸è¦ä»»ä½•è§£é‡Šï¼‰ï¼Œå­—æ®µè¦æ±‚å¦‚ä¸‹ï¼š

- "index": æ•´æ•°ï¼Œå›ºå®šä¸º {index}
- "scene_description_zh": å½“å‰ç”»é¢çš„æ•´ä½“ä¸­æ–‡æè¿°ï¼ˆ1-3 å¥ï¼ŒåŒ…å«äººç‰©+åŠ¨ä½œ+åœºæ™¯ç©ºé—´+è§†è§’ï¼‰
- "tags_zh": çŸ­ä¸­æ–‡æ ‡ç­¾æ•°ç»„ï¼Œä¾‹å¦‚ ["#é«˜é€Ÿè¿½é€", "#ç©ºä¸­æ»‘ç¿”"]
- "camera": å¯¹è±¡ï¼ŒåŒ…å«ï¼š
  - "shot_type_zh", "shot_type"
  - "angle_zh", "angle"
  - "movement_zh", "movement"
  - "composition_zh", "composition"
- "color_and_light_zh": è‰²å½©ä¸å…‰çº¿
- "mood_zh": æƒ…ç»ªæ°›å›´

- "characters": æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªäººç‰©å¯¹è±¡ï¼š
  - "role_zh": äººç‰©èº«ä»½ï¼ˆå¥³ä¸»/ç”·ä¸»/å†’é™©è€…/èˆ¹é•¿ç­‰ï¼‰
  - "gender_zh": æ€§åˆ«
  - "age_look_zh": å¹´é¾„è§‚æ„Ÿ
  - "body_type_zh": ä½“å‹
  - "clothing_zh": æœè£…é£æ ¼ä¸é¢œè‰²ï¼ˆåç»­ç»Ÿä¸€æœè£…ä¼šç”¨åˆ°ï¼‰
  - "hair_zh": å‘å‹å‘è‰²
  - "expression_zh": ç®€è¦è¡¨æƒ…
  - "pose_body_zh": èº«ä½“å§¿æ€
  - "props_zh": è¯¥äººç‰©èº«ä¸Šæˆ–æ‰‹æŒçš„é“å…·

- "character_action_detail_zh": ç”¨ 1-3 å¥è¯¦ç»†å†™æ¸…æ­¤äººç‰©ç°åœ¨çš„åŠ¨ä½œï¼ˆå¤´/ä¸Šè‚¢/èº¯å¹²/ä¸‹è‚¢åŠæ¥è§¦ç‚¹ï¼‰
- "face_expression_detail_zh": é¢éƒ¨ä¸çœ¼ç¥ç»†èŠ‚ï¼ˆè‚Œè‚‰ç´§å¼ åº¦ã€çœ¼ç›çŠ¶æ€ã€æ˜¯å¦æœ‰å˜å½¢ç­‰ï¼‰
- "cloth_hair_reaction_zh": å¤´å‘ä¸è¡£ç‰©åœ¨é£/é€Ÿåº¦/çˆ†ç‚¸ç­‰å½±å“ä¸‹çš„å½¢æ€ï¼ˆè¢«å¹èµ·ã€ç´§è´´èº«ä½“ç­‰ï¼‰

- "environment_detail_zh": æŒ‰å‰æ™¯/ä¸­æ™¯/èƒŒæ™¯æè¿°åœºæ™¯ç©ºé—´ç»“æ„ï¼ˆå®¤å†…/å®¤å¤–/åœ°å½¢/å»ºç­‘ç­‰ï¼‰
- "weather_force_detail_zh": é£/é›¨/é›ª/å†²å‡»æ³¢/æ°”æµç­‰ç¯å¢ƒåŠ›çš„æ–¹å‘å’Œå¼ºåº¦åŠå¯¹äººç‰©çš„å½±å“

- "props_and_tech_detail_zh": åœºæ™¯ä¸­é‡è¦é“å…·/ç§‘æŠ€å…ƒç´ çš„å¤–è§‚ä¸ä½ç½®
- "physics_reaction_detail_zh": å—åŠ›ä¸ç‰©ç†åé¦ˆï¼ˆå¦‚æ‹³å¤´æ‰“åœ¨è„¸ä¸Šå½¢æˆå½¢å˜â†’å›å¼¹ã€è½¦è¾†æ’å‡»ç­‰ï¼‰
- "structure_damage_detail_zh": ç‰©ä½“/å»ºç­‘/è½¦è¾†/æœºç¿¼ç­‰çš„ç»“æ„æŸåæƒ…å†µï¼ˆå¦‚æœæ²¡æœ‰å¯å†™â€œæ— æ˜æ˜¾ç»“æ„æŸåâ€ï¼‰
- "debris_motion_detail_zh": ç¢ç‰‡/ç»ç’ƒæ¸£/çŸ³å—/é›¶ä»¶çš„é£æ•£è½¨è¿¹ï¼ˆå¦‚æœæ²¡æœ‰å¯å†™â€œæ— æ˜æ˜¾ç¢ç‰‡é£æ•£â€ï¼‰

- "motion_detail_zh": ä¸Šä¸€ç¬é—´â†’å½“å‰â†’ä¸‹ä¸€ç¬é—´çš„åŠ¨ä½œè¶‹åŠ¿
- "fx_detail_zh": ç«èŠ±/çƒŸé›¾/å°˜åœŸ/èƒ½é‡æ³¢ç­‰ç‰¹æ•ˆçš„å½¢æ€ä¸è¿åŠ¨
- "lighting_color_detail_zh": å…‰æºæ•°é‡/æ–¹å‘/è‰²æ¸©å·®å¼‚/æ˜¯å¦æœ‰è½®å»“å…‰ã€é—ªå…‰ç­‰
- "audio_cue_detail_zh": æ¨æµ‹çš„å£°éŸ³è®¾è®¡ï¼ˆç¯å¢ƒå£°/ç‰¹æ•ˆå£°/BGM èŠ‚å¥æ„Ÿï¼‰
- "edit_rhythm_detail_zh": å‰ªè¾‘èŠ‚å¥ï¼ˆæ­£å¸¸/æ…¢åŠ¨ä½œ/åŠ é€Ÿ/ç”©é•œè½¬åœºç­‰ï¼‰

- "midjourney_prompt": ä¸€è¡Œè‹±æ–‡ Midjourney v6 æç¤ºè¯ï¼ˆé€‚åˆç”Ÿæˆè¿™ä¸€å¸§ï¼‰
- "midjourney_negative_prompt": ä¸€è¡Œè‹±æ–‡è´Ÿé¢æç¤ºè¯
- "video_prompt_en": è‹¥å¹²å¥è‹±æ–‡è§†é¢‘æç¤ºè¯ï¼Œé€‚åˆ SORA/VEOï¼ˆæè¿°äººç‰©ã€åŠ¨ä½œã€åœºæ™¯ã€æœºä½ã€å…‰çº¿å’Œæ—¶é•¿ï¼‰

è¦æ±‚ï¼š
1. ä¸€å®šè¦æ˜¯åˆæ³• JSONï¼Œæ‰€æœ‰ key ä½¿ç”¨åŒå¼•å·ï¼Œä¸èƒ½æœ‰æ³¨é‡Šã€ä¸èƒ½æœ‰å¤šä½™é€—å·ã€‚
2. æ‰€æœ‰ä¸Šè¿°å­—æ®µéƒ½å¿…é¡»å‡ºç°ï¼ˆå³ä½¿å†…å®¹ä¸ºç©ºå­—ç¬¦ä¸²æˆ–ç©ºæ•°ç»„ï¼‰ã€‚
3. åªè¾“å‡º JSONï¼Œä¸è¦é¢å¤–æ–‡å­—ã€‚
"""
        resp = model.generate_content([prompt, img])
        text = _extract_text_from_response(resp)
        if not text:
            raise ValueError("æ¨¡å‹æœªè¿”å›æ–‡æœ¬")

        start = text.find("{")
        end = text.rfind("}")
        if start == -1 or end == -1 or end <= start:
            raise ValueError("æœªæ£€æµ‹åˆ°æœ‰æ•ˆ JSON ç»“æ„")

        json_str = text[start : end + 1]
        info = json.loads(json_str)

        # è¡¥é½å­—æ®µï¼Œé¿å… KeyError
        info["index"] = index
        info.setdefault("scene_description_zh", "")
        info.setdefault("tags_zh", [])
        info.setdefault("camera", {})
        info.setdefault("color_and_light_zh", "")
        info.setdefault("mood_zh", "")
        info.setdefault("characters", [])
        info.setdefault("character_action_detail_zh", "")
        info.setdefault("face_expression_detail_zh", "")
        info.setdefault("cloth_hair_reaction_zh", "")
        info.setdefault("environment_detail_zh", "")
        info.setdefault("weather_force_detail_zh", "")
        info.setdefault("props_and_tech_detail_zh", "")
        info.setdefault("physics_reaction_detail_zh", "")
        info.setdefault("structure_damage_detail_zh", "")
        info.setdefault("debris_motion_detail_zh", "")
        info.setdefault("motion_detail_zh", "")
        info.setdefault("fx_detail_zh", "")
        info.setdefault("lighting_color_detail_zh", "")
        info.setdefault("audio_cue_detail_zh", "")
        info.setdefault("edit_rhythm_detail_zh", "")
        info.setdefault("midjourney_prompt", "")
        info.setdefault("midjourney_negative_prompt", "")
        info.setdefault("video_prompt_en", "")

        cam = info["camera"]
        cam.setdefault("shot_type_zh", "")
        cam.setdefault("shot_type", "")
        cam.setdefault("angle_zh", "")
        cam.setdefault("angle", "")
        cam.setdefault("movement_zh", "")
        cam.setdefault("movement", "")
        cam.setdefault("composition_zh", "")
        cam.setdefault("composition", "")

        return info

    except Exception as e:
        # å‡ºé”™æ—¶è¿”å›ç©ºå£³ï¼Œä¿è¯åç»­æµç¨‹ä¸ç‚¸
        return {
            "index": index,
            "scene_description_zh": f"ï¼ˆAI åˆ†æå¤±è´¥ï¼š{e}ï¼‰",
            "tags_zh": [],
            "camera": {
                "shot_type_zh": "",
                "shot_type": "",
                "angle_zh": "",
                "angle": "",
                "movement_zh": "",
                "movement": "",
                "composition_zh": "",
                "composition": "",
            },
            "color_and_light_zh": "",
            "mood_zh": "",
            "characters": [],
            "character_action_detail_zh": "",
            "face_expression_detail_zh": "",
            "cloth_hair_reaction_zh": "",
            "environment_detail_zh": "",
            "weather_force_detail_zh": "",
            "props_and_tech_detail_zh": "",
            "physics_reaction_detail_zh": "",
            "structure_damage_detail_zh": "",
            "debris_motion_detail_zh": "",
            "motion_detail_zh": "",
            "fx_detail_zh": "",
            "lighting_color_detail_zh": "",
            "audio_cue_detail_zh": "",
            "edit_rhythm_detail_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
            "video_prompt_en": "",
        }


def analyze_images_concurrently(
    images: List[Image.Image], model, max_ai_frames: int
) -> List[Dict[str, Any]]:
    """
    å¹¶å‘åˆ†æå¤šå¼ å›¾ç‰‡ã€‚
    åªå¯¹å‰ max_ai_frames å¸§åš AI è°ƒç”¨ï¼Œå…¶ä½™å¸§ç”¨å ä½è¯´æ˜ã€‚
    """
    n = len(images)
    if n == 0:
        return []

    use_n = min(max_ai_frames, n)
    results: List[Dict[str, Any]] = [None] * n  # type: ignore

    status = st.empty()
    status.info(f"âš¡ æ­£åœ¨å¯¹å‰ {use_n} å¸§è¿›è¡Œ AI åˆ†æï¼ˆå…± {n} å¸§ï¼‰ï¼Œå…¶ä½™å¸§ä¿ç•™æˆªå›¾ä¸è‰²å¡ã€‚")

    with concurrent.futures.ThreadPoolExecutor(max_workers=min(use_n, 6)) as executor:
        future_to_index = {
            executor.submit(analyze_single_image, images[i], model, i + 1): i
            for i in range(use_n)
        }
        for future in concurrent.futures.as_completed(future_to_index):
            i = future_to_index[future]
            try:
                results[i] = future.result()
            except Exception as e:
                results[i] = {
                    "index": i + 1,
                    "scene_description_zh": f"ï¼ˆAI åˆ†æå¤±è´¥ï¼š{e}ï¼‰",
                    "tags_zh": [],
                    "camera": {
                        "shot_type_zh": "",
                        "shot_type": "",
                        "angle_zh": "",
                        "angle": "",
                        "movement_zh": "",
                        "movement": "",
                        "composition_zh": "",
                        "composition": "",
                    },
                    "color_and_light_zh": "",
                    "mood_zh": "",
                    "characters": [],
                    "character_action_detail_zh": "",
                    "face_expression_detail_zh": "",
                    "cloth_hair_reaction_zh": "",
                    "environment_detail_zh": "",
                    "weather_force_detail_zh": "",
                    "props_and_tech_detail_zh": "",
                    "physics_reaction_detail_zh": "",
                    "structure_damage_detail_zh": "",
                    "debris_motion_detail_zh": "",
                    "motion_detail_zh": "",
                    "fx_detail_zh": "",
                    "lighting_color_detail_zh": "",
                    "audio_cue_detail_zh": "",
                    "edit_rhythm_detail_zh": "",
                    "midjourney_prompt": "",
                    "midjourney_negative_prompt": "",
                    "video_prompt_en": "",
                }

    for i in range(use_n, n):
        results[i] = {
            "index": i + 1,
            "scene_description_zh": "ï¼ˆæœ¬å¸§æœªåš AI åˆ†æï¼Œç”¨äºèŠ‚çœå½“å‰ API é…é¢ï¼Œä½†ä»å¯ç”¨äºè§†è§‰å‚è€ƒå’Œè‰²å¡ã€‚ï¼‰",
            "tags_zh": [],
            "camera": {
                "shot_type_zh": "",
                "shot_type": "",
                "angle_zh": "",
                "angle": "",
                "movement_zh": "",
                "movement": "",
                "composition_zh": "",
                "composition": "",
            },
            "color_and_light_zh": "",
            "mood_zh": "",
            "characters": [],
            "character_action_detail_zh": "",
            "face_expression_detail_zh": "",
            "cloth_hair_reaction_zh": "",
            "environment_detail_zh": "",
            "weather_force_detail_zh": "",
            "props_and_tech_detail_zh": "",
            "physics_reaction_detail_zh": "",
            "structure_damage_detail_zh": "",
            "debris_motion_detail_zh": "",
            "motion_detail_zh": "",
            "fx_detail_zh": "",
            "lighting_color_detail_zh": "",
            "audio_cue_detail_zh": "",
            "edit_rhythm_detail_zh": "",
            "midjourney_prompt": "",
            "midjourney_negative_prompt": "",
            "video_prompt_en": "",
        }

    status.empty()
    return results


# ========================
# æ•´ä½“å‰§æƒ…åˆ†æ
# ========================

def analyze_overall_video(frame_infos: List[Dict[str, Any]], model) -> str:
    described = [
        info
        for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆæ•´ä½“å‰§æƒ…å¤§çº²ã€‚ï¼‰"

    parts = []
    for info in described:
        idx = info["index"]
        cam = info.get("camera", {})
        tags = info.get("tags_zh", [])
        part = (
            f"ç¬¬ {idx} å¸§ï¼š{info.get('scene_description_zh', '')}\n"
            f"æ™¯åˆ«ï¼š{cam.get('shot_type_zh', '')}ï¼›è§’åº¦ï¼š{cam.get('angle_zh', '')}ï¼›è¿é•œï¼š{cam.get('movement_zh', '')}ï¼›æ„å›¾ï¼š{cam.get('composition_zh', '')}\n"
            f"è‰²å½©ä¸å…‰å½±ï¼š{info.get('color_and_light_zh', '')}\n"
            f"æƒ…ç»ªæ°›å›´ï¼š{info.get('mood_zh', '')}\n"
            f"æ ‡ç­¾ï¼š{'ã€'.join(tags)}"
        )
        parts.append(part)

    joined = "\n\n".join(parts)

    prompt = f"""
ä½ ç°åœ¨æ˜¯èµ„æ·±è§†é¢‘å¯¼æ¼” + å‰ªè¾‘å¸ˆ + çŸ­è§†é¢‘è¿è¥ä¸“å®¶ + å†…å®¹åˆè§„å®¡æ ¸å‘˜ã€‚
ä¸‹é¢æ˜¯ä»ä¸€æ®µè§†é¢‘ä¸­æŠ½å–çš„è‹¥å¹²å…³é”®å¸§çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›è¯´æ˜ï¼Œå¯¹æ•´æ®µè§†é¢‘åšæ•´ä½“åˆ†æã€‚

=== å¸§çº§è¯´æ˜å¼€å§‹ ===
{joined}
=== å¸§çº§è¯´æ˜ç»“æŸ ===

è¯·ä¸¥æ ¼æŒ‰ä¸‹é¢ç»“æ„è¾“å‡ºä¸­æ–‡åˆ†æï¼š

ã€å‰§æƒ…å¤§çº²ã€‘
ç”¨ 2-4 å¥æ¦‚æ‹¬è¿™æ®µè§†é¢‘çš„å¤§è‡´å†…å®¹/äººç‰©å…³ç³»/å‘ç”Ÿåœºæ™¯ã€‚

ã€æ•´ä½“è§†å¬é£æ ¼ã€‘
ä»èŠ‚å¥å¿«æ…¢ã€é•œå¤´æ„Ÿã€è‰²å½©æ°”è´¨ï¼ˆæš–/å†·/æ—¥å¸¸/æ¢¦å¹»ï¼‰ã€æƒ…ç»ªæ°›å›´ç­‰è§’åº¦æ€»ç»“æ•´ä½“é£æ ¼ã€‚

ã€é€‚åˆçš„è¯é¢˜æ ‡ç­¾ã€‘
ç”¨ #æ ‡ç­¾ å½¢å¼ç»™å‡º 5-10 ä¸ªï¼Œé€‚åˆæŠ–éŸ³/å°çº¢ä¹¦/è§†é¢‘å·ç­‰å¹³å°ã€‚

ã€å•†ä¸šä¸åˆè§„é£é™©ã€‘
ä»â€œè¡€è…¥/æš´åŠ›/è‰²æƒ…/æ”¿æ²»/å“ç‰Œå•†æ ‡â€ç­‰ç»´åº¦ï¼Œç®€å•è¯„ä¼°ï¼š
æ•´ä½“é£é™©çº§åˆ«ï¼šä½ / ä¸­ / é«˜
å¹¶ç”¨ 2-3 å¥è¯è¯´æ˜éœ€è¦æ³¨æ„çš„ç‚¹ã€‚

è¯·ç›´æ¥è¾“å‡ºä»¥ä¸Š 4 ä¸ªå°èŠ‚ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
    try:
        resp = model.generate_content(prompt)
        return _extract_text_from_response(resp)
    except Exception as e:
        msg = str(e)
        if "quota" in msg or "You exceeded your current quota" in msg:
            return "æ•´ä½“åˆ†æå¤±è´¥ï¼šå½“å‰ Gemini å…è´¹é¢åº¦çš„æ¯åˆ†é’Ÿè°ƒç”¨æ¬¡æ•°å·²ç”¨å®Œï¼Œè¯·ç¨ç­‰å‡ åç§’æˆ–å‡å°‘æœ¬æ¬¡åˆ†æå¸§æ•°åé‡è¯•ã€‚"
        return f"æ•´ä½“åˆ†æå¤±è´¥ï¼š{msg}"


# ========================
# 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬
# ========================

def generate_ad_script(frame_infos: List[Dict[str, Any]], model) -> str:
    described = [
        info
        for info in frame_infos
        if info.get("scene_description_zh")
        and "æœªåš AI åˆ†æ" not in info["scene_description_zh"]
        and "AI åˆ†æå¤±è´¥" not in info["scene_description_zh"]
    ]
    if not described:
        return "ï¼ˆæš‚æœªè·å–åˆ°æœ‰æ•ˆçš„å¸§çº§åˆ†æï¼Œæ— æ³•ç”Ÿæˆå¹¿å‘Šæ—ç™½è„šæœ¬ã€‚ï¼‰"

    parts = []
    for info in described:
        idx = info["index"]
        tags = info.get("tags_zh", [])
        parts.append(
            f"ç¬¬ {idx} å¸§ï¼š{info.get('scene_description_zh', '')}ï¼›æ ‡ç­¾ï¼š{'ã€'.join(tags)}"
        )
    joined = "\n".join(parts)

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±å¹¿å‘Šå¯¼æ¼” + æ–‡æ¡ˆã€‚
æˆ‘æœ‰ä¸€ä¸ªç”±è‹¥å¹²ç”»é¢ç»„æˆçš„ç«–ç‰ˆçŸ­è§†é¢‘ï¼Œæ—¶é•¿å¤§çº¦ 8-12 ç§’ã€‚
ä¸‹é¢æ˜¯æ¯ä¸ªç”»é¢çš„ç®€è¦è¯´æ˜ï¼Œè¯·ä½ åŸºäºè¿™äº›ä¿¡æ¯ï¼Œå†™ä¸€æ¡é€‚åˆé…åˆè¿™äº›ç”»é¢æ’­æ”¾çš„ä¸­æ–‡å¹¿å‘Šæ—ç™½è„šæœ¬ã€‚

=== å…³é”®å¸§æ¦‚è§ˆ ===
{joined}
=== å…³é”®å¸§æ¦‚è§ˆç»“æŸ ===

è¦æ±‚ï¼š
1. æ—ç™½æ€»æ—¶é•¿æ§åˆ¶åœ¨ 8-12 ç§’å·¦å³ï¼ˆæ­£å¸¸è¯­é€Ÿï¼‰ï¼Œæ–‡æœ¬ 35-70 å­—å³å¯ã€‚
2. é£æ ¼ä¸ç”»é¢è°ƒæ€§åŒ¹é…ã€‚
3. ç”¨è‡ªç„¶å£è¯­åŒ–ä¸­æ–‡ï¼Œä¸è¦å‡ºç°â€œç”»é¢ä¸­â€â€œé•œå¤´é‡Œâ€å­—çœ¼ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢æ ¼å¼è¾“å‡ºï¼š

ã€10ç§’å¹¿å‘Šæ—ç™½è„šæœ¬ã€‘
ï¼ˆåœ¨è¿™é‡Œå†™å®Œæ•´çš„ä¸€æ®µæ—ç™½ï¼‰

ä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹ã€‚
"""
    try:
        resp = model.generate_content(prompt)
        return _extract_text_from_response(resp)
    except Exception as e:
        msg = str(e)
        if "quota" in msg or "You exceeded your current quota" in msg:
            return "å¹¿å‘Šæ–‡æ¡ˆç”Ÿæˆå¤±è´¥ï¼šå½“å‰ Gemini å…è´¹é¢åº¦çš„æ¯åˆ†é’Ÿè°ƒç”¨æ¬¡æ•°å·²ç”¨å®Œï¼Œè¯·ç¨ç­‰å‡ åç§’æˆ–å‡å°‘æœ¬æ¬¡åˆ†æå¸§æ•°åé‡è¯•ã€‚"
        return f"å¹¿å‘Šæ–‡æ¡ˆç”Ÿæˆå¤±è´¥ï¼š{msg}"


# ========================
# æ—¶é—´è½´ç®€ç‰ˆæ€»è§ˆ
# ========================

def generate_timeline_shotlist(
    frame_infos: List[Dict[str, Any]],
    used_range: Tuple[float, float],
) -> str:
    """
    ç®€ç‰ˆæ—¶é—´è½´æ€»è§ˆï¼š
    - ä¸å†é€å¸§å±•å¼€
    - è‡ªåŠ¨å°†æ•´æ®µè§†é¢‘åˆ†æˆ 3~5 æ®µï¼Œç»™å‡ºç®€è¦æè¿°ï¼ˆç”»é¢+åŠ¨ä½œ+æƒ…ç»ªï¼‰
    - æ–¹ä¾¿â€œè‡ªå·±çœ‹ä¸€çœ¼å°±æ‡‚æ•´æ¡è§†é¢‘ç»“æ„â€
    """

    def short(text: str, max_len: int = 50) -> str:
        text = (text or "").strip()
        if len(text) <= max_len:
            return text
        return text[:max_len].rstrip("ï¼Œ,ï¼›;ã€‚.!?ï¼Ÿï¼ ") + "â€¦"

    n = len(frame_infos)
    if n == 0:
        return "ï¼ˆæš‚æ— å…³é”®å¸§ï¼Œæ— æ³•ç”Ÿæˆæ—¶é—´è½´æ€»è§ˆã€‚ï¼‰"

    start_used, end_used = used_range
    total_len = max(0.1, end_used - start_used)

    # å†³å®šåˆ†å‡ æ®µï¼šçŸ­ â†’ 3 æ®µï¼Œä¸­ â†’ 4 æ®µï¼Œé•¿ â†’ 5 æ®µ
    if total_len <= 6:
        seg_count = 3
    elif total_len <= 12:
        seg_count = 4
    else:
        seg_count = 5

    seg_len = total_len / seg_count
    segments = []

    for s in range(seg_count):
        t0 = s * seg_len
        t1 = total_len if s == seg_count - 1 else (s + 1) * seg_len
        # å–è¿™ä¸€æ®µä¸­ç‚¹å¯¹åº”çš„å¸§
        mid_t = (t0 + t1) / 2
        mid_index = int(mid_t / total_len * (n - 1) + 0.5)
        mid_index = max(0, min(n - 1, mid_index))
        info = frame_infos[mid_index]

        scene = info.get("scene_description_zh", "")
        char_act = info.get("character_action_detail_zh", "")
        motion = info.get("motion_detail_zh", "")
        mood = info.get("mood_zh", "")

        parts = []
        if scene:
            parts.append(short(scene, 60))
        if char_act:
            parts.append("åŠ¨ä½œï¼š" + short(char_act, 40))
        if motion:
            parts.append("è¶‹åŠ¿ï¼š" + short(motion, 40))
        if mood:
            parts.append("æƒ…ç»ªï¼š" + short(mood, 30))

        text = "ï¼›".join(parts) if parts else "ï¼ˆæœ¬æ®µæœªæ£€æµ‹åˆ°æœ‰æ•ˆæè¿°ï¼‰"
        segments.append((t0, t1, text))

    lines = ["ã€æ•´æ®µæ—¶é—´è½´æ€»è§ˆã€‘"]
    for i, (t0, t1, text) in enumerate(segments, start=1):
        lines.append(f"ã€ç¬¬{i}æ®µ | {t0:.1f}-{t1:.1f} ç§’ã€‘{text}")

    return "\n".join(lines)


# ========================
# ä¾§è¾¹æ ï¼šAPI Key & å‚æ•°è®¾ç½®
# ========================

with st.sidebar:
    st.header("ğŸ”‘ ç¬¬ä¸€æ­¥ï¼šé…ç½® Gemini API Key")
    api_key = st.text_input(
        "è¾“å…¥ Google API Key",
        type="password",
        value=st.session_state["api_key"],
        help="ç²˜è´´ä½ çš„ Gemini API Keyï¼ˆé€šå¸¸ä»¥ AIza å¼€å¤´ï¼‰",
    )
    st.session_state["api_key"] = api_key

    st.markdown("---")
    max_ai_frames = st.slider(
        "æœ¬æ¬¡æœ€å¤šåš AI åˆ†æçš„å¸§æ•°ï¼ˆæ¶ˆè€—é…é¢ï¼‰",
        min_value=4,
        max_value=20,
        value=10,
        step=1,
    )
    st.caption("å»ºè®®ï¼š10 ç§’è§†é¢‘ 6~10 å¸§å³å¯ï¼›è¶…å‡ºéƒ¨åˆ†ä»æ˜¾ç¤ºæˆªå›¾å’Œè‰²å¡ï¼Œä½†ä¸è°ƒ AIã€‚")

    st.markdown("---")
    st.markdown("â± åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå•ä½ï¼šç§’ï¼‰")
    start_sec = st.number_input(
        "ä»ç¬¬å‡ ç§’å¼€å§‹ï¼ˆå«ï¼‰",
        min_value=0.0,
        value=0.0,
        step=0.5,
        help="ç²¾ç¡®åˆ° 0.5 ç§’ï¼›é»˜è®¤ 0 è¡¨ç¤ºä»å¤´å¼€å§‹",
    )
    end_sec = st.number_input(
        "åˆ°ç¬¬å‡ ç§’ç»“æŸï¼ˆ0 æˆ– â‰¤å¼€å§‹ç§’ è¡¨ç¤ºç›´åˆ°ç»“å°¾ï¼‰",
        min_value=0.0,
        value=0.0,
        step=0.5,
        help="ä¾‹å¦‚ï¼šåªåˆ†æ 3~8 ç§’ï¼Œå°±å¡« 3 å’Œ 8ï¼›å¡« 0 æˆ–ä¸å¤§äºå¼€å§‹ç§’åˆ™åˆ†æåˆ°ç»“å°¾",
    )

    if not api_key:
        st.warning("ğŸ”´ è¿˜æ²¡æœ‰ Keyï¼Œå…ˆå» https://ai.google.dev/ ç”³è¯·ä¸€ä¸ª")
    else:
        st.success("ğŸŸ¢ Key å·²å°±ç»ª")


# ========================
# åˆå§‹åŒ– Gemini æ¨¡å‹
# ========================

model = None
if api_key:
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
    except Exception as e:
        st.error(f"âŒ åˆå§‹åŒ– Gemini æ¨¡å‹å¤±è´¥ï¼š{e}")
        model = None


# ========================
# ä¸»æµç¨‹
# ========================

source_mode = st.radio(
    "ğŸ“¥ é€‰æ‹©è§†é¢‘æ¥æº",
    ["ä¸Šä¼ æœ¬åœ°æ–‡ä»¶", "è¾“å…¥ç½‘ç»œè§†é¢‘é“¾æ¥ï¼ˆæŠ–éŸ³ / Bç«™ / TikTok / YouTubeï¼‰"],
    index=0,
)

video_url: Optional[str] = None
uploaded_file = None

if source_mode == "ä¸Šä¼ æœ¬åœ°æ–‡ä»¶":
    uploaded_file = st.file_uploader(
        "ğŸ“‚ ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼ˆå»ºè®® < 50MBï¼‰",
        type=["mp4", "mov", "m4v", "avi", "mpeg"],
    )
else:
    video_url = st.text_input(
        "ğŸ”— è¾“å…¥è§†é¢‘é“¾æ¥",
        placeholder="ä¾‹å¦‚ï¼šhttps://v.douyin.com/xxxxxx æˆ– https://www.douyin.com/video/xxxxxxxxx",
    )

if st.button("ğŸš€ ä¸€é”®è§£ææ•´æ¡è§†é¢‘"):
    if not api_key or model is None:
        st.error("è¯·å…ˆåœ¨å·¦ä¾§è¾“å…¥æœ‰æ•ˆçš„ Google API Keyã€‚")
    else:
        tmp_path: Optional[str] = None
        source_label = ""
        source_type = ""

        try:
            # 1. å‡†å¤‡è§†é¢‘è·¯å¾„
            if source_mode == "ä¸Šä¼ æœ¬åœ°æ–‡ä»¶":
                source_type = "upload"
                if not uploaded_file:
                    st.error("è¯·å…ˆä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚")
                    st.stop()
                suffix = os.path.splitext(uploaded_file.name)[1] or ".mp4"
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                    tmp.write(uploaded_file.read())
                    tmp_path = tmp.name
                source_label = uploaded_file.name
            else:
                source_type = "url"
                if not video_url:
                    st.error("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„è§†é¢‘é“¾æ¥ã€‚")
                    st.stop()
                st.info("ğŸŒ æ­£åœ¨ä»ç½‘ç»œä¸‹è½½è§†é¢‘ï¼Œè¯·ç¨å€™...")
                tmp_path = download_video_from_url(video_url)
                source_label = video_url

            if not tmp_path:
                st.error("è§†é¢‘è·¯å¾„å¼‚å¸¸ï¼Œè¯·é‡è¯•ã€‚")
            else:
                # 2. æŠ½å¸§ï¼ˆå¸¦æ—¶é—´åŒºé—´ï¼‰
                st.info("â³ æ­£åœ¨æ ¹æ®æŒ‡å®šæ—¶é—´åŒºé—´è‡ªåŠ¨æŠ½å–å…³é”®å¸§...")
                images, duration, used_range = extract_keyframes_dynamic(
                    tmp_path,
                    start_sec=start_sec,
                    end_sec=end_sec if end_sec > 0 else None,
                )
                start_used, end_used = used_range

                try:
                    os.remove(tmp_path)
                except OSError:
                    pass

                if not images:
                    st.error("âŒ æ— æ³•ä»è§†é¢‘ä¸­è¯»å–å¸§ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ˜¯å¦æŸåæˆ–æ ¼å¼å¼‚å¸¸ã€‚")
                    st.stop()

                st.success(
                    f"âœ… å·²æˆåŠŸæŠ½å– {len(images)} ä¸ªå…³é”®å¸§ï¼ˆè§†é¢‘æ€»é•¿çº¦ {duration:.1f} ç§’ï¼Œ"
                    f"æœ¬æ¬¡åˆ†æåŒºé—´ï¼š{start_used:.1f}â€“{end_used:.1f} ç§’ï¼‰ã€‚"
                )

                # 3. ä¸»è‰²è°ƒ
                frame_palettes: List[List[Tuple[int, int, int]]] = []
                for img in images:
                    try:
                        palette_colors = get_color_palette(img, num_colors=5)
                    except Exception:
                        palette_colors = []
                    frame_palettes.append(palette_colors)

                # 4. æ§åˆ¶æœ¬æ¬¡ AI è°ƒç”¨æ€»æ•°ï¼šå¸§çº§åˆ†æ + æ•´ä½“ + å¹¿å‘Š
                overhead_calls = 2  # overall + ad_script
                max_ai_frames_safe = max(
                    1,
                    min(max_ai_frames, FREE_TIER_RPM_LIMIT - overhead_calls),
                )
                if max_ai_frames_safe < max_ai_frames:
                    st.info(
                        f"ä¸ºé¿å…è§¦å‘å…è´¹é¢åº¦é™åˆ¶ï¼Œæœ¬æ¬¡åªå¯¹ **å‰ {max_ai_frames_safe} å¸§** åš AI åˆ†æ "
                        f"ï¼ˆä¾§è¾¹æ è®¾ç½®ä¸º {max_ai_frames} å¸§ï¼‰ã€‚"
                    )

                # 5. å¸§çº§åˆ†æ
                with st.spinner("ğŸ§  æ­£åœ¨ä¸ºå…³é”®å¸§ç”Ÿæˆç»“æ„åŒ–åˆ†æ + MJ æç¤ºè¯ + è§†é¢‘æç¤ºè¯..."):
                    frame_infos = analyze_images_concurrently(
                        images, model, max_ai_frames=max_ai_frames_safe
                    )

                # 6. æ•´ä½“åˆ†æ + å¹¿å‘Š + æ—¶é—´è½´æ€»è§ˆ
                with st.spinner("ğŸ“š æ­£åœ¨ç”Ÿæˆæ•´æ®µè§†é¢‘çš„å‰§æƒ…å¤§çº²ä¸è¯é¢˜æ ‡ç­¾..."):
                    overall = analyze_overall_video(frame_infos, model)
                with st.spinner("ğŸ¤ æ­£åœ¨ç”Ÿæˆ 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬..."):
                    ad_script = generate_ad_script(frame_infos, model)
                with st.spinner("ğŸ¬ æ­£åœ¨ç”Ÿæˆæ—¶é—´è½´æ€»è§ˆï¼ˆç®€ç‰ˆï¼‰..."):
                    timeline_shotlist = generate_timeline_shotlist(
                        frame_infos, used_range=used_range
                    )

                # 7. ç»„è£…å¯¼å‡º JSON + å†å²è®°å½•
                export_frames = []
                for info, palette in zip(frame_infos, frame_palettes):
                    export_frames.append(
                        {
                            "index": info.get("index"),
                            "scene_description_zh": info.get("scene_description_zh", ""),
                            "tags_zh": info.get("tags_zh", []),
                            "camera": info.get("camera", {}),
                            "color_and_light_zh": info.get("color_and_light_zh", ""),
                            "mood_zh": info.get("mood_zh", ""),
                            "characters": info.get("characters", []),
                            "character_action_detail_zh": info.get("character_action_detail_zh", ""),
                            "face_expression_detail_zh": info.get("face_expression_detail_zh", ""),
                            "cloth_hair_reaction_zh": info.get("cloth_hair_reaction_zh", ""),
                            "environment_detail_zh": info.get("environment_detail_zh", ""),
                            "weather_force_detail_zh": info.get("weather_force_detail_zh", ""),
                            "props_and_tech_detail_zh": info.get("props_and_tech_detail_zh", ""),
                            "physics_reaction_detail_zh": info.get("physics_reaction_detail_zh", ""),
                            "structure_damage_detail_zh": info.get("structure_damage_detail_zh", ""),
                            "debris_motion_detail_zh": info.get("debris_motion_detail_zh", ""),
                            "motion_detail_zh": info.get("motion_detail_zh", ""),
                            "fx_detail_zh": info.get("fx_detail_zh", ""),
                            "lighting_color_detail_zh": info.get("lighting_color_detail_zh", ""),
                            "audio_cue_detail_zh": info.get("audio_cue_detail_zh", ""),
                            "edit_rhythm_detail_zh": info.get("edit_rhythm_detail_zh", ""),
                            "midjourney_prompt": info.get("midjourney_prompt", ""),
                            "midjourney_negative_prompt": info.get("midjourney_negative_prompt", ""),
                            "video_prompt_en": info.get("video_prompt_en", ""),
                            "palette_rgb": [list(c) for c in (palette or [])],
                            "palette_hex": [rgb_to_hex(c) for c in (palette or [])],
                        }
                    )

                export_data = {
                    "meta": {
                        "model": GEMINI_MODEL_NAME,
                        "frame_count": len(images),
                        "max_ai_frames_this_run": max_ai_frames_safe,
                        "duration_sec_est": duration,
                        "start_sec_used": start_used,
                        "end_sec_used": end_used,
                        "source_type": source_type,
                        "source_label": source_label,
                    },
                    "frames": export_frames,
                    "overall_analysis": overall,
                    "ad_script_10s": ad_script,
                    "timeline_shotlist_zh": timeline_shotlist,
                }

                json_str = json.dumps(export_data, ensure_ascii=False, indent=2)

                history = st.session_state["analysis_history"]
                run_id = f"run_{len(history) + 1}"
                history.append(
                    {
                        "id": run_id,
                        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "meta": export_data["meta"],
                        "data": export_data,
                    }
                )
                st.session_state["analysis_history"] = history

                # 8. å¤šæ ‡ç­¾é¡µå±•ç¤º
                tab_frames, tab_story, tab_json, tab_history = st.tabs(
                    [
                        "ğŸ å…³é”®å¸§ & æç¤ºè¯",
                        "ğŸ“š å‰§æƒ…æ€»ç»“ & å¹¿å‘Šæ—ç™½ & æ—¶é—´è½´æ€»è§ˆ",
                        "ğŸ“¦ JSON å¯¼å‡ºï¼ˆæœ¬æ¬¡ï¼‰",
                        "ğŸ•˜ å†å²è®°å½•ï¼ˆæœ¬ä¼šè¯ï¼‰",
                    ]
                )

                # --- Tab1ï¼šé€å¸§å¡ç‰‡ ---
                with tab_frames:
                    st.markdown(
                        f"å…±æŠ½å– **{len(images)}** ä¸ªå…³é”®å¸§ï¼Œå…¶ä¸­å‰ **{min(len(images), max_ai_frames_safe)}** å¸§åšäº† AI åˆ†æã€‚"
                    )
                    st.markdown("---")

                    for i, (img, info, palette) in enumerate(
                        zip(images, frame_infos, frame_palettes)
                    ):
                        with st.container():
                            st.markdown(f"### ğŸ“˜ å…³é”®å¸§ {i + 1}")

                            c1, c2 = st.columns([1.2, 2])

                            with c1:
                                st.image(
                                    img,
                                    caption=f"ç¬¬ {i + 1} å¸§ç”»é¢",
                                    width=DISPLAY_IMAGE_WIDTH,
                                )
                                palette_img = make_palette_image(palette)
                                st.image(
                                    palette_img,
                                    caption="ä¸»è‰²è°ƒè‰²å¡",
                                    width=PALETTE_WIDTH,
                                )
                                if palette:
                                    hex_list = ", ".join(
                                        rgb_to_hex(c) for c in palette
                                    )
                                    st.caption(f"ä¸»è‰² HEXï¼š{hex_list}")

                            with c2:
                                cam = info.get("camera", {})
                                tags = info.get("tags_zh", [])

                                analysis_lines = [
                                    f"ã€æ™¯åˆ«ã€‘{cam.get('shot_type_zh', '')}",
                                    f"ã€è¿é•œã€‘{cam.get('movement_zh', '')}",
                                    f"ã€æ‹æ‘„è§’åº¦ã€‘{cam.get('angle_zh', '')}",
                                    f"ã€æ„å›¾ã€‘{cam.get('composition_zh', '')}",
                                    f"ã€è‰²å½©ä¸å…‰å½±ã€‘{info.get('color_and_light_zh', '')}",
                                    f"ã€ç”»é¢å†…å®¹ã€‘{info.get('scene_description_zh', '')}",
                                    f"ã€æƒ…ç»ªæ°›å›´ã€‘{info.get('mood_zh', '')}",
                                    f"ã€å…³é”®è¯æ ‡ç­¾ã€‘{' '.join(tags)}",
                                ]
                                analysis_text = "\n".join(analysis_lines).strip()

                                st.markdown("**åˆ†é•œåˆ†æï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                                st.code(
                                    analysis_text
                                    or "ï¼ˆæš‚æ— åˆ†é•œåˆ†æï¼Œå¯èƒ½æœªåš AI åˆ†æï¼‰",
                                    language="markdown",
                                )

                                st.markdown("**äººç‰©åŠ¨ä½œç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                                st.code(
                                    info.get("character_action_detail_zh")
                                    or "ï¼ˆæš‚æ— åŠ¨ä½œç»†èŠ‚ï¼Œå¯èƒ½æœªåš AI åˆ†æï¼‰",
                                    language="markdown",
                                )

                                st.markdown("**åœºæ™¯ç»†èŠ‚ï¼ˆå¯å¤åˆ¶ï¼‰ï¼š**")
                                scene_detail = info.get("environment_detail_zh", "")
                                props_detail = info.get("props_and_tech_detail_zh", "")
                                scene_text = (
                                    scene_detail
                                    + ("\n\né“å…·ä¸ç§‘æŠ€å…ƒç´ ï¼š" + props_detail if props_detail else "")
                                ).strip()
                                st.code(
                                    scene_text or "ï¼ˆæš‚æ— åœºæ™¯ç»†èŠ‚ï¼Œå¯èƒ½æœªåš AI åˆ†æï¼‰",
                                    language="markdown",
                                )

                                st.markdown("**SORA / VEO è§†é¢‘æç¤ºè¯ï¼ˆè‹±æ–‡ï¼Œå¯å¤åˆ¶ï¼‰ï¼š**")
                                st.code(
                                    info.get("video_prompt_en") or "ï¼ˆæš‚æ— è§†é¢‘æç¤ºè¯ï¼‰",
                                    language="markdown",
                                )

                                st.markdown("**Midjourney é™å¸§æç¤ºè¯ï¼ˆå¯é€‰ï¼‰ï¼š**")
                                st.code(
                                    info.get("midjourney_prompt")
                                    or "ï¼ˆæš‚æ—  Midjourney æç¤ºè¯ï¼‰",
                                    language="markdown",
                                )

                            st.markdown("---")

                # --- Tab2ï¼šæ•´ä½“åˆ†æ + å¹¿å‘Šæ–‡æ¡ˆ + æ—¶é—´è½´æ€»è§ˆ ---
                with tab_story:
                    st.markdown("### ğŸ“š æ•´ä½“å‰§æƒ…ä¸è§†å¬é£æ ¼æ€»ç»“")
                    st.code(overall, language="markdown")

                    st.markdown("### ğŸ¤ 10 ç§’å¹¿å‘Šæ—ç™½è„šæœ¬")
                    st.code(ad_script, language="markdown")

                    st.markdown("### ğŸ¬ æ—¶é—´è½´æ€»è§ˆï¼ˆç®€ç‰ˆï¼Œå¯å¤åˆ¶ï¼‰")
                    st.code(timeline_shotlist, language="markdown")

                # --- Tab3ï¼šæœ¬æ¬¡ JSON å¯¼å‡º ---
                with tab_json:
                    st.markdown("### ğŸ“¦ ä¸‹è½½æœ¬æ¬¡åˆ†æçš„ JSON æ–‡ä»¶")
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½æœ¬æ¬¡ video_analysis.json",
                        data=json_str,
                        file_name="video_analysis.json",
                        mime="application/json",
                    )

                    with st.expander("ğŸ” é¢„è§ˆéƒ¨åˆ† JSON å†…å®¹"):
                        preview = json_str[:3000] + (
                            "\n...\n" if len(json_str) > 3000 else ""
                        )
                        st.code(preview, language="json")

                # --- Tab4ï¼šå†å²è®°å½• ---
                with tab_history:
                    st.markdown("### ğŸ•˜ å½“å‰ä¼šè¯å†å²è®°å½•ï¼ˆåˆ·æ–°é¡µé¢ä¼šæ¸…ç©ºï¼‰")

                    history = st.session_state.get("analysis_history", [])
                    if not history:
                        st.info("å½“å‰ä¼šè¯è¿˜æ²¡æœ‰ä»»ä½•å†å²è®°å½•ã€‚")
                    else:
                        options = [
                            f"{len(history) - i}. {h['created_at']} | {h['meta'].get('source_label','')} | "
                            f"{h['meta'].get('frame_count',0)} å¸§ | åŒºé—´ {h['meta'].get('start_sec_used',0):.1f}-{h['meta'].get('end_sec_used',0):.1f}s"
                            for i, h in enumerate(reversed(history))
                        ]
                        idx_display = st.selectbox(
                            "é€‰æ‹©ä¸€æ¡å†å²è®°å½•æŸ¥çœ‹",
                            options=list(range(len(history))),
                            format_func=lambda i: options[i],
                        )
                        real_index = len(history) - 1 - idx_display
                        selected = history[real_index]

                        st.markdown(
                            f"**IDï¼š** `{selected['id']}`  \n"
                            f"**æ—¶é—´ï¼š** {selected['created_at']}  \n"
                            f"**æ¥æºç±»å‹ï¼š** {selected['meta'].get('source_type','')}  \n"
                            f"**æ¥æºæ ‡è¯†ï¼š** {selected['meta'].get('source_label','')}  \n"
                            f"**åˆ†æåŒºé—´ï¼š** {selected['meta'].get('start_sec_used',0):.1f}â€“{selected['meta'].get('end_sec_used',0):.1f} ç§’  \n"
                            f"**å¸§æ•°ï¼š** {selected['meta'].get('frame_count',0)}  \n"
                            f"**æ¨¡å‹ï¼š** {selected['meta'].get('model','')}"
                        )

                        hist_json = json.dumps(
                            selected["data"], ensure_ascii=False, indent=2
                        )
                        st.download_button(
                            label="â¬‡ï¸ ä¸‹è½½è¯¥å†å²è®°å½• JSON",
                            data=hist_json,
                            file_name=f"video_analysis_{selected['id']}.json",
                            mime="application/json",
                        )

                        frames = selected["data"].get("frames", [])
                        if frames:
                            st.markdown("#### éƒ¨åˆ†å¸§é¢„è§ˆï¼ˆä¸­æ–‡åœºæ™¯ + è‹±æ–‡è§†é¢‘æç¤ºè¯ï¼‰")
                            for f in frames[:3]:
                                st.markdown(f"**ç¬¬ {f.get('index')} å¸§ï¼š**")
                                st.write(f.get("scene_description_zh", ""))
                                vp = f.get("video_prompt_en", "")
                                if vp:
                                    st.code(vp, language="markdown")
                                st.markdown("---")

        except Exception as e:
            st.error(f"ä¸‹è½½æˆ–è§£æè§†é¢‘æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
